<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>学习笔记 on XZY&#39;s BLOG</title>
    <link>https://www.xzywisdili.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</link>
    <description>Recent content in 学习笔记 on XZY&#39;s BLOG</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 26 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://www.xzywisdili.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>5 分钟简单生成指定形状的词云</title>
      <link>https://www.xzywisdili.com/post/2023-04-26-generatewordcloudin5minutes/</link>
      <pubDate>Wed, 26 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.xzywisdili.com/post/2023-04-26-generatewordcloudin5minutes/</guid>
      <description>群里有人问，想要做一张这样的指定形状的词云：
我试了一下，其实很简单，基本上 5 分钟就搞定。这里的简单思路就是：
使用 jieba 包对原始文本进行分词处理; 将分词之后的结果导入 wordcloud 包生成分词。 至于指定形状的需求，需要注意在 WordCloud() 中的 mask 参数导入指定的形状遮罩图片就好，具体的代码如下：
import jieba from wordcloud import WordCloud import numpy as np import PIL.Image as image # 对分析文本做分词处理 def cut(text): word_list = jieba.cut(text) result = &amp;#34; &amp;#34;.join(word_list) return result # 读取分析文本，进行分词 with open(&amp;#34;D:\\测试文章.txt&amp;#34;, &amp;#39;rb&amp;#39;) as fp: text = fp.read().decode(&amp;#39;utf-8&amp;#39;) words = cut(text) # 设置目标词云的遮罩 mask = np.array(image.open(&amp;#34;D:\\测试图片.png&amp;#34;)) # 进行词云分析并生成 test_cloud = WordCloud( mask = mask, background_color = &amp;#39;#FFFFFF&amp;#39;, font_path=&amp;#39;C:\\Windows\\Fonts\\方正聚珍新仿.</description>
    </item>
    
    <item>
      <title>数据挖掘的奇效——爬到网站没有显示的数据</title>
      <link>https://www.xzywisdili.com/post/2023-04-23-userjsonfindextradata/</link>
      <pubDate>Sun, 23 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.xzywisdili.com/post/2023-04-23-userjsonfindextradata/</guid>
      <description>有没有想过，有些网站的页面上展示的数据只有一部分的时候，通过稍微一点挖掘就能发现额外的数据，收获更多信息。
这里就举一个最近发现的例子——云顶之弈公开赛“云巅赛道”的官方实时排名更新。 可以看到，这里只展示了云巅赛道前 60 名的用户昵称、段位、胜点、排位场数、登顶率和前四率。
但是其实，只要扒到网站请求的数据，兴许可以看到更多的数据。
发现网站请求返回的数据 这里其实没有想象中那么难。在「检查」中的「网络」选项卡查看网站的请求，一眼就能发现这个全是大写的请求返回的 json 数据。 稍微看一下，各类数据也都对的上，就是这个了：
这里可以复制请求链接，再每隔一段时间请求一次，查看实时更新变化，我这里就不做了，有兴趣的可以查看我的另一篇文章。我就直接将 json 数据保存到了本地。
使用 RJson 解析 json 格式数据 这里就使用一下 R 语言中的 RJson 包来解析一下 json 格式数据：
library(rjson) result &amp;lt;- fromJSON(file=&amp;#34;云巅.json&amp;#34;) result 可以看到，直接使用 fromJSON 读取出来的是 list 格式的数据，还是多次嵌套的 list：
第一层：每一个用户作为一层; 第二层：单个用户下的每一个变量数据作为第二层； 同时，可以看到，最关键的用户名 sName 变量用了 URL Code 编码，需要再使用 URLdecode 解一下。
这就需要先进行多层 list 数据的解包，加上稍微一点预处理了：
library(tidyverse) # 将 json 数据的每一层（每一个用户）转换成单行数据 row_results &amp;lt;- lapply(result[[2]], function(x) { as_tibble(x) }) # 对单行数据进行合并，得到最终数据 toc_rank_result &amp;lt;- do.call(&amp;#34;rbind&amp;#34;, row_results) # 使用 URLdecode 转换用户昵称 toc_rank_result &amp;lt;- toc_rank_result %&amp;gt;% mutate(nickname = URLdecode(sName)) %&amp;gt;% select(nickname, iTier, iRank, iPoints) # 查看转换后的数据 View(toc_rank_result) 大功告成！可以看到，相比于官网页面上的 60 条数据，这里分析的请求数据总共有 100 条，足足多了 40 条。 而且爬下来的数据想要做更多的查询和分析也更加方便了。 有时候，网页上没有显示的数据，可能还真需要扩展一下思路，看看能不能用其他方法得到。</description>
    </item>
    
    <item>
      <title>【学习笔记】实用统计学复习手册01——探索性数据分析</title>
      <link>https://www.xzywisdili.com/post/2023-03-23-dataanalysisnote01/</link>
      <pubDate>Thu, 23 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.xzywisdili.com/post/2023-03-23-dataanalysisnote01/</guid>
      <description>总结 介绍了数据的分类（数值型、分类型）等； 单变量分析：位置度量（均值、中位数）、变异性度量（方差、标准差、百分位数）等； 单变量的可视化：箱线图、频数表、直方图等； 多变量分析：探索相关性。 多变量的可视化：散点图、热力图、列联表、分类箱线图等； 探索性数据分析（Exploratory Data Analysis，EDA）是数据科学项目的第一步。
数据的分类和主要形式 首先，需要了解数据的分类。在目前世界，数据来源非常丰富，而大多数数据是非结构化的，比如图像、文本、用户交互。这些“信息”，或者说非结构化的数据必须先转化为结构化的数据，才能够用于下一步的分析。
结构化数据的分类：
数值型数据 离散型数据：只能取整数； 连续型数据：在一个区间可以取任意值 分类型数据 二元数据：两个值中取其一，0 或 1 多元数据 特殊形式：有序数据，按照分类进行排序 需要强调的是，数据的分类对于后续可视化方法的选择、统计模型的选择都非常重要。
接下来需要说明的是数据科学中最经典的引用结构——矩形数据。最常见的就是我们经典的电子表格。其中每一行代表一条观测记录，每一列代表一个特征（变量）。在不同统计编程语言中也有对应的处理方式：
R语言: data.table Python Pandas: DataFrame 其他的几种非矩形的数据形式包括：时间序列数据、空间数据和图形或网络数据。他们都有对应的处理方法。
如何描述连续性变量？ 位置度量 变量代表了测量数据或者计数数据。探索数据的一个基本步骤，就是了解每个变量（特征）的特点。不同种类的数据，我们对其的主要关注点也不尽相同。统计学中主要关注以下数据特征值：
均值：基本的位置度量，对极值敏感 加权均值：将每个值乘一个权重值然后除以总和 切尾均值：消除极值之后的均值，比均值更加稳健 中位数：更加稳健，不易受均值影响 加权中位数：将数据集排序之后进行加权，加权中位数就是可以使数据集上下两部分的权重总和相同的值 离群值：并不一定是无效或错误的数据，但往往是由于数据的错误所导致的。 统计学家替换用估计量（estimate）来表示从手头已有数据计算得到的值，来描述数据情况与真实状态之间的差异。数据科学家和商业分析师更倾向于把这些值称为度量（metric）。因为统计学的核心在于如何解释不确定度，而数据科学则更关注如何解决一个具体的商业或企业目标。
变异性度量 变异性（variability），也称为离差（dispersion），是另外一个描述数据的视角，表示数据是紧密聚集的还是发散的。在分析中主要会考虑：
测量数据的变异性； 识别各种变异性的来源； 如何降低变异性 统计学中有以下数据特征描述变异性：
偏差：观测值和估计值之间的直接差异 方差 标准差：方差的平方根 平均绝对偏差：对偏差值取绝对值然后求平均 中位数绝对偏差 极差：最大值和最小值之间的差值 顺序统计量：又称为秩 百分位数 四分位距（IQR）：75 百分位数和 25 百分位数之间的差值 方差和标准偏差是最广泛使用的变异统计量，且都对离群值敏感。更稳健的度量包括百分位数、四分位距和中位数绝对偏差。
sd (state$Population) IQR(state$Population) mad (state$Population) quantile(state$Murder.Rate, p=c(.05, .25, .5, .75, .95)) 使用图表描述数据分布 箱线图 boxplot(state$Population/1000000, ylab=&amp;#34;Population (millions)&amp;#34;) 箱线图的顶部和底部分别是 75 百分位数和 25 百分位数。水平线代表的是中位数，虚线称为须（whisker） ，从最大值一直延伸到最小值，体现了数据的极差。</description>
    </item>
    
    <item>
      <title>【学习笔记】保序回归——适用于单调递增数据的统计回归方法</title>
      <link>https://www.xzywisdili.com/post/2023-03-07-isotonicregression/</link>
      <pubDate>Tue, 07 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.xzywisdili.com/post/2023-03-07-isotonicregression/</guid>
      <description>什么是保序回归？ 保序回归（Isotonic Regression）是一种适用于单调函数非参数统计回归方法，即在一个序列中，Xn ≥ Xn-1。通过字面理解「Isotonic Regression」：
iso的意思就是「相等、相同」的意思； tonic 就是 tone 的意思，指的是「调」。 所以保序回归的核心还是在于单调递增的函数，当需要分析的数据资料符合单调递增的趋势可以用。 保序回归最常用的应用场景之一是探索药量和药效的关系，因为一般来说药物剂量越高，药效应该更强，因此通过保序回归的方式可以从药效和经济学的角度估计最合适的药量。
保序回归使用 weighted least-squares 来进行拟合： 怎么做保序回归？ 求解保序回归的一种最常用算法是 PAVA 算法（ Pool-Adjacent-Violators Algorithm，池相邻违规者算法）。PAVA 算法的直观形式只需要看下面这张图就行了：
这种算法是通过从左往右逐渐扫描数据序列，并且保证整个序列是单调递增的，以此来获得 Beta 值的结果。如果 Beta_i &amp;lt; Beta_i-1，那么就同时把这两个值替换为 (Beta_i + Beta_i-1) / 2。以此就能获得严格且平滑的保序回归。
通过 PAVA 算法，可以获得一个包括多个 Beta 参数组成的单调递增序列，用可视化的方法可以看到是由多条上升线和水平线组成的函数图：
如何使用 R 语言进行保序回归？ 在 R 中可以轻松进行保序回归，只需要使用 stats 包中的 isoreg 函数即可。下面的代码提供一个简单的示例，并将原始数据和拟合值（蓝点）绘制出来。注意，拟合后的蓝点是单调递增的。
# Generate Training Data set.seed(15) x &amp;lt;- sample(2 * 1:15) y &amp;lt;- 0.5 * x + rnorm(length(x)) # Isotonic Regression Model Fit reg.</description>
    </item>
    
  </channel>
</rss>
