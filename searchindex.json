{"categories":[{"title":"Hexo","uri":"https://www.xzywisdili.com/categories/hexo/"},{"title":"LaTeX","uri":"https://www.xzywisdili.com/categories/latex/"},{"title":"Python","uri":"https://www.xzywisdili.com/categories/python/"},{"title":"R 语言","uri":"https://www.xzywisdili.com/categories/r-%E8%AF%AD%E8%A8%80/"},{"title":"R语言","uri":"https://www.xzywisdili.com/categories/r%E8%AF%AD%E8%A8%80/"},{"title":"SAS","uri":"https://www.xzywisdili.com/categories/sas/"},{"title":"分享","uri":"https://www.xzywisdili.com/categories/%E5%88%86%E4%BA%AB/"},{"title":"博客","uri":"https://www.xzywisdili.com/categories/%E5%8D%9A%E5%AE%A2/"},{"title":"可视化","uri":"https://www.xzywisdili.com/categories/%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"title":"学习","uri":"https://www.xzywisdili.com/categories/%E5%AD%A6%E4%B9%A0/"},{"title":"学习笔记","uri":"https://www.xzywisdili.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"title":"影视剧","uri":"https://www.xzywisdili.com/categories/%E5%BD%B1%E8%A7%86%E5%89%A7/"},{"title":"影评","uri":"https://www.xzywisdili.com/categories/%E5%BD%B1%E8%AF%84/"},{"title":"想法","uri":"https://www.xzywisdili.com/categories/%E6%83%B3%E6%B3%95/"},{"title":"新闻","uri":"https://www.xzywisdili.com/categories/%E6%96%B0%E9%97%BB/"},{"title":"机器学习","uri":"https://www.xzywisdili.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"title":"流行病","uri":"https://www.xzywisdili.com/categories/%E6%B5%81%E8%A1%8C%E7%97%85/"},{"title":"流行病学","uri":"https://www.xzywisdili.com/categories/%E6%B5%81%E8%A1%8C%E7%97%85%E5%AD%A6/"},{"title":"统计","uri":"https://www.xzywisdili.com/categories/%E7%BB%9F%E8%AE%A1/"},{"title":"统计学","uri":"https://www.xzywisdili.com/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"},{"title":"翻译","uri":"https://www.xzywisdili.com/categories/%E7%BF%BB%E8%AF%91/"},{"title":"读书笔记","uri":"https://www.xzywisdili.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"title":"阅读","uri":"https://www.xzywisdili.com/categories/%E9%98%85%E8%AF%BB/"},{"title":"随想","uri":"https://www.xzywisdili.com/categories/%E9%9A%8F%E6%83%B3/"}],"posts":[{"content":"总结 通过一则创立 ADSL 数据集的练习示例加深对 ADSL 数据集的理解； ADSL（Subject-Level Analysis Data Set） 是 ADaM 中的一种数据集，特点是每一行是一个单独患者的记录（类似 SDTM 中的 DM 数据集），主要包括的变量有：\nDemographic 相关的变量； 计划和实际的 Treatment 相关变量； 是否为 \u0026ldquo;Safety\u0026rdquo; 的患者标识 flag ADSL 的建立过程中，一般需要从 SDTM Domain 中的数个数据集合并而来：\nDM 中的大部分变量 EX (Exposure)，DS (Disposition) 和其他 SDTM Domain ADSL 的结构 ADSL 的结构是一行记录代表一个患者，包括 Identifier，Demographics, Population flags, treatment, dose 和其他 subject-level 的变量。下面一一进行分析。\nIdentifier 变量 STUDYID, USUBJID, SUBJID, SITEID，在存在于 DM 中； Subject Demographics 变量 AGE：DM.AGE，如果分析需要 derive 一个 age 出来，需要添加 AAGE 变量； AGEU、SEX、RACE：都从 DM 中合并过来； Population Flag 变量 FASFL：全分析数据集的标识 SAFFL：Safety Population 的标识 Dose 变量 TRTSDT、TRTSDTM：分别是 Treatment 中第一次服药的日期和日期时间； TRTxxSDTF：xx一般是两位数字，代表第几个阶段，也就是第几个阶段的TRT； TRTSDTF：第一次服药的标识。 Subject-Level Trial Experience 变量 EOSDT：研究结束的日期 EOSSTT：研究结束的状态，比如取值“COMPLETED” 阅读和熟悉 ADSL 的 Spec 熟悉 ADSL 的 Spec，对于我们初学制作 ADSL 的合并数据集过程非常有帮助。\n可以看到 Spec 里面的各个内容：\nVariable：变量名 Label：每个变量的含义说明，基本上和 SDTM IG 中每个变量的 Label 对应； Type：存储格式，分为 char 和 num 两种，需要仔细对照检查，必要时转换； Length：变量储存长度，Derive 新变量的时候要注意设置； Controlled Terminology（CT），有 CT 要求时严格遵守即可； Notes：需要写明每个变量的来源，如果是 Derive 的，需要写清楚计算过程； ADSL 的程序 第一部分 Derive 以下这几个新变量：\nAETHNIC：如果 ETHINIC 为空，则赋值 \u0026ldquo;NOT COLLECTED\u0026rdquo;; ARACE：转换种族 RACE 变量，方便分析； TRTSDT, TRTSTM：分别是 Treatment 开始时第一次暴露（给药）的日期和时间，注意是以 num 存储并按照规定格式，所以需要 input 转换； TRTEDT，TRTETM：最后一次暴露（给药）的日期和时间，要求同上； TRT01P, TRT01A：分别是计划和实际的治疗方式，按要求转换； SAFFL，FASFL：对应 Safety Population 和 Full Analysis Set 的标识；Safety Population 意指至少经过一次暴露（给药）的人群，所以只要 RFSTDTC 不为空，就代表 \u0026ldquo;Y\u0026rdquo;； 注意：Derive 的新版量最好注意一下 length 的设置；\n/* Begin Writing SAS Program sdtm.dm.sas */ /* Demographic Variables */ data DM1; set sdtm.DM; /* Derive AETHNIC, ARACE */ length AETHNIC $40.; if missing(ETHTIC) then AETHNIC = \u0026quot;NOT COLLECTED\u0026quot;; else AETHNIC = ETHTIC; if RACE=\u0026quot;WHITE\u0026quot; then ARACE=\u0026quot;W\u0026quot;; else if RACE=\u0026quot;BLACK OR AFRICAN AMERICAN\u0026quot; then ARACE=\u0026quot;B\u0026quot;; else if RACE=\u0026quot;NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDERS\u0026quot; then ARACE=\u0026quot;HP\u0026quot;; else if RACE=\u0026quot;ASIAN\u0026quot; then ARACE=\u0026quot;A\u0026quot;; else if RACE=\u0026quot;AMERICAN INDIAN OR ALASKA AMERICAN\u0026quot; then ARACE=\u0026quot;AA\u0026quot;; /* Derive TRTSDT, TRTSTM, TRTEDT, TRTETM */ TRTSDT = input(substr(RFSTDTC, 1, 10), yymmdd10.); TRTSTM = input(substr(RFSTDTC, 12), time5.); TRTEDT = input(substr(RFENDTC, 1, 10), yymmdd10.); TRTETM = input(substr(RFENDTC, 12), time5.); format TRTSDT yymmdd10. TRTSTM time5. TRTEDT yymmdd10. TRTETM time5.; /* Derive TRT01P, TRT01A */ length TRT01P TRT01A $20.; if ARMCD = \u0026quot;DRUG A\u0026quot; then TRT01P = \u0026quot;TRTA\u0026quot;; if ACTARMCD = \u0026quot;DRUG A\u0026quot; then TRT01A = \u0026quot;TRTA\u0026quot;; /* Derive SAFFL, FASFL */ if ^missing(RFSTDTC) then do; SAFFL = \u0026quot;Y\u0026quot;; FASFL = \u0026quot;Y\u0026quot;; end; else do; SAFFL = \u0026quot;N\u0026quot;; FASFL = \u0026quot;N\u0026quot;; end; run; 第二部分 Derive 以下这几个新变量：\nEOSDT 和 EOSSTT：需要用到 DS Domain 的 DSDECOD（表示患者完成试验的状态），DSSTDTC（表示患者对应试验结束状态的日期）。思路是：先把 DM 和 DS merge 起来，再取出 DSDECOD 和 DSSTDTC 进行转换即可； TR01SDTM，TR01SDT，TR01EDTM，TR01EDT 代表治疗中第一次暴露（给药）的日期和时间，以及最后一次的日期和时间； 来源是 EX Domain 中的 EXSTDTC，EXENDTC，思路是要取出每个 USUBJID 下的第一条给药记录中的 EXSTDTC 和最后一条给药记录中的 EXENDTC，再和 DM 进行 merge，转换对应变量即可； /* Merge DS and DM1 to catch DSDECOD, DSSTDTC */ proc sql; create table DM2 as select a.*, b.DSDECOD, DSSTDTC from DM1 a left join sdtm.DS b on a.USUBJID=b.USUBJID; quit; /* Derive EOSDT, EOSSTT variables */ data DM3; set DM2; length EOSDT 8 EOSSTT $20.; EOSDT = input(substr(DSSTDTC, 1, 10), yymmdd10.); if DSDECOD=\u0026quot;COMPLETED\u0026quot; then EOSSTT=\u0026quot;COMPLETED\u0026quot;; else EOSSTT=\u0026quot;DISCONTINUED\u0026quot;; format EOSDT yymmdd10.; run; /* Sort data set SDTM.EX by USUBJID, EXESTDC without duplicate values */ proc sort data=sdtm.EX out=EX nodupkey; by USUBJID EXESTDC EXENDTC; run; data EX1 EX2; set EX; by USUBJID EXSTDTC EXENDTC; /* Create dataset EX1 if the first.USUBJID statement */ /* Cretae dataset EX2 if the last.USUBJID statement */ if first.USUBJID then output EX1; if last.USUBJID then output EX2; run; /* Create data set DM4 and DM5 */ proc sql; create table DM4 as select a.*, b.EXSTDTC from DM3 a left join EX1 b on a.USUBJID=b.USUBJID; create table DM5 as select a.*, b.EXENDTC from DM4 a left join EX2 b on a.USUBJID=b.USUBJID; quit; data FINAL; set DM5; /* Derive TRT01SDTM, TRT01SDT, TRT01EDTM, TRT01EDT */ length TRT01SDTM TRT01SDT TRT01EDTM TRT01EDT 8.; TRT01SDTM = input(EXSTDTC, e8601dt.); TRT01SDT = input(substr(EXSTDTC, 1, 10), yymmdd10.); TRT01EDTM = input(EXENDTC, e8601dt.); TRT01EDT = input(substr(EXENDTC, 1, 10), yymmdd10.); format TRT01SDTM TRT01EDTM e8601dt. TRT01SDT TRT01EDT yymmdd10.; run; 第三部分 主要包括声明 libname、校正所有变量的 label 和 length，以及变量顺序：\nlibname ADAM \u0026quot;.../directory\u0026quot;; data ADAM.ADSL(label=\u0026quot;Subject-Level Analysis Data set\u0026quot;); /*Assign variable attributes such as label and length to conform with ADAM.ADSL Specification (these will also be the same attributes as the ADAM IG). */ attrib STUDYID label=\u0026quot;Study Identifier\u0026quot; length=$20 USUBJID label=\u0026quot;Unique Subject Identifier\u0026quot; length=$20 SUBJID label=\u0026quot;Subject Identifier for the Study\u0026quot; length=$20 SITEID label=\u0026quot;Study Site Identifier\u0026quot; length=$10 BRTHDTC label=\u0026quot;Date/Time of Birth\u0026quot; length=$20 AGE label=\u0026quot;Age\u0026quot; length=8 AGEU label=\u0026quot;Age Units\u0026quot; length=$10 SEX label=\u0026quot;Sex\u0026quot; lenght=$2 RACE label=\u0026quot;Race\u0026quot; length=$100 ARACE label=\u0026quot;Analysis Race\u0026quot; length=$100 ETHNIC label=\u0026quot;Ethnicity\u0026quot; length=$60 AETHNIC label=\u0026quot;Analysis Ethnicity\u0026quot; length=$60 SAFFL label=\u0026quot;Safety Population Flag\u0026quot; length=$1 ARM label=\u0026quot;Description of Planned Arm\u0026quot; length=$200 ARMCD label=\u0026quot;Planned Arm Code\u0026quot; length=$20 ACTARMCD label=\u0026quot;Actual Arm Code\u0026quot; length=$20 ACTARM label=\u0026quot;Description of Actual Arm\u0026quot; length=$200 TRT01P label=\u0026quot;Planned Treatment for Period 1\u0026quot; length=$20 TRT01A label=\u0026quot;Actual Treatment for Period1\u0026quot; length=$20 RFSTDTC label=\u0026quot;Subject Reference Start Date/Time\u0026quot; length=$20 RFENDTC label=\u0026quot;Subject Reference End Date/Time\u0026quot; length=$20 TRTSDT label=\u0026quot;Date of First Exposure to Treatment\u0026quot; length=8 TRTSTM label=\u0026quot;Time of First Exposure to Treatment\u0026quot; length=8 TRT01SDTM label=\u0026quot;Datetime of First Exposure in Period 1\u0026quot; length=8 TRT01SDT label=\u0026quot;Date of First Exposure of Period 1\u0026quot; length=8 TRTEDT label=\u0026quot;Date of Last Exposure to Treatment\u0026quot; length=8 TRTETM label=\u0026quot;Time of Last Exposure to Treatment\u0026quot; length=8 TRT01EDTM label=\u0026quot;Datetime of Last Exposure in Period 1\u0026quot; length=8 TRT01EDT label=\u0026quot;Date of Last Exposure in Period 1\u0026quot; length=8 EOSSTT label=\u0026quot;End of Study Status\u0026quot; length=$20 EOSDT label=\u0026quot;End of Study\u0026quot; length=8 COUNTRY label=\u0026quot;Country\u0026quot; length=$4 ; set FINAL; keep STUDYID USUBJID SUBJID SITEID BRTHDTC AGE AGEU SEX RACE ARACE ETHNIC AETHNIC SAFFL ARM ARMCD ACTARMCD ACTARM TRT01P TRT01A RFSTDTC RFENDTC TRTSDT TRTSTM TRT01SDTM TRT01SDT TRTEDT TRTETM TRT01EDTM TRT01EDT EOSSTT EOSDT COUNTRY ; run; 这样就完成了一个初步练习。\n","id":0,"section":"posts","summary":"总结 通过一则创立 ADSL 数据集的练习示例加深对 ADSL 数据集的理解； ADSL（Subject-Level Analysis Data Set） 是 ADaM 中的一种数据集，特点是每一行是一","tags":["分享","SAS","CDISC"],"title":"ADSL 数据集的建立：练习示例","uri":"https://www.xzywisdili.com/2023/05/2023-05-08-adslexample/","year":"2023"},{"content":"总结 ADaM 标准实为了让临床试验统计分析人员梗快速、标准准确地进行统计分析和制作报表的标准； ADaM 主要分为三类数据集：ADSL，BDS 和 OCCDS； 详细梳理了三类数据集的特点以及对应的重点变量。 ADaM 数据集是做什么用的？ ADaM，Analysis Data Model，顾名思义，为了更加方便、标准且准确地进行统计分析，输出相关统计图表的标准模型。\n临床实验的流程包括：实验设计，得到原始数据 → 经过 SDTM 标准化得到标准数据 → 将 SDTM 标准数据转化为 ADaM 标准数据 → 使用 ADaM 数据集生成统计图表 → 撰写药物上市申请文档。\n在这个工作流程中：\nSDTM 标准更偏向于数据标准化； 不同实验的数据集，结构内容和变量基本大部分相同； 如果直接从 SDTM 数据集创建统计图表，需要增加大量计算语句，非常冗杂并且容易出错； ADaM 标准更偏向于数据的可分析性； 数据集的变量可以根据实验的不同进行调整，更加灵活； 更加方便快捷、标准、准确地编写统计图表生成的语句。 跟 SDTM IG 文件类似，ADaM 也有对应的 IG（Implementation Guide） 文件，在 CDISC 的官网就可以找到。\nADaM 有五大基础原则 清晰且无歧义，Clearly adn unambiguously，有标准则需要与标准一致，如果没有标准就需要描述清晰； 可追溯性，Traceability，每个变量，每条记录的来源，都必须能从对应的 SDTM 数据集中找到； 能够被常用的统计软件 SAS 使用； 必须具备 metadata 元数据，即必须把 ADaM 中每个变量的意义放在一个元数据文件中； 即分析性 analysis-ready，使用最少的编程工作就可以完成分析任务；ADaM 数据可以只包含分析所需要的数据，如果收集了一些原始数据并不会用于分析中，那么就不需要纳入 ADaM 数据集中。 可追溯性 可追溯性非常重要，它要求所有数据有依可查，而不是凭空产生。这样可以方便分析人员和审阅者查看数据的产生过程，排除可能存在的异常数据。可追溯性分为以下两点：\nMetadata 层面：ADaM 中的变量，如果是在 ADaM 编程过程中创建出来的，必须通过说明文档，说明计算产生过程； Datapoint 层面：ADaM 中的某条记录，可以知道是从 SDTM 中的哪条记录而来的。常用的方法就是保留 SDTM 中的 SEQ 变量。 变量的存在度 和 SDTM 相同，ADaM 数据集标准对变量也有存在度的要求。回忆一下，SDTM 中的三种分别是 REQ 必须存在, EXP 期望存在, PERM 允许存在；而 ADaM 中的三种分别是：\nRequired：必须存在； Conditionally Required：条件性必须，即在某些特定条件下这些变量的存在是必须的； Permissible：允许存在； 重要的数据结构 ADSL、BDS 和 OCCDS ADaM 标准中有以下这三种类型的数据结构，且这三种数据集是互斥关系的，也就是说每个 ADaM 数据集能且只能属于他们三个其中的一类。从实际情况来说，BDS 结构最多。\n如果一个 SDTM 数据集中的数据需要用于统计分析，就需要创建对应的 ADaM 数据集，命名方法是字母 AD 加上所用的主要 SDTM 数据集的名称，举例：\nLB → ADLB VS → ADVS ADSL，全称 Subject Level Analysis Dataset\n代表每个患者一条记录的记录结构（类似 SDTM 中的 DM 数据集） 一般都将 DM 数据集先 set 进来，再增加其他变量； ADSL 结构的数据集在一个实验中只有一个，名称就叫 ADSL。 OCCDS，全称 Occurrence Data\n发生数据，常常是指 SDTM 标准下的 Events 结构类的数据； 把发生的事件按照事件的层级列举出来，比如副作用 AE 数据集中，需要 AELLT AEHLT AEHLGT 和 AESOC 来表示副作用所属的最高层级，而经过 ADaM 处理后依然需要保留； OCCDS 的 IG 文件是在压缩包中单独列出来的； BDS，全称 Basic Data Structure\n基础数据结构 一般会将 SDTM 标准下的 Findings 和 Findings About 的数据生成 BDS 结构的数据集； 基本要求：每个患者，每个参数，每个时间点，一条记录，举例：和 SDTM 下的 LB 数据集结构类似。 ADSL 的变量梳理 下面进行 ADSL 重点变量的梳理：\nSTUDYID, USUBJID, SUBJID, SITEID，都是之前很熟悉的变量了； SITEGRy，SITEGRyN IG 文件中，变量名后面所跟的小写字母代表实际中的数字，比如 SITEGR1, SITEGR2； 多中心的临床实验，可能部分中心（SITE）可以分类到一个组中，就用 SITEGRy 作为分组变量，而 SITEGRyN 作为分组序号方便排序； AGE, AGEU：年龄相关，不赘述； AGEGRy，AGEGRyN：年龄分组 SEX，RACE：DM 提到的变量，不赘述； FLAG 相关变量，用 Y 或者 N 来表示是否成立，最重要的是： SAFFL：如果这名患者，至少吃过一次药，就叫做 safety population，这时 SAFFL 取值为 Y； ITTFL：Inten-To-Treat Population，是否有意向参加这个实验 和实验 TRAETMENT 相关的变量 ARM，ACTARM：DM 里的分组； TRTxxP，TRTxxPN：计划中这名患者所参与的临床实验的分组；使用 xx 可以作为分时间段，来记录患者所参与的分组，常见于 crossover-design 的实验设计； TRTxxA，TRTxxAN：实际中患者所参与的实验分组 和实验时间 TREATMENT TIMING 相关的变量 TRTSDT：实验的开始日期，数值型 TRTSTM：实验的开始时间，数值型 TRTSDTM：实验开始的日期和时间，数值型 TRTSDTF, TRTSTMF：如果实验的开始日期和时间是填补补充的，需要标示出来； TRTEDT, TRTETM, TRTEDTM：最后一次参与实验暴露的日期和时间； TRxxSDT：第一段、第二段等多段实验的截止日期，如果只有一段，不需要这个变量； 整个研究的时间范围是从患者签署同意意向书到最后一次随访，而 TREATMENT 的全程是从第一次服药（暴露）到最后一次服药（暴露）的时间段。这个时间段要比前者短。\nEOSSTT：表明一个患者在结束研究的时候是什么状态，包括 COMPLETED，DISCONTINUED，ONGOING EOSDT：患者什么时候结束的研究； DCSREAS：什么原因，不再参与这个实验；如果已完成实验，就标记为 Null； EOTSTT：表明一个患者在结束 TREATMENT ，停止吃药是什么状态； DCTREAS：这名患者为什么停止接受 TREATMENT（停止吃药）。 DTHDT：患者死亡的日期； BDS 的变量梳理 BDS，Basic Data Structure，重要的变量比 ADSL 要少一些。\n通用变量：STUDYID, USUBJID, SUBJID, SITEID ASEQ：相当于 SDTM 中的 SEQ 变量一样，做完 BDS 数据集后，在每一个患者 ID 之内，从 1 开始依次累加； TRTP，TRTA：实验设计分组和实际分组； ADT，ATM，ADTM，ADY：检测日期和时间； ASTDT，ASTMM，ASTDTM，ASTDY：检测开始的日期和时间； AENDT，AENDM，AENDTM，AENDY：检测结束的日期和时间； AVISIT，AVISITN：就是 Findings 数据集的 VISIT 和 VISITNUM，表示这是患者的第几次来访； ATPT，ATPTN：Analysis timepoint，如果一次来访需要做多次检测，那么 AVISIT 和 AVISITN 的数据是一样的，但是 Timepoint 不同； PARAM，PARAMCD：参数，对应 TEST 变量，即检测的具体内容，形式往往是“测试名称（标准单位）”； PARAMN：对 PARAM 进行标号，方便排序； PARCATy，PARCATyN：类别，对应 LBCAT； AVAL，AVALC：代表测量的结果，LBSTRESN 和 LBSTRESC； 如果测量结果不是数字，而是一串字符，就可以只存储在 AVALC 中，AVAL 留空。 BASE 和 BASEC：代表基准线值，和 SDTM 储存方式不同（后者通过 flag 存储）； CHG：测量值减去基准值； PCHG：percent change from baseline，和基准值相比的百分比变化；如果基准值是0，PCHG 设为缺失值； SHIFTy，SHIFTyN：表示从基准线值到现在测量值的状态对比，比如 low to high 或者 normal to high，类似这种情况；这两个变量为一对一关系； CRITy，CRITyFL：用用户自定义的某些标准和特点，如果符合这一特点，就把 CRITyFL 变量设置为 y，CRIT 变量则要写清楚是什么标准； DTYPE：如果变量不在原来的变量中，而是新生成的，按照什么方法生成的，要用 DTYPE 简明地讲清楚； ANRIND，BNRIND：分别是当前测量值和基准线测量值时，测量结果是高或者低，可以用这两个变量制作出 SHIFT 和 SHIFTN 变量； ABLFL：Baseline Record Flag，如果纳入了 SDTM 数据集中没有的新纪录，就需要我们自己手动创建 ABLFL 标识基准线值； ANLzzFL：分析数据集，如果这条记录符合某一个要求，就可以标记为 Y； SRCDOM，SRCVAR，SRCSEQ 表示 datapoints 级别的可追溯性 SRCDOM：数据是从哪个 SDTM 数据集来的； SRCVAR：从哪个变量来的； SRCSEQ：从哪个记录来的。 OCCDS 的变量梳理 OCCDS 应用的场景并不多，一般只用在 AE 的副作用数据集中。\n学到这里，头已经有一点点晕了，容我稍微缓一缓……\n","id":1,"section":"posts","summary":"总结 ADaM 标准实为了让临床试验统计分析人员梗快速、标准准确地进行统计分析和制作报表的标准； ADaM 主要分为三类数据集：ADSL，BDS 和 OCCDS； 详","tags":["分享","SAS","CDISC"],"title":"CDISC 标准（五）——ADaM 标准概述","uri":"https://www.xzywisdili.com/2023/05/2023-05-02-cdiscnotes5/","year":"2023"},{"content":"总结 本笔记的主要内容：\n介绍 SDTM 标准下的七个大类别，以及主要包括的数据集； 梳理两个最重要的数据集：AE（副作用数据集）和 LB（实验室检测结果数据集）。 SDTM 的全称是 SDTM Standard dataset tabulation model，即实验数据标准化的模型，在之前的笔记中有一个概述的介绍。SDTM 标准力图能够尽可能覆盖在临床实验进行过程中所需要收集到的所有数据和相关信息，因此也详细地设计了各种不同分类的大类别和数据集。\n在这个分类下，总共包括七个大类别和 44 个数据集。每一种数据集都属于唯一一个类别。七个大类别“SIFEFTR”：\n特殊目的类：Special Purpose 干涉类：Intervention 发现类：Findings 发现相关类：Findings About 事件类：Events 实验设计类：Study Trial 关系类：Relationship 下面分门别类介绍每一种类型。\n特殊目的类 Special Purpose 特殊目的类：包括和实验参与者本人相关的数据，无法归类到其他类别。主要包括以下几个数据集：\nCO：评价数据集，Comments**，医生手写或者补充的文本备注和记录，需要与其他数据集相连接；如果手写的记录超过 CDISC 规定的最长值长度 200 个字符，就需要创建新的 COVAL1，COVAL2 变量保存字符，以此类推，每个变量容纳 200 个字符； DM：人口统计学，Demographics，用于保存患者的基础信息，比如患者编号、性别、国籍、种族、实验开始时间等等，在之前的内容中有所记录； SV：患者来访数据，Subject Visit SE，患者时期元素，Subject Element 一个患者在整个临床实验过程中需要来访多次。一般是从 Screen 和纳入开始，之后的来访依次是基线，第一次来访，第二次来访等，直到实验结束。在服用药物过后，因为药物在人体内有残留期，也需要几次患者来访，来观察是否具有副作用和其他治疗效果。 患者来访的时间数据都需要记录在 SV 数据集中。每个患者在实验过程中经历的每个时期和元素的时间信息，这些数据被放在 SE 中。 干涉类 Interventions 干涉类主要包括对患者有影响的行为的相关数据，包括以下几个数据集：\nCM：伴随用药 Concomitant and prior medications 参加临床实验的患者，如果有其他疾病，需要服用已经上市的药物，就需要在数据集中记录伴随用药的相关信息；如果是和癌症相关的临床实验，可能需要让受试者服用一种统计的抗癌药物，再服用待实验的药物，那么这个统一服用的药物也需要放置在 CM 数据集中； CM 数据集存放患者服用的其他药物（非本实验药物）的名称、类别、剂量、计量单位、频率、药物类型（胶囊/饮剂）等数据； EX：实验用药 Exposure 实验计划中药物的使用情况； 根据实验设计来确定，核心变量包括 EXTRT（药物的名称），EXDOSE（药物的剂量），EXDOSU（药物剂量的单位），EXDOSFRQ（药物按此剂量服用的频率） 和 EXDOSFRM（药物的形式，液体/胶囊/含服片）； EC：实际实验用药 Exposure as Collected 实际中**药物的使用情况 由 ECOCCUR 变量指定该次是否服药，ECDOSE 记录剂量，ECDOSU 记录剂量单位； EC 和 EX 数据集的结构基本相同，实际分析使用 EX 数据集，因为根据实验设计而来的 PR：过程 Procedures 患者治疗和诊断相关的过程 比如患者参与心血管相关的临床实验，中间去口腔科诊断并拔了智齿，这个过程无法判断是否与临床实验相关，所以也要记录下来。 核心变量：PRTRT，记录做的手术名称 PRSTDTC，PRENDTC，PRSTDY，PRENDY 分别对应手术开始的日期和结束的日期，以及对应的天数； SU：物质使用 Substance Use 其他非药物的物质的使用情况，比如最常见的咖啡（含有咖啡因），吸烟、喝酒； 主要包括以下几个变量：SUTRT（使用物质的名称，包括 CIGARETTES，COFFEE）、SUCAT（使用物质所属的大类）、SUDOSE（这些物质使用的数量）、SUDOSEU（使用物质的数量单位）、SUDOSFRQ（使用物质的频率）、SUSTAT，SUREASND（数值没有被记录和没有记录的原因） 事件类 Events 事件类，代表计划之外的事件，总共包括 6 个数据集：\nAE：副作用，Adverse Events CE：临床事件，Clinical Events DS：处置，Disposition DV：实验计划偏差，Protocol Deviations HO：医疗事件，Healthcare Encounters MH：既往药史，Medical History 其中最最重要的是 AE，也就是副作用数据集。\nAE 的主要变量包括四种类型：\n第一类：副作用的名称和类别 AE 中，每个副作用不仅要求记录它的名称，还需要记录所属的更高级身体组织和系统。基本上原始数据集会提供这些数据，如果需要查询，每个副作用的详细分类信息存在一个名为 MedDRA 的文档之中； AETERM（副作用名称）、AEDECOD（副作用标准名称） 第二类：属性类变量 相当于给每条副作用的属性进行记录，比如 AESEV（副作用严重程度），取值为 3 种，由轻到重分别是 MILD，MODERATE 和 SEVERE； AEACN（副作用发生后采取的行动，包括药量减少、暂时停药、退出实验等）、AEOUT（副作用的结果，包括解决中、已解决、甚至患者死亡）、AEREL（副作用是否与药物相关）； 第三类：评价类变量 往往用 Flag 来表示是否满足标量所描述的情况，Y 表示符合，N 表示不符合； 比如 AESCONG 表示是否导致了胎儿先天性异常和缺陷；AESDISAB 表示是否导致了残疾，AESDTH 表示是否导致死亡，AESHOSP 表示是否导致住院等； 第四类：时间类变量 AESTDTC 和 AEENDTC 表示副作用开始和结束的时间； 对应的 DY 类变量：AESTDY 和 AEENDY； AEDUR 表示副作用持续时间的变量。 发现类 Findings 发现类中包括最多数量的数据集。其中，大部分数据集只用于特殊疾病或者药物中，真正常用的发现类数据集如下：\nEG：心电图测试指标**，ECG Test Results IE：每名患者纳入排除标准的情况，Inclusion/Exclusion Creterion Not Met **LB：血检尿检指标 Laboratory Test Results QS：调查问卷 Questionnaires**，在涉及生活质量的实验当中，QS 往往是最重要的数据集之一； **VS：记录人体的一些基本信息 Vital Signs，包括身高、体重、脉搏、血压等。 而关于癌症的发现类数据集有下列几项：\nTU：Tumor Identification TR：Tumor Results RS：Disease Response 发现类数据集一般有如下通用的结构，其中 XX 表示对应的数据集名字：\n检测的名称：XXTEST； 检测名称的标准缩写：XXTESTCD，CDISC 中规定或者药厂自行规定； 检测分类：XXCAT，XXSCAT； 检测结果和对应的单位：XXORRES，XXORRESU； 检测的标准化结果（取国际通用单位）：XXSTRESC：字符型结果，XXSTRESN：数值型结果，XXSTRESU：标准储存单位； 患者来访的名称和次数：VISIT，VISITNUM（没有前缀）； 时间类型变量：XXDTC/DY（检测时间和距离实验开始天数），比如 LBDTC 代表记录血检/尿检的检测时间，QSDTC 代表记录问卷填写的检测时间。 发现相关类数据集 Findings About 也算是发现类的一种类型，之所以叫发现相关类是因为收集的数据不像发现类那么标准，主要包括以下两类：\nFA：Findings About，除了 FATEST 以外还有一个 FAOBJ，表示实验的对象 SR：Skin Response 实验设计类 Trial design 实验设计类记录实验设计相关的内容。这些数据一般都是从统计分析计划的 SAP 中直接获得，查看这些数据集可以帮我们理解实验是如何设计的。主要包括如下 6 个数据集：\nTA：实验分组 Trial Arms TD：实验疾病数据 Trial Disease Assessment TE：实验时期元素 Trial Elements TV：实验来访计划 Trial Visits TI：实验纳入排除标准 Trial Inclusion / Exclusion Criteria TS：实验总结 Trial Summary Relationship 关系类 主要包括以下 2 个数据集：\nRELREC：关系数据，比如一个患者的副作用被医生发现是由于伴随用药表引起的，那么应该有一个记录将副作用和伴随用药记录连接起来。 SUPP：补充数据集，之前讲过，可以用于补充任何一个标准 SDTM 数据集，命名为 SUPP + XX（关联的数据集名字）； 其他内容 SUPP 补充数据集 每一个 SDTM 数据集都可以用一个补充数据集，用来存放与主数据集相关的数据；\n名称是 SUPP + XX（原始数据集名字），比如 LB 对应的就是 SUPPLB； 举例： SUPPDM 中放置实验人口的 FLAG（包括 ITT，PPROT）; SUPPAE 放置是否是实验期间副作用和非实验期间副作用的标识 TRTEM； SUPP 数据集下面总共有 10 个变量：\nSTUDYID，USUBJID：通用变量，不赘述； RDOMAIN：对应的主数据集，举例，如果对应的是 AE，RDOMAIN 值为 AE； IDVAR，IDVARVAL：在每名患者有多条记录的时候使用，需要指出对应主数据集的哪条记录，比如如果需要对应 AE 中的 AESEQ，那么 IDVAR 的值就是 AESEQ，IDVARVAL 的值就是对应的记录条数（但需要注意转化为字符型）； QNAM：描述一下所定义的值是什么意思，但只能使用不长于 8 位数的字符； QLABEL：不超过 40 个字符来描述这个值是什么意思； QVAL：具体的数据值是多少； QORIG：这个值是如何获取的，比如 CRF（原始数据收集）、ASSIGNED（根据一些条件直接赋值）、DERIVED（通过计算和逻辑分类得到的）； QEVAL：Evaluator，表示谁来完成的这次变量； 实战创建一个补充数据集 SUPPAE，以便加深理解：\ndata ae; set data.ae_trtem; if input(rfstdtc, yyddmm10.) \u0026lt;= input(substr(aestdtc, 1, 10), yymmdd10.) \u0026lt;= input(rfendtc, yymmdd10.) + 14 then trtem='Y'; else trtem='N'; run; options validvarname=upcase; data suppae; retain studyid rdomain usubjid idvar idvarval qnam qlabel qval qorig qeval; set ae; rdomain = 'AE'; idvar = 'AESEQ'; idvarval = strip(put(aeseq, best.)); qnam = 'TRTEM'; qlabel = 'Treatment-Emergent Adverse Event'; qval = trtem; qorig = 'DERIVED'; qeval = ''; keep studyid usubjid rdomain idvar idvarval qnam qlabel qval qorig qeval; run; 自定义数据集 CUSTOM DOMAIN 自定义数据集。因为临床试验毕竟有一些不同，为了保留自由度，SDTM 提供了自定义数据集允许创建一些需要定义的数据集。\n在创建自定义数据集时，需要遵守以下步骤：\n仔细阅读 SDTM IG 文件，查看是否自己想要使用的变量内容确实无法归类到任何一个已经提供的标准数据集之中； 确定自己的数据集类别，是干涉类，发现类还是事件类，给自定义数据集搭建一个基本的框架； 找到每个类别中的几个数据集，找到共同存在的 required 变量，看看自己的数据如何放置在这些变量之中；如果实在无法容纳，就添加额外的变量。比如 LBFAST 变量表示是否空腹，如果你的数据集中也有需要表示是否空腹的变量，也需要使用 XXFAST 作为变量名。 同时在创建自定义数据集需要注意：\n数据集命名以 XYZ 开头+两位大写字母，可以保证在所有标准数据集之后； 部分变量的命名依然需要遵守标准，比如 STUDYID 和 USUBJID； 不同分类的数据集的变量命名规则不同，比如干涉类里干涉的名称放在 TRT 变量中、事件类里事件名称放置在 TERM 和 DECODE 这两个变量中，发现类所有的测试名放在 TEST 变量中，测量值和单位放在 ORRES 和 ORRESU 中。 控制术语 Controlled Terminology 控制术语（Controlled Terminology） 是对 SDTM 里某些特定变量的值进行规范的一套规定。在查看 SDTM IG 的时候，经常会看到 Controlled Terms 这一项中有一些不为空的部分：\n这就表示这个变量的值并不是我们想写什么就能写什么，必须按照给定的规范进行赋值。在 SDTM IG 文件中点击这些蓝色的链接，再结合搜索，就可以查看详细的规定：\n","id":2,"section":"posts","summary":"总结 本笔记的主要内容： 介绍 SDTM 标准下的七个大类别，以及主要包括的数据集； 梳理两个最重要的数据集：AE（副作用数据集）和 LB（实验室检测结果数据","tags":["分享","SAS"],"title":"CDISC 标准（四）——SDTM 数据集详细分类","uri":"https://www.xzywisdili.com/2023/05/2023-05-02-cdiscnotes4/","year":"2023"},{"content":"进行变量类型转换，用 input 还是 put 在 SAS 编程过程中，往往会遇到需要转换变量的需求。而转换变量，往往需要考虑使用 INPUT 还是 PUT。那么这时候就要想想几个变量转换中的几个关键点：\n源变量是字符型还是数值型？ 如果源变量是字符型，那么值是字符还是数字？ 想要转换后的变量是字符型还是数值型？ 那么，根据这三个问题的答案，就可以推出是使用 INPUT 还是 PUT，他们分别有以下几个特点：\nPUT() 永远创建字符型变量； PUT() 中的格式参数必须和源变量类型匹配； INPUT() 的源变量必须为字符型变量； INPUT() 可以根据指定的格式，选择创建字符型或者数值型变量。 下面是几个例子：\n需求 代码 源变量类型 源变量值 返回变量类型 返回变量值 PUT 将字符变量转换为字符变量 PUT(name, $10.); 字符型 \u0026lsquo;Richard\u0026rsquo; 永远是字符型 \u0026lsquo;Richard ' PUT 将数值变量转换为字符变量 PUT(age, 4.); 数值型 30 永远是字符型 \u0026rsquo; 30\u0026rsquo; PUT 将自定义格式转换为字符变量 PUT(name, $nickname.); 字符型 \u0026lsquo;Richard\u0026rsquo; 永远是字符型 \u0026lsquo;Rick\u0026rsquo; INPUT 将值为数字的字符型变量转换为数值变量 INPUT(agechar, 4.); 永远是字符型 \u0026lsquo;30\u0026rsquo; 数值型 30 INPUT 将值为数字的字符型变量转换为字符变量 INPUT(agechar, $4.); 永远是字符型 \u0026lsquo;30\u0026rsquo; 字符型 \u0026rsquo; 30' 都用于分类变量：class 与 by 的辨析 SAS中的 BY 语句和 CLASS 语句都允许指定一个或多个分类变量。主要区别在于：\nBY 语句进行多次计算分析，每次计算针对数据的一个子集； CLASS 语句会将分组变量包含在分析中，进行一次计算输出结果。 BY 语句会重复对每个子集进行分析，如果输出的结果以表格和图形显示的话，BY 进行的分组分析会输出相同格式的 N 个表格和图形，第一组是一号结果表格，第二组是二号结果表格，以此类推。\nCLASS 语句则会将分类变量作为分析的一部分。通常，用于比较各组中，比如 t 检验或 ANOVA 分析。在回归模型中，CLASS 语句可以估计分类变量级别的参数，从而估计每个分类对结局变量的影响。\n举例，我们根据 Sashelp.Cars 数据来进行统计分析，进行对比：\ndata Cars; set Sashelp.Cars; where cylinders in (4,6,8) and type ^= 'Hybrid'; run; proc sort data=Cars out=CarsSorted; by Origin; run; 接下来使用 PROC MEANS 进行分组的描述统计：\n* 使用 CLASS 语句; proc means data=Cars N Mean Std; class Origin; var Horsepower Weight Mpg_Highway; run; * 使用 BY 语句; proc means data=CarsSorted N Mean Std; by Origin; var Horsepower Weight Mpg_Highway; run; 此外，在统计分析的 PROC 中，往往使用 CLASS 语句，可以适应更多类型的模型，比如分析交互影响。\n","id":3,"section":"posts","summary":"进行变量类型转换，用 input 还是 put 在 SAS 编程过程中，往往会遇到需要转换变量的需求。而转换变量，往往需要考虑使用 INPUT 还是 PUT。那么这时候就要想想几个变","tags":["分享","SAS"],"title":"SAS 几个重要概念辨析","uri":"https://www.xzywisdili.com/2023/05/2023-05-01-sasnotes1/","year":"2023"},{"content":"总结 第一次尝试仔细阅读 SDTM IG 文件中的一个 domain 变量信息； 通过 SAS 代码初试按照 SDTM 标准输出标准化的数据集。 本章要开始使用 SAS，并结合 SDTM 标准，看看如何让目标数据集符合 IG 文件里面的标准。\n使用的目标 Domain 是 IG 文件 64-65 页介绍的，最重要的 Domain 之一—— DM Domain，具体变量说明表如下：\nDM 是 Demographics 的缩写，代表人口统计学信息。所有人类参与的临床实验都需要 DM Domain，内容包括实验参与者的年龄、性别、种族、国籍、实验分组、实验开始日期和结束日期等等等等数据。\nDM 数据集往往是接手某个项目最先阅读的数据集，可以查看和了解参与实验的患者的所有信息。在临床实验中，有几个患者参与，那么 DM 数据集中就需要包括几条观测数据。比如，有 10 名患者参与了临床实验，收集了原始数据，那么 DM Domain 中就应该包括 5 条观测。\n接下来就是初学 SDTM 标准的学习方法：仔细阅读 SDTM IG 文件中对于每一条变量的说明，并且按照对应的要求，在 SAS 中处理数据集。处理过程中，需要注意以下几个重点：\n注意每一条变量的 Type，字符型或者数值型，一定要在 SAS 中按照对应的格式存储，不符合的需要进行转换（INPUT 或者 PUT）； 日期型的变量一定要注意按照规定的 ISO8601 格式； 变量顺序和变量的 Label 要按照表中规定的要求； 多多检查。 从 SDTM IG 表慢慢看起：\nSTUDYID：实验的编号，每个临床实验都有的唯一编号； DOMAIN：固定取值 DM； USUBJID：患者的唯一编号，Notes 里写明了格式为 STUDYID-SITEID-SUBJID； RFSTDTC、RFENDTC：实验开始和结束的时间；在一些药物实验中，往往将患者第一次吃药作为实验开始的时间，将患者最后一次吃药为实验结束的时间； RFICDTC：介绍里是“date/time of informed consent”，代表患者正式签署或者参与临床实验协议的时间，往往也是收集到的第一个日期； RFPENDTC：参与的最后一个日期，往往是实验结束后跟踪随访的最后一次日期；如果患者出现失联或者去世等失访的情况，那就是失访前最后一次联系的日期； DTHDTC、DTHFL：和患者死亡相关，如果有死亡的患者，记录死亡的日期和 FLAG = Y； SITEID：研究站点/中心的编号； INVID、INVNAM：调查研究者的编号和名称； BIRTHDTC：患者的出生日期； AGE、AGEU：患者的年龄和年龄的单位（一般取值 YEARS）； SEX 为性别，取值为 M 或 F； RACE、ETHIC：种族 / 是否为拉丁裔或者西班牙裔； ARMCD、ARM：计划的患者分组以及对应的描述； ACTARMCD、ACTARM：实际的实验分组以及对应的描述，根据实验要求可能不一样，需要仔细阅读 SAP 来决定； COUNTRY：国籍，要求里写明要遵守国际标准 ISO 3166-1，比如中国采用名称 CHN； DMDTC、DMDY：DM 数据是在什么日期收集的，距离实验开始有多少天。 接下来给出部分的进行 SDTM 标准的 SAS 代码：\nlibname learn \u0026quot;D:\\CDISC标准\\raw_data_files\\\u0026quot;; * 完成 STUDYID, DOMAIN, USUBJID, SUBJID 的设置; data dm1; set learn.dmmr; length studyid $10.; studyid = 'XYZ-001'; domain = 'DM'; usubjid = strip(studyid) || '-' || strip(subject); subjid = subject; run; * 完成 RFSTDTC RFENDTC 的设置; data ex; set learn.exto; exdtc = put(input(compress(reportdt), date9.), yymmdd10.); run; proc sort data=ex; by subject exdtc; run; data ex1 ex2; set ex; by subject; if first.subject then output ex1; if last.subject then output ex2; run; data dm2; merge dm1 ex1(keep=subject exdtc rename=(exdtc=rfstdtc)) ex2(keep=subject exdtc rename=(exdtc=rfendtc)); by subject; run; * 完成 DTHDTC, DTHFL 的设定; data dth1; set learn.dth; dthdtc = put(input(compress(dthdat_raw), date9.), yymmdd10.); dthfl = 'Y'; keep subject dthdtc dthfl; run; * 完成 SITEID, BRTHDTC, AGE, AGEU, SEX, RACE, ARM, ARMCD, ACTARM, ACTARMCD, COUNTRY 的设定; data dm3; merge dm2(drop=siteid) dth1; by subject; *siteid = substr(subject, 1, 4); siteid = scan(subject, 1, '-'); brthdtc = strip(put(brthdat_yyyy, best.)) ||'-'|| strip(put(brthdat_mm, z2.)) ||'-'|| strip(put(brthdat_dd, z2.)); age =.;age_raw=''; age = int((input(rfstdtc, yymmdd10.)-input(brthdtc, yymmdd10.)) / 365.25); ageu = 'YEARS'; sex = substr(sex, 1, 1); length race $50; if american_indian = 1 then race = 'American Indian'; if asian=1 then race = 'Asian'; if black = 1 then race = 'Black'; if naticve_hawaiian = 1 then race = 'Native Hawaiian'; if white = 1 then race = 'White'; if other = 1 then race = 'Other'; length arm $20 armcd $8 actarm $20 actarmcd $8; if substr(siteid, 1, 1) eq '1' then do; arm = 'Treatment'; armcd = 'TRT'; end; else if substr(siteid, 1, 1) eq '3' then do; arm = 'Placebo'; armcd = 'PLA'; end; actarm = arm; actarmcd = armcd; COUNTRY = 'CHN'; run; * 规范 VARIABLE LABEL 以及变量顺序; data dm4; retain studyid domain usubjid subjid ......; set dm3; label studyid=\u0026quot;Study Identifier\u0026quot;; domain=\u0026quot;Domain Abbreviation\u0026quot;; ...... ","id":4,"section":"posts","summary":"总结 第一次尝试仔细阅读 SDTM IG 文件中的一个 domain 变量信息； 通过 SAS 代码初试按照 SDTM 标准输出标准化的数据集。 本章要开始使用 SAS，并结合 SDTM 标准，看看如何让","tags":["分享","SAS","CDISC"],"title":"CDISC 标准（三）——SDTM 标准初试","uri":"https://www.xzywisdili.com/2023/04/2023-04-30-cdiscnote3/","year":"2023"},{"content":"总结 介绍了 SDTM 和 IG 文件的基本概念； 通过阅读 SDTM IG 文件，了解 Domain 和 Variable 的各种信息； 重点是 Variable 的五种分类：Identifier, Topic, Qualifier, Timing, Rule。 SDTM 的基本概念 SDTM（Study Data Tabulation Model）是一个服务于临床实验的标准数据指标模型，也是被业界和 FDA 广泛采用的标准。它规定了在临床实验中，原始数据收集之后的标准化的呈现方式。可能不同种类的药物，在实验中收集的数据样式是不一样的，但是经过 SDTM 标准化之后，相同类别的数据一定是相同的。\n标准化的好处是，可以更好地服务于药物开发全链条中的各方人员，大大降低了沟通成本，提高审查部门的审核效率。经过多年的数次更新，SDTM 标准已经几乎可以涵盖所有类型的临床实验数据格式。\n而 SDTM IG 文件（SDTM Implementation Guide）中就详细说明了所有 SDTM 标准化数据的方方面面的信息。想要了解 SDTM，就必须要从阅读 SDTM IG 文件开始。\n从阅读 SDTM IG 文件开始 SDTM IG 文件可以从 CDISC 组织的官网中免费获得。官网下载得到的是英文版本，虽然民间存在个人翻译的中文版本，但还是推荐采用原汁原味的英文版本进行阅读。同时，官网还提供了 PDF 和 HTML 的多种格式，方便阅读。\nDomain 域 在第二章 Fundamentals of the SDTM 中，向我们介绍了 SDTM 标准中的基础元素：Domain（域）。每个 Domain 可以理解为围绕一个主题，相关的所有观测和变量组成的数据集，其中有一些基础的通用要求：\n每个 Domain 的名称都用两个大写字母表示； 举例：记录人口统计学的 Domain 名称为 DM，记录副作用的 Domain 名称为 AE； 每个 Domain 包括若干变量，绝大多数变量名称都以 Domain 的名称开头； 举例：副作用的 Domain AE 下的大部分变量以 AE 作为开头。 Variable 变量信息 从 IG 文件中可以看到，每个 Domain 中都存在若干变量，并且有一个变量说明表，从左到右依次是变量名称，变量标签，存储的数据类型，取值范围，变量类型，变量意义说明，变量存在度要求。接下来对这些基本概念进行介绍，就可以基本看懂不同 Domain 的变量表。\n变量也有几个的通用要求：\n每个变量名称（Variable Name）总长度是 8 位； 每个变量有标准化的标签（Variable Label），长度是 40 位； 可以看到 Type 列，表示变量的存储类型要求，在 SAS 中只有两种存储类型：Char 代表字符型，Num 代表数值型。 SDTM 标准下的变量可以分为 5 个大类，可以看到上图中的 Role 那一列：\nIdentifier：用于标识和编号，比如研究 ID，患者 ID 等，举例 STYDYID（实验编号），USUBJID（患者唯一标识 ID 号码）； Topic：上面提过，一个 Domain 的核心内容，举例 AE Domain 中的 Topic 就是副作用相关变量； Qualifier：修饰作用，对于 Topic 内容的补充说明，也包括以下几类： Grouping Qualifier：表示分类的说明，举例 LB Domain（实验室检测域）中的 LBCAT 代表检测类别（血检 / 尿检）； Synonym Qualifier：对 TOPIC 的一个修饰，举例 LB Domain 中的 LBTEST 表示对检测名称的说明； Record Qualifier：对某条记录的说明，举例 LB Domain 中的 LBBLFL 代表这条记录是否是基准线值（如果为 Y 代表是基准线值）； Result Qualifier：表示结果的内容，举例 LB Domain 中的 LBORRES 代表检测结果； Variable Qualifier：对某些变量的修饰，举例 LB Domain 中的 LBORRESU 代表检测结果的单位； Timing：表示日期和时间相关的变量，字符型变量格式，必须按照 ISO8601 的格式：yyyy-mm-ddThh:mm:ss； 每一个日期都必须对应一个从实验开始计算的时间差，用数值型存储，举例 AE Domain 中 AESTDTC 表示副作用开始的时间，AESTDY 就是副作用在实验开始后的第几天产生； Rule：规则相关的变量，主要放在实验设计相关的 Domain 中。 另外，IG 文件中变量表的最后一列 Core 代表对变量存在度的要求，分为以下三类：\nReq：代表 Required，指这个变量必须存在； Exp：代表 Expected，比 Req 低一级，表示希望存在的变量，没有也可以； Perm，代表 Permissible，代表允许存在的变量，是否添加到数据集中要看实验设计。 注意，SDTM 标准下只允许规定的变量存在于数据集中。收集到的数据变量如果在 SDTM 标准中没有，一定不能添加到 SDTM 数据集中。每个数据集变量必须存在于 IG 中，只能少，但是不能多。\nSDTM 放不进去的数据怎么办 虽然 SDTM 在逐步更新的过程中，基本上能够覆盖到超过 90% 的常见临床实验的情况。但毕竟每一款药物都有自己特殊的地方，有可能遇到某一些变量无法放置在 SDTM 标准数据集的情况，这时候要介绍两个概念：补充数据集和自定义数据集。\n如果数据是某些 SDTM domain 的补充内容，就放置在补充数据集中。\n补充数据集的命名要求：SUPP + 原始 SDTM Domain 名称； 举例：关于心电图的 Domain EG，医生对于心电图的解读结果无法放在标准化的 EG 数据集中，就可以放置在命名为 SUPPEG 的补充数据集中； 注意“追溯性”：补充数据集中的每一条数据必须在原始数据集有对应记录。 而如果某一种数据，无法归类在 SDTM 标准下的任何一个 Domain，就要使用自定义数据集了，这一类型的命名要求是：XYZ + 两位字母。不过这种情况极为极为少见，如果真遇到这种情况，还是好好检查一下，看看自己是不是遗漏了 IG 文件中的某个 Domain。\n","id":5,"section":"posts","summary":"总结 介绍了 SDTM 和 IG 文件的基本概念； 通过阅读 SDTM IG 文件，了解 Domain 和 Variable 的各种信息； 重点是 Variable 的五种分类：Identifier, Topic, Qualifier, Timing, Rule。 SDTM 的基本概","tags":["分享","SAS","CDISC"],"title":"CDISC 标准（二）——SDTM 标准概述","uri":"https://www.xzywisdili.com/2023/04/2023-04-29-cdiscnote2/","year":"2023"},{"content":"今天偶然翻看以前的博客，突然发现图片都显示不出来了，第一时间想到可能是以前用的图床出了问题。\n果不其然，经过查找和检索网上信息，发现新浪图床从去年某个时间开始，限制了外部链接的访问。这就导致了，如果是在自己的博客站上使用了新浪的图床，会导致无法访问（403）。\n这时候，不要慌，你的图片并没有丢！网上也提供了很多方法，教你怎么能够重新访问这些图片。我这里推荐一个亲测好用的方法。\n第一步：查看你的新浪图床外链链接 按照如下表格修改你的外链链接：\n图床链接前缀 修改之后 wx1 tva1 wx2 tva2 wx3 tva3 wx4 tva4 ww1/2/3/4 不用改 tva1/2/3/4 不用改 tvax1/2/3/4 不用改 第二步：通过修改之后的链接找回图片 这时候需要使用外面的托管服务：https://i0.wp.com/ + 你的图床链接。\n举例：现在的图床链接是：tva1.sinaimg.cn/large/006tNbRwgy1g9qkc9s187j30pd0nlgqq.jpg，那么代入上述网址之后的链接就变成：\nhttps://i0.wp.com/tva1.sinaimg.cn/large/006tNbRwgy1g9qkc9s187j30pd0nlgqq.jpg\n下载下来之后，再迁移到新的图床上就好了，我目前选择的新图床是 sm.ms。\n如果以前的图片很多，可以考虑自己写代码批量转换迁移，少的话自己手动也可以完成。\n不过需要给自己提个醒了，没有 100% 永远保证稳定的图床服务，对于重要的图片，还是需要在本地备份。\n","id":6,"section":"posts","summary":"今天偶然翻看以前的博客，突然发现图片都显示不出来了，第一时间想到可能是以前用的图床出了问题。 果不其然，经过查找和检索网上信息，发现新浪图床从","tags":["分享","博客"],"title":"博客的新浪图床图片崩了？简单办法教你解决！（2023年4月有效）","uri":"https://www.xzywisdili.com/2023/04/2023-04-28-saveyourblogpics/","year":"2023"},{"content":"群里有人问，想要做一张这样的指定形状的词云：\n我试了一下，其实很简单，基本上 5 分钟就搞定。这里的简单思路就是：\n使用 jieba 包对原始文本进行分词处理; 将分词之后的结果导入 wordcloud 包生成分词。 至于指定形状的需求，需要注意在 WordCloud() 中的 mask 参数导入指定的形状遮罩图片就好，具体的代码如下：\nimport jieba from wordcloud import WordCloud import numpy as np import PIL.Image as image # 对分析文本做分词处理 def cut(text): word_list = jieba.cut(text) result = \u0026quot; \u0026quot;.join(word_list) return result # 读取分析文本，进行分词 with open(\u0026quot;D:\\\\测试文章.txt\u0026quot;, 'rb') as fp: text = fp.read().decode('utf-8') words = cut(text) # 设置目标词云的遮罩 mask = np.array(image.open(\u0026quot;D:\\\\测试图片.png\u0026quot;)) # 进行词云分析并生成 test_cloud = WordCloud( mask = mask, background_color = '#FFFFFF', font_path='C:\\\\Windows\\\\Fonts\\\\方正聚珍新仿.TTF' ).generate(text) image_produce = test_cloud.to_image() test_cloud.to_file(\u0026quot;D:\\\\生成图片.jpg\u0026quot;) 当然，再总结一下完成过程，需要注意这两个小小的翻车点：\nwordcloud 自身不支持中文，直接生成会产生乱码，注意在 WordCloud() 中通过 font_path 参数指定本地的中文字体文件（程序第24行）； mask 使用的图片是你想生成的词云的遮罩形状，注意你准备好的图片需要是白色底或者透明底 Over.\n","id":7,"section":"posts","summary":"群里有人问，想要做一张这样的指定形状的词云： 我试了一下，其实很简单，基本上 5 分钟就搞定。这里的简单思路就是： 使用 jieba 包对原始文本进行分词处理;","tags":["分享","Python"],"title":"5 分钟简单生成指定形状的词云","uri":"https://www.xzywisdili.com/2023/04/2023-04-26-generatewordcloudin5minutes/","year":"2023"},{"content":"有没有想过，有些网站的页面上展示的数据只有一部分的时候，通过稍微一点挖掘就能发现额外的数据，收获更多信息。\n这里就举一个最近发现的例子——云顶之弈公开赛“云巅赛道”的官方实时排名更新。 可以看到，这里只展示了云巅赛道前 60 名的用户昵称、段位、胜点、排位场数、登顶率和前四率。\n但是其实，只要扒到网站请求的数据，兴许可以看到更多的数据。\n发现网站请求返回的数据 这里其实没有想象中那么难。在「检查」中的「网络」选项卡查看网站的请求，一眼就能发现这个全是大写的请求返回的 json 数据。 稍微看一下，各类数据也都对的上，就是这个了：\n这里可以复制请求链接，再每隔一段时间请求一次，查看实时更新变化，我这里就不做了，有兴趣的可以查看我的另一篇文章。我就直接将 json 数据保存到了本地。\n使用 RJson 解析 json 格式数据 这里就使用一下 R 语言中的 RJson 包来解析一下 json 格式数据：\nlibrary(rjson) result \u0026lt;- fromJSON(file=\u0026quot;云巅.json\u0026quot;) result 可以看到，直接使用 fromJSON 读取出来的是 list 格式的数据，还是多次嵌套的 list：\n第一层：每一个用户作为一层; 第二层：单个用户下的每一个变量数据作为第二层； 同时，可以看到，最关键的用户名 sName 变量用了 URL Code 编码，需要再使用 URLdecode 解一下。\n这就需要先进行多层 list 数据的解包，加上稍微一点预处理了：\nlibrary(tidyverse) # 将 json 数据的每一层（每一个用户）转换成单行数据 row_results \u0026lt;- lapply(result[[2]], function(x) { as_tibble(x) }) # 对单行数据进行合并，得到最终数据 toc_rank_result \u0026lt;- do.call(\u0026quot;rbind\u0026quot;, row_results) # 使用 URLdecode 转换用户昵称 toc_rank_result \u0026lt;- toc_rank_result %\u0026gt;% mutate(nickname = URLdecode(sName)) %\u0026gt;% select(nickname, iTier, iRank, iPoints) # 查看转换后的数据 View(toc_rank_result) 大功告成！可以看到，相比于官网页面上的 60 条数据，这里分析的请求数据总共有 100 条，足足多了 40 条。 而且爬下来的数据想要做更多的查询和分析也更加方便了。 有时候，网页上没有显示的数据，可能还真需要扩展一下思路，看看能不能用其他方法得到。\n","id":8,"section":"posts","summary":"有没有想过，有些网站的页面上展示的数据只有一部分的时候，通过稍微一点挖掘就能发现额外的数据，收获更多信息。 这里就举一个最近发现的例子——云顶","tags":["分享","R语言","爬虫"],"title":"数据挖掘的奇效——爬到网站没有显示的数据","uri":"https://www.xzywisdili.com/2023/04/2023-04-23-userjsonfindextradata/","year":"2023"},{"content":"总结 介绍了数据的分类（数值型、分类型）等； 单变量分析：位置度量（均值、中位数）、变异性度量（方差、标准差、百分位数）等； 单变量的可视化：箱线图、频数表、直方图等； 多变量分析：探索相关性。 多变量的可视化：散点图、热力图、列联表、分类箱线图等； 探索性数据分析（Exploratory Data Analysis，EDA）是数据科学项目的第一步。\n数据的分类和主要形式 首先，需要了解数据的分类。在目前世界，数据来源非常丰富，而大多数数据是非结构化的，比如图像、文本、用户交互。这些“信息”，或者说非结构化的数据必须先转化为结构化的数据，才能够用于下一步的分析。\n结构化数据的分类：\n数值型数据 离散型数据：只能取整数； 连续型数据：在一个区间可以取任意值 分类型数据 二元数据：两个值中取其一，0 或 1 多元数据 特殊形式：有序数据，按照分类进行排序 需要强调的是，数据的分类对于后续可视化方法的选择、统计模型的选择都非常重要。\n接下来需要说明的是数据科学中最经典的引用结构——矩形数据。最常见的就是我们经典的电子表格。其中每一行代表一条观测记录，每一列代表一个特征（变量）。在不同统计编程语言中也有对应的处理方式：\nR语言: data.table Python Pandas: DataFrame 其他的几种非矩形的数据形式包括：时间序列数据、空间数据和图形或网络数据。他们都有对应的处理方法。\n如何描述连续性变量？ 位置度量 变量代表了测量数据或者计数数据。探索数据的一个基本步骤，就是了解每个变量（特征）的特点。不同种类的数据，我们对其的主要关注点也不尽相同。统计学中主要关注以下数据特征值：\n均值：基本的位置度量，对极值敏感 加权均值：将每个值乘一个权重值然后除以总和 切尾均值：消除极值之后的均值，比均值更加稳健 中位数：更加稳健，不易受均值影响 加权中位数：将数据集排序之后进行加权，加权中位数就是可以使数据集上下两部分的权重总和相同的值 离群值：并不一定是无效或错误的数据，但往往是由于数据的错误所导致的。 统计学家替换用估计量（estimate）来表示从手头已有数据计算得到的值，来描述数据情况与真实状态之间的差异。数据科学家和商业分析师更倾向于把这些值称为度量（metric）。因为统计学的核心在于如何解释不确定度，而数据科学则更关注如何解决一个具体的商业或企业目标。\n变异性度量 变异性（variability），也称为离差（dispersion），是另外一个描述数据的视角，表示数据是紧密聚集的还是发散的。在分析中主要会考虑：\n测量数据的变异性； 识别各种变异性的来源； 如何降低变异性 统计学中有以下数据特征描述变异性：\n偏差：观测值和估计值之间的直接差异 方差 标准差：方差的平方根 平均绝对偏差：对偏差值取绝对值然后求平均 中位数绝对偏差 极差：最大值和最小值之间的差值 顺序统计量：又称为秩 百分位数 四分位距（IQR）：75 百分位数和 25 百分位数之间的差值 方差和标准偏差是最广泛使用的变异统计量，且都对离群值敏感。更稳健的度量包括百分位数、四分位距和中位数绝对偏差。\nsd (state$Population) IQR(state$Population) mad (state$Population) quantile(state$Murder.Rate, p=c(.05, .25, .5, .75, .95)) 使用图表描述数据分布 箱线图 boxplot(state$Population/1000000, ylab=\u0026quot;Population (millions)\u0026quot;) 箱线图的顶部和底部分别是 75 百分位数和 25 百分位数。水平线代表的是中位数，虚线称为须（whisker） ，从最大值一直延伸到最小值，体现了数据的极差。\n频数表 频数表是直方图中频数计数的表格形式。\nbreaks \u0026lt;- seq(from=min(state$Population), to=max(state$Population), length=11) pop_freq \u0026lt;- cut(state$Population, breaks=breaks, right=TRUE, include.lowest=TRUE) table(pop_freq) 组距必须为大小相等的组距，如果组距过大，就会隐藏掉分布的一些重要特性\n直方图 直方图在 x 轴上绘制变量值，在 y 轴上绘制频数计数情况，显示了数据的分布。\nhist(state$Population, breaks=breaks) 直方图的注意事项：\n空组距也应该包括在直方图中 各个组距是等宽的 组距的数量（或大小）取决于用户 各个条块应该紧密连接，没有空隙 密度图 密度图通过一条连续的线显示数据值的分布情况，是通过一种核密度估计量直接计算得到的。可以理解为直方图的平滑表示形式。\nhist(state$Murder.Rate, freq=FALSE) lines(density(state$Murder.Rate), lwd=3, col=\u0026quot;blue\u0026quot;) 如何描述分类数据？ 分类数据通常按照比例进行总结，一般使用条形图和饼图进行可视化。\n一般描述分类型数据的变量如下：\n众数：出现次数最多的类别或值； 期望值：如果类别和其他数据关联，根据类别出现概率计算的平均值； 探索变量相关性 探索性数据分析中，相关性描述的是两个变量 X 和 Y 之间的相关程度。如果 Y 随着 X 的增大而增大，则 X 和 Y 是正相关的；如果 Y 随着 X 的增大而减小，则 X 和 Y 是负相关的。两者之间没有明显规律，则两个变量之间不相关。\n要表示变量之间的相关性，一般使用以下三种工具：\n相关系数：用于测量相关程度的度量值，取值范围在 -1（完全负相关）到 1（完全正相关）之间； 如果相关系数为0，那么表示两个变量之间没有相关性； 相关系数有不同的几种：皮尔逊相关系数、斯皮尔曼秩相关系数、肯德尔秩相关系数 相关矩阵：将变量在一个表格中按行和列显示； 散点图：直观表示两个变量之间关系； 相关系数 计算相关系数：\ncor(data, method=\u0026quot;spearman\u0026quot;) 相关矩阵将变量在一个表格中按照行和列进行显示，表格中每个单元格的值是对应变量间的相关性。\nlibrary(corrplot) corrplot(cor(etfs), method=\u0026quot;ellipse\u0026quot;) 这里要注意：如果两个变量之间的实际关系是非线性的，那么相关系数就不是一个好的度量值。\n散点图 散点图是用来可视化两个变量之间关系的标准方法。\nplot(data$x, data$y, xlab=\u0026quot;x\u0026quot;, ylab=\u0026quot;y\u0026quot;) 除此之外，探索多个变量的不同方法：\n多个连续型变量：热力图、六边形图 多个分类型变量：列联表 分类型和数值型变量：分类箱线图、小提琴图 这类类型的图表都可以使用 ggplot2 包完成：\nstat_binhex geom_boxplot，geom_violin 在灵活探索多个变量之间关系的时候，可以考虑关键变量的分组进行显示，比如探索不同人群收入和性别的关系的时候， 可以按照“受教育程度”作为分类进行探索。这里可以使用 facet_wrap，结合 ggplot 进行。\n","id":9,"section":"posts","summary":"总结 介绍了数据的分类（数值型、分类型）等； 单变量分析：位置度量（均值、中位数）、变异性度量（方差、标准差、百分位数）等； 单变量的可视化：箱线","tags":["分享","R语言","统计分析"],"title":"【学习笔记】实用统计学复习手册01——探索性数据分析","uri":"https://www.xzywisdili.com/2023/03/2023-03-23-dataanalysisnote01/","year":"2023"},{"content":"什么是保序回归？ 保序回归（Isotonic Regression）是一种适用于单调函数非参数统计回归方法，即在一个序列中，Xn ≥ Xn-1。通过字面理解「Isotonic Regression」：\niso的意思就是「相等、相同」的意思； tonic 就是 tone 的意思，指的是「调」。 所以保序回归的核心还是在于单调递增的函数，当需要分析的数据资料符合单调递增的趋势可以用。 保序回归最常用的应用场景之一是探索药量和药效的关系，因为一般来说药物剂量越高，药效应该更强，因此通过保序回归的方式可以从药效和经济学的角度估计最合适的药量。\n保序回归使用 weighted least-squares 来进行拟合： 怎么做保序回归？ 求解保序回归的一种最常用算法是 PAVA 算法（ Pool-Adjacent-Violators Algorithm，池相邻违规者算法）。PAVA 算法的直观形式只需要看下面这张图就行了：\n这种算法是通过从左往右逐渐扫描数据序列，并且保证整个序列是单调递增的，以此来获得 Beta 值的结果。如果 Beta_i \u0026lt; Beta_i-1，那么就同时把这两个值替换为 (Beta_i + Beta_i-1) / 2。以此就能获得严格且平滑的保序回归。\n通过 PAVA 算法，可以获得一个包括多个 Beta 参数组成的单调递增序列，用可视化的方法可以看到是由多条上升线和水平线组成的函数图：\n如何使用 R 语言进行保序回归？ 在 R 中可以轻松进行保序回归，只需要使用 stats 包中的 isoreg 函数即可。下面的代码提供一个简单的示例，并将原始数据和拟合值（蓝点）绘制出来。注意，拟合后的蓝点是单调递增的。\n# Generate Training Data set.seed(15) x \u0026lt;- sample(2 * 1:15) y \u0026lt;- 0.5 * x + rnorm(length(x)) # Isotonic Regression Model Fit reg.fit \u0026lt;- isoreg(x, y) # Isotonic Regression Plot plot(x, y, pch = 4) points(reg.fit$x[reg.fit$ord], fit$yf, pch = 16, col = \u0026quot;blue\u0026quot;) 这里需要注意，保序回归模型和其他回归不同的点是，在提取拟合值时，代码语法不同。isoreg 函数的返回值无法和预测数据互动。这里可以考虑使用 sd.stepfun 来完成对预测数据拟合值的校准。代码如下：\n# Generate Test Data test_x \u0026lt;- sample(2 * 1:15 - 1) test_y \u0026lt;- 0.5 * test_x + rnorm(length(test_x)) all_x \u0026lt;- c(x, test_x) all_y \u0026lt;- c(y, test_y) # Isotonic Regression Model Fit iso.fit \u0026lt;- as.stepfun(isoreg(x, y)) # Predict and Plot plot(all_x, all_y, pch = 4) points(all_x, iso.fit(all_x), pch = 15, col = \u0026quot;red\u0026quot;) points(reg.fit$x[reg.fit$ord], reg.fit$yf, pch = 16, col = \u0026quot;blue\u0026quot;) 只需要把代码中的 iso.fit 看作是在训练数据上保序回归的拟合得到的黑箱。绘制结果可以显示训练拟合的模型（蓝点）和所有数据的预测值（红点）。可以看到，训练值和拟合值是兼容的，整体拟合依然是单调的。\n在 R 语言中使用 PAVA 算法 想要使用 PAVA 算法的话，只需要用 isotone 包下面的 gpava 函数就搞定了，具体文档在此。\n使用例子也很简单：\nlibrary(isotone) data(pituitary) gpava(pituitary[,1],pituitary[,2], ties = \u0026quot;primary\u0026quot;) ","id":10,"section":"posts","summary":"什么是保序回归？ 保序回归（Isotonic Regression）是一种适用于单调函数非参数统计回归方法，即在一个序列中，Xn ≥ Xn-1。通过","tags":["分享","R语言","统计分析"],"title":"【学习笔记】保序回归——适用于单调递增数据的统计回归方法","uri":"https://www.xzywisdili.com/2023/03/2023-03-07-isotonicregression/","year":"2023"},{"content":"总结 《所罗门的密码》总结了人工智能在方方面面改变整个社会的同时，可能出现的问题和挑战，并倡议出版一份《全球人工智能经济大宪章》。 书中大篇内容均是过去材料的堆叠和老生常谈的问题陈述，没有作者自己深刻的思考，和独到的见解； 评分：2.5/5.0； 推荐受众：无，不推荐任何人阅读。 作者借用了所罗门的故事，想要寓言人类社会不善用人工智能的未来。[所罗门王](所羅門 - 維基百科，自由的百科全書 (wikipedia.org))是《圣经》中的一个人物，代表着智慧。他最出名的一个故事就是“所罗门的审判”。 但拥有智慧的他误入歧途，让他建立和统治的王国深陷衰落和动荡。天才的智慧得到了浪费，并为此付出了沉痛的代价。\n但我认为，单纯地用这个例子去比喻人工智能，也不是那么恰当。\n从人工智能的本质去理解信任问题 目前来看，人工智能的本质，即收集数据，分析数据，构建模型。一般而言，想要得到更好的模型，需要海量，且高质量的数据。最近火爆的 ChatGPT 更是证明了这一点：当数据量足够大，同时模型参数足够多的情况下，人工智能模型能够展现出无比强大的通用语言能力。\n但是，这里就引出来问题的关键：谁能同时拥有高质量的海量数据，又拥有能够驾驭这些数据的分析团队？\n那就是数字行业的巨头公司，比如脸书、谷歌和阿里巴巴等。\n那么，关于对人工智能的信任问题，其实就是广大用户和数字巨头之间的信任问题了。\n在数字巨头提供 AI 服务，和大众用户供养数据的过程中，不可避免会产生关于以下的信任问题：\n准确性与责任：广告推送可能无需很高的准确性，但医疗、无人驾驶等需要极高的准确性要求； 信息收集和滥用问题； 价值观：AI 模型的利益取向、价值观等； 其他问题 而在此过程中，由于数字巨头一开始便拥有垄断优势，在常年累月的数据积累和模型迭代后，垄断的趋势会更加强烈。这就导致对于大众用户来说，想要像选择超市商品一样更换品牌，是非常之困难的。\n这就对数字巨头有着相当高的要求了：不能为了自身利益损害用户权益，不能滥用用户信息和数据，时刻对模型和算法进行监督和自查，在出现问题时承担起应有的责任……而以上的每一项对于一个商业公司来说都是巨大的显性或隐性的成本负担。\n那么如何进行约束？ 这就是问题真正的困难点所在。一般而言，大家能想到的是以下两种方式，不过这两种方式都有显而易见的困难点。\n通过政府或第三方对算法和模型进行监督和审核。 但是，要清楚的是，目前主流的人工智能模型，如深度神经网络，其结果的输出是「在混沌中涌现出来」的。即使是算法工程师本身，面对这个黑箱，或者炼丹炉，都无法 100% 完全参透其中的作用机制。那么也实在难以确保其中的公开透明，或者公正和准确。而且，不可否认的是，监督本身也会对人工智能的创新产生或多或少的遏制作用。\n只允许人工智能给人类提供建议，真正的决策权还是把握在人类手中。 听起来很美好，但仔细想想，人类自己的认知和心智是否足够强大到，可以理性地看待 AI 的建议而不受其影响。这样是否也会隐性地左右人类的价值观和所谓的选择权？又或者这也是一种逃避责任的方式罢了？\n无论如何，人工智能都已经在逐步对整个社会进行改造和重塑。但数据并非万能全面，至少价值观念、人类情感等等属性还暂时难以完美地量化。在此基础上，我们需要找到人类和人工智能的相处方式，让 AI 能够良性地辅助我们的工作和生活，而非吞噬。\n","id":11,"section":"posts","summary":"总结 《所罗门的密码》总结了人工智能在方方面面改变整个社会的同时，可能出现的问题和挑战，并倡议出版一份《全球人工智能经济大宪章》。 书中大篇内容","tags":["分享","读书笔记","阅读","人工智能"],"title":"【把书读薄07】《所罗门的密码》：试图分析人工智能的信任问题却深度不足","uri":"https://www.xzywisdili.com/2023/03/2023-03-03-bookreview07/","year":"2023"},{"content":"总结 《跟李锐学Excel数据分析》 名为「数据分析」，内容实则是 Excel 软件使用技巧，这点需要读者注意； 虽然页数看起来较多，但是大量篇幅均为各种实操步骤截图，伴随每一个步骤讲解，友好度不如视频； 评分：2.0/5.0，更建议想要学习 Excel 工具的读者去寻找视频教程；想学习数据分析相关内容的读者就更不用看了。 最近读完了《跟李锐学Excel数据分析》。可惜的是，书名有点误导性质，全书和「数据分析」关系不大，更多是介绍 Excel 的使用方式和进阶技巧。但多多少少还是跟着看完了全书，并且把对自己有点用的部分记录了下来，权当给自己复习巩固。\n书中讲解的 Excel 相关知识，可以归纳为以下几点：\n需要知道的进阶知识 函数和公式 实用技巧和快捷键 更多高级用法：数据透视表、Power Query、Power Pivot 在这些内容中，最常用的需要非常熟悉。而相对不那么常用的，只要知道其存在，脑子里有个印象即可。具体用法细节可以在用的时候，稍加检索，基本上也可以了。\n一、需要知道的进阶知识 这里有几个需要优先了解的进阶知识，方便在后面的公式部分进行组合，发挥出更强大的威力。\n绝对引用，相对引用，混合引用 通过在行和列号前面的 $ 符号进行控制，可以将 $ 理解为锚定，锚定住的行或列号是不会改变的。比如 $A$1:$B$8 就是一个绝对引用的区域，在自动填充中不会变化。\n字符串连接符号 \u0026amp; 符号，代表字符串的连接。比如 =\u0026quot;联想\u0026quot;\u0026amp;\u0026quot;笔记本\u0026quot;，显示结果就会是 联想笔记本。\n通配符 通配符，就是这个符号能匹配多种字符，在查找和替换里经常会用到，以下几种最常用：\n问号 ?：占位一个字符； 星号 *：占位多个字符； 波浪号 ~：右侧的符号为普通字符，比如 ~* 指代的就是星号。 跨表符号 一般在需要跨表引用的时候，使用 ! 进行连接，比如 sheet1!$B$4:$F$8 代表引用 sheet1 的 B4 到 F8 范围。\n二、快捷键 说到快捷键，其实最推荐的学习方式就是在某宝上购买一个快捷键桌垫，在手边经常能看到，日积月累耳濡目染也能记得不少，可以反复加深印象：\n不过还是总结了书里提到的几个常用的快捷键：\n\u0026lt;Alt + =\u0026gt;：行和列的求和汇总； \u0026lt;Ctrl + Enter\u0026gt;：批量填充，非常好用； \u0026lt;Ctrl + E\u0026gt;：自动填充，可以批量提取和合并数据，甚至部分场景可以智能合并数据； \u0026lt;Ctrl + T\u0026gt;：快速在现有表上创建表格； \u0026lt;Ctrl + 1\u0026gt;：设置单元格格式； \u0026lt;Ctrl + Shift + 数字\u0026gt;：快速切换单元格格式（常规、文本、日期、货币、百分比等等）； \u0026lt; F5 \u0026gt;：定位 三、公式 有一些非常经典好用的公式，是需要记住使用方法的。而我认为，其他公式，只需要脑子里有个印象即可，当需要用到的时候，在网上稍微查一下用法和各个参数就行了。这里列举一些比较常用和好用的公式。\n逻辑判断 IF(条件, 条件成立时的结果, 条件不成立时的结果) 举例：IF(A2\u0026gt;=60, \u0026quot;及格\u0026quot;, \u0026quot;不及格\u0026quot;) 多条逻辑判断组合 AND 和 OR 举例：IF(OR(A2\u0026gt;=60, B2\u0026gt;=60), \u0026quot;及格\u0026quot;, \u0026quot;不及格\u0026quot;) 特定查找 VLOOKUP(查找值, 查找区域, 返回值在查找区域所处的列数, 0) 举例：VLOOKUP(A2, 表1!$A2:B$50, 2, 0) 返回单元格所在行和列 ROW(单元格引用)、COLUMN(单元格引用) 提取某个区域中指定位置的数据 INDEX(区域，所查找的行数，所查找的列数) 举例单列数据：INDEX(B2:B13, 5) 提取 B2-B13 区域第 5 行数据 多行多列数据：INDEX(B2:G13, 5, 3)，提取 B2-G13 区域第 5 行，第 3 列的数据 查询指定数据在某一列中的相对位置 MATCH(指定数据，单行或单列区域, 0) 举例：MATCH(D2, A2:A13, 0)，返回 A2-A13 区域中，D2 数据所在的相对位置 INDEX 和 MATCH 往往组合在一起使用 改变数值显示形式 TEXT(数值，格式代码) 举例：TEXT(A2, \u0026quot;0.00\u0026quot;) 即显示保留 2 位小数 根据指定条件统计个数 COUNTIF(统计区域, 判断条件) 举例：COUNTIF(A2:A10, \u0026quot;男\u0026quot;)，统计 A2-A10 区域中值为\u0026quot;男\u0026quot;的个数 多个区域和条件统计个数：COUNTIFS(统计区域1, 判断条件1, 统计区域2, 判断条件2, ...) 根据指定条件求和 SUMIF(判断区域，判断条件，求和数据所在区域) 举例：SUMIF(C:C, \u0026quot;小米*\u0026quot;, D:D) 根据 C 列进行条件判断，然后对 D 列对应数值求和 多个条件汇总求和：SUMIFS(求和区域，判断区域1, 判断条件1, 判断区域2, 判断条件2, ...) 经典的分类汇总函数 SUBTOTAL(功能参数，统计区域) 可以自动无视被筛选或隐藏掉的行影响，具体功能参数用法可以查询相关文档 四、实用技巧 从外部导入数据，推荐 Power Query：「数据」→「获取数据」； 科学记数法录入数据：1**8 可以方便录入 100000000； 规定单元格输入内容范围：「数据」 → 「数据验证」； 多个区域录入相同的内容：选中区域 → 录入内容 → \u0026lt;Ctrl + Enter\u0026gt;； 转换不规范的日期数据：「数据」 → 「分列」 → 「列数据格式」选择「日期」； 可视化数据条：「条件格式」 → 「数据条」，具体的各种设置可以在「编辑格式规则」中调整； 可视化图标集：「条件格式」 → 「图标集」，添加方向、形状等等标记，也可以在「编辑格式规则」中调整设置； 五、更多高级用法 更多的高级用法包括数据透视表、Power BI（主要是 Power Query、Power Pivot），可以针对复杂的大数据量的表格通过多维度进行分类、汇总、转换等等操作，功能相当强大。\n但可惜的是，这部分内容通过书本讲解起来相当吃力，作者想要通过案例展示功能，但只能将逐个步骤进行截图，配上文字讲解，在占去大量篇幅的同时，读者看起来也相当吃力。\n这部分还是推荐通过视频的方法更系统地学习一下，B站上就有非常多的教学视频：\n结尾的碎碎念 2022 年过去了，总感觉有点浑浑噩噩。 本来这本书在年前就已经开始阅读了，但整个过年期间，心思也一直安定不下来，一直拖到了年后才读完，并且做了总结。 希望新的一年里，至少能多看一点书，毕竟囤了不少很感兴趣的书还没有来得及阅读。而且也希望自己能输出更多的博客文章。\n","id":12,"section":"posts","summary":"总结 《跟李锐学Excel数据分析》 名为「数据分析」，内容实则是 Excel 软件使用技巧，这点需要读者注意； 虽然页数看起来较多，但是大量篇幅均为各种实操","tags":["分享","读书笔记","阅读","Excel"],"title":"【把书读薄06】Excel 学习真的不适合借助书本了","uri":"https://www.xzywisdili.com/2023/01/2023-01-30-bookreview06/","year":"2023"},{"content":"今天终于通过了 SAS Advance 的考试。有一些最新的备考和考试过程的经验可以跟大家分享一下。 这篇攻略我会从两个方面来说，分别是备考篇和考试篇。\n备考篇 SAS Advance 的考试并不难，主要是考察三个部分：Proc SQL 语句，SAS 中Macro宏的应用以及一些进阶编程技巧。 只要用我下面列出的备考材料复习，就完全不用担心过不了。所有材料的分享链接在最后。\n官方教材《SAS Certified Professional Prep Guide: Advanced Programming Using SAS 9.4》 注意，一定一定不要用老版的教材，老版的教材有 800 多页，看完太费时间，而且还有大部分内容不会考察，还有部分要考察的内容没有涉及。所以一定要认准总共 400 余页的新版教材。另外，可能在某亩三分地或者其他论坛里分享的教材版本是乱码的，里面的代码片段和图片也有很多缺失。我找到了完整的电子版书籍，内容是完整的，分享链接也在下面。\n论坛大神 Mike Q 总结的 20 道 Lab 试题 Lab 试题，即需要你在考场上实机编写代码进行解答，具体情况是：\n左侧显示题目信息和要求； 右侧是一个远程编程环境窗口，供你写代码使用； 备考时候可以提前看一下 SAS 官方提供的讲解视频，熟悉操作； 实机操作的题目不用担心，不会很难，题型基本上都涵盖在了上面的 20 道 Lab 机经中，考前一定要至少刷一遍！最好自己亲手敲一遍代码，更加熟悉。\n论坛大神总结 63+20 题 这部分主要针对的是考试中出现的选择题，63+20 题版本已经可以涵盖绝大部分的考试范围了。我分享的版本除了题目，还有详细的题目答案解析，总共内容不到 100 页，考前至少刷一遍就好。另外需要注意，正式考试中还覆盖了正则表达式（Perl Regular Expression）的题目，在刷题之余还需要翻看一下教材。\n备考总结 总结一下怎么备考：\n有富余时间建议过一遍教材（包括课后习题），没有时间的话可以跳过直接刷题； 一定过一遍 Lab 20 题，至少亲手敲一遍代码； 一定过一遍 63+20 题，可以看答案过一遍之后，挡住答案自己做一遍。 考试篇 考试篇我会分为线上考试和线下考试分别讲讲是怎么样的（对，我很悲催地两种都经历过）。\n无论是线上还是线下都可以在官网进行报名；按照步骤注册登录，然后选择自己想要参加的考试（本文中是 SAS Advance Programming - Performance Based）就行了。之后在选择考试方式（线下 or 线下），预约时间，进行付费就可以了。付费需要使用海外信用卡支付 180 美元。\n线上需要怎么准备 线上需要下载官方提供的 OnVue 软件。在考试当天开考前 30 分钟，你会获得一个考试码进入到软件，进行 Check-in 的阶段。\n在 Check-in 阶段，官方会让你上传一张自拍，上传个人证件的正反面照片，并且将自己的考试环境（房间）的前后左右拍照上传。在完成上传后，有一名监考会审阅你上传的照片，并且跟你语音确认监考的情况。\n我第一次就栽到了这一个步骤。分配给我的印度监考官表示我的电脑摄像头只能看到脸，无法看到考试周围的环境，因此不准予我参加考试，后续与其理论也无果。只能取消了当天的考试。\n在考试取消后，官方会向你的邮箱发送两封邮件告知取消和退款的情况；退款一般会在 1-2 个工作日后退换到你的付款账户中。\n线下需要怎么准备 无奈，我又预约了离我最近的线下考点进行考试，不过整体体验下来还算方便（所以我也推荐线下考试一步到位）。\n需要注意的是，最好提前跟考点进行电话咨询，询问是否需要准备任何东西。比如我要去的考点在一个大厦里，需要提前进行访客登记和预约，给一个二维码才能进大楼。在前往考点时需要携带两个自己的带照片的个人证件，比如身份证和驾照。\n在指定时间到达考点之后，会有工作人员让你在考场须知上签字，并且登记个人信息和考试科目编号，查验证件并且进行现场拍照。之后需要将携带的包和其他所有个人物品锁在柜子里进入考场考试。不过还好，考场提供草稿纸和笔。\n考试的系统如下图所示（在上面的所贴的视频中有），有可能你的考场提供的机器会有一点卡，所以慢慢操作就好了：\n考试内容 考试分为三个部分：\n准备阶段：给你时间熟悉考试系统，打开 SAS Studio，调整界面大小和分辨率，熟悉怎么提交答案； 实机编写代码阶段：12 道题目，5 道题目考察 SQL 语句，4道考察 Macro 宏语句，1 道考察 Array 的使用，1道考察 PROC FCMP 自定义函数，还有 1 道考察使用 Hash Object 进行查询； 选择题：21 道题目，绝大部分知识点都在 63+20 题目里，还有一点正则表达式的题目可以翻教材看一下就好； 考试结束之后，会当场显示成绩和是否通过。同时考场也会直接给你一份打印出来的成绩单报告：\n资料分享 最后就是最重要的 3 份资料的分享：\n百度盘链接 提取码：nacd 祝贺所有朋友都能顺利通过！\n","id":13,"section":"posts","summary":"今天终于通过了 SAS Advance 的考试。有一些最新的备考和考试过程的经验可以跟大家分享一下。 这篇攻略我会从两个方面来说，分别是备考篇和考试篇。 备考篇 SAS Advance 的","tags":["分享","SAS","考试"],"title":"（2022年11月过）SAS Advance 考试最新完全攻略！","uri":"https://www.xzywisdili.com/2022/11/2022-11-08-sasadvance/","year":"2022"},{"content":"总结 《向心城市》中作者通过大量的数据分析实证，介绍了城市的发展规律、为什么“向心城市”是发展趋势，以及城市化带来的各种方面的内容分析； 评分：3.8/5.0，书中的大量内容更加面向城市的规划者，感兴趣的可以细读；至于想要在书中找到未来人生的抉择，其实可以跳读，内容不算太多。 城市的本质是什么？城市的未来会怎么发展？《向心城市》中通过对大城市的各种数据的实证分析，为我们揭示了城市的发展规律。\n城市越来越大是未来的趋势 为什么会出现城市？为什么在经济发展的过程中，出现了一个又一个的大城市？\n作者告诉我们，这是经济发展的客观规律。城市本身出现和长大的过程，其实是为了方便人与人的见面。无论是投资、生产还是消费行为，都极度依赖人与人的见面沟通。\n而且，你会发现，越方便人与人见面的地方，比如市中心，根据市场的竞租机制，一般都会留给金融业；而次靠近市中心的地区，则是科技型企业和教育行业；而外围则一般是制造业的企业。住宿用地则夹在制造业用地和市中心中间的部分。这便是城市的基本构成，也是天然形成的最优化的结果。\n这样来看，白天，人口集中在靠近城市中心的地段进行工作；而到了夜晚，则向外扩散回到居住地，即内环和内外环中间的地区。\n那为什么城市越来越大，即“向心城市”才是未来的趋势呢？\n作者认为，在现有发展阶段，农业和工业趋向发展到了一定阶段，其实很大一部分的经济动力来源于新增的服务业。服务业也会带来大量的营收和新增就业岗位。这样一来，很多小城市或者农村的人员就会进城，得到收入更高的工作。这样就形成了一个循环：\n城市有更多的人口，能够产生更丰富的需求和消费行为，供养更丰富的服务业； 更丰富的服务业同时带来了更多收入更好的岗位，吸引人口流入。 如此一来，大城市就会越长越大，人口也会从小城市慢慢向大城市集中。这个趋势已经在中国和众多国家实现和发生，未来也将持续。\n大城市越来越大的问题怎么办？ 那就有很多人表示，大城市的人口越来越多，那所造成的拥堵和污染问题怎么解决呢？\n这一点，作者也列举出了数据来回答：那不是人口的锅。\n比如，作者认为人口较多的大城市与小城市相比，拥堵指数的 1.8 和 1.6 相差并不大。而北京、济南和哈尔滨则作为离群值，拥堵的根源主要在于交通规划和治理水平不到位。而从另一个角度想，人口密度更高的地区，居住场所周围的商业中心、交通站点和医疗中心的平均距离和时间也变短了。换句话说，人们的生活圈半径也缩小了。\n所以，人口越多的大城市的拥堵会略微增大，但是还是在可接受范围内。\n而对于污染，作者通过控制一系列经济发展变量，认为“城市市区人口与八类污染排放量均不再相关”。\n这里我觉得太过于武断了，首先，人口规模和一系列变量（比如经济变量）一定存在多重共线性的问题，这里面的混杂因素是否考虑进去了；其次，决定系数（R^2）小于 0.8 就认为不再相关也在统计学上比较具有争议；另外，作者认为“斜率（系数）均小于 1代表污染排放的增速远小于人口增速”就更匪夷所思了。人口规模和污染排放的单位必然不同，其次用线性模型时已经假设了人口规模和污染排放呈现线性关系，又怎么再用拟合之后的结果去反过来推断人口在大城市和小城市的贡献率高低呢？\n那这样是不是对人口流出地区不太公平？ 当然，那有人会觉得这对于人口流出地区太不公平，人口越来越少，可能经济凋敝，没有发展空间……\n作者认为流出地区还是应该顺应经济发展的规律，既然人口在流出，就要谨慎规划自己的建设。不要随意地建设新城和各种大面积的住宅和金融中心。相反，应该找到本地区的特色产业。发展有时候不一定等同于「经济总量」，如果把思路转变成「发展」和「平衡」意味人均 GDP 的均等化，或许更为合理。\n人口的流动，其实也是一种「用脚投票」。既然当地已经无法大幅拉高 GDP，那人口的减少其实也是变相增加了人均 GDP。而让大城市和小城市在某种程度上，在流动中走向了平衡。同时，从总体上来看，人均收入也在逐步获得提高。\n比如广东，广州和深圳 GDP 总量增长非常快，同时人口也在向这两个城市集中；而其他城市 GDP 虽然增加较慢，但是人口减少，这样人均 GDP 也在上升。通过人均 GDP 的比较发现，人均 GDP 的差距是有缩小的趋势。\n最大的收获是用数据找规律 读完本书，除了真的部分了解了城市发展的规律之外，其实最大的收获是收集数据来发现规律的做法。\n可能很多人觉得这没有什么新奇的，但是在互联网上有太多信口胡说的财经或者经济博主，单纯凭自己的个人体验和主观臆断，就能“胡扯”出很多完全靠不住脚的结论。这也让遵从数据事实认真分析的内容变得更加可贵。\n比如，作者在书中通过收集早晨和夜晚的通信数据来反映早晚城市人口迁移情况；通过货车运输轨迹密度图反映中国城市群的情况；通过第三产业产值、城市夜间灯光与其他国家进行对比来反映我国城市的发展进程情况……\n在数据中发现规律，或者验证理论，我觉得这是我阅读本书的最大收获之一。\n","id":14,"section":"posts","summary":"总结 《向心城市》中作者通过大量的数据分析实证，介绍了城市的发展规律、为什么“向心城市”是发展趋势，以及城市化带来的各种方面的内容分析； 评分：","tags":["分享","读书笔记","阅读","城市"],"title":"【把书读薄05】向心城市：带你了解现代城市的发展规律","uri":"https://www.xzywisdili.com/2022/10/2022-10-19-bookreview05/","year":"2022"},{"content":"在碰到一些网页，想通过爬虫抓取页面信息的时候，会发现网页采用了一些动态 HTML 的相关技术来展示信息。这样直接使用 requests 是无法直接获取想要的 HTML 元素内容的。比如我们查看微博网页端的粉丝列表：\n微博网页端的粉丝列表，在向下刷的时候是会动态加载和更新的。这种情况下，我们想要的元素是通过 js 事件动态请求和返回的。那么，就需要我们分析页面请求，找到那个发送的请求和对应返回的数据。在 F12 的「网络」选项卡下面进行刷新，可以比较轻松地找到对应发送的请求和返回的数据（json 格式）：\n可以看到，这个请求会返回 json 格式的数据。当用户下拉粉丝列表页面时，会触发一个 js 事件，项服务器发送这个请求获取数据，再通过一定的逻辑将这些 json 数据填充到 HTML 页面中。而我们的爬虫只要获取这些 json 数据，再进行整理就可以了。\n这样一来就比较轻松了：（当然需要注意一下，发出的相关请求需要你保持登录的 Cookie，不然返回会报错）\nimport requests headers = { 'cookie': 'Your Cookie', 'User-Agent': 'Your User Agent', 'referer': 'Your referer' } url = \u0026quot;https://weibo.com/ajax/friendships/friends?relate=fans\u0026amp;page=1\u0026amp;uid=3668829440\u0026amp;type=all\u0026amp;newFollowerCount=0\u0026quot; r = requests.get(url, headers=headers).json() for user in r['users']: print(\u0026quot;id: {} - name: {} = fans:{}\u0026quot;.format(user['id'], user['screen_name'], user['followers_count'])) 当然，我们可以看到 url 里面的参数 page=1，那么我们可以修改这个参数就可以获得很多页面的粉丝列表信息了。再保存到准备好的数据库中：\nimport requests import pymysql # 连接 MySQL 数据库 conn = pymysql.connect( host='127.0.0.1', user='root', passwd='your pwd', port=xxxx, db='your db', charset='utf8' ) headers = { 'cookie': 'Your Cookie', 'User-Agent': 'Your User Agent', 'referer': 'Your referer' } # 每次查看并更新 10 页粉丝列表 for page_i in range(10): url = \u0026quot;https://weibo.com/ajax/friendships/friends?relate=fans\u0026amp;page={}\u0026amp;uid=3668829440\u0026amp;type=all\u0026amp;newFollowerCount=0\u0026quot;.format(page_i) r = requests.get(url, headers=headers).json() for user in r['users']: name = user['name'] screen_name = user['screen_name'] followers_count = user['followers_count'] followers_count_str = user['followers_count_str'] # 如果粉丝数超过 10000，进行播报 if followers_count \u0026gt;= 10000: print(\u0026quot;新增过万粉丝播报：id:{} - name:{} - fans:{}\u0026quot;.format(user['id'], screen_name, followers_count)) sql = f\u0026quot;\u0026quot;\u0026quot; insert ignore into your_table (user_id, name, screen_name, followers_count, followers_count_str) values('{user['id']}', '{name}', '{screen_name}', '{followers_count}', '{followers_count_str}'); \u0026quot;\u0026quot;\u0026quot; cursor = conn.cursor() cursor.execute(sql) conn.commit() add_fans_count = after_update_fans_count - before_update_fans_count print(\u0026quot;本次新增查询粉丝数：{}\u0026quot;.format(add_fans_count)) print(\u0026quot;数据库中已存储粉丝数：{}\u0026quot;.format(after_update_fans_count)) conn.close() 当然，这只是爬一个单独的账号的粉丝列表（我喜欢的一个明星）。一般而言，不用实时监听，只需要每隔一段时间爬取一次，就能更新最近新关注的用户，并且发现是否有粉丝数过万的大V。需要注意的一个问题是，每隔一段时间（2-3天）就需要更新一下 Cookie，不然也会报错。\n以上内容供大家参考，共同学习。\n","id":15,"section":"posts","summary":"在碰到一些网页，想通过爬虫抓取页面信息的时候，会发现网页采用了一些动态 HTML 的相关技术来展示信息。这样直接使用 requests 是无法直接获取想要的 HTML 元素内容的","tags":["Python","爬虫"],"title":"动态页面的爬虫示例一则：抓取微博粉丝列表","uri":"https://www.xzywisdili.com/2022/10/2022-10-05-dynamichtmlspider/","year":"2022"},{"content":"总结 《一日两餐》想让我们通过限制碳水摄入和间歇性禁食这两个重要手段来摆脱未来的胰岛素抵抗； 书中的大量内容更像是别处观点的拼凑，同时时不时夹杂一些带货着实会令读者不悦； 评分：2.5/5.0，仅摘取前半部分的关键阅读即可，后半部分实在不推荐。 现代饮食理论提出的新观点认为，每日三餐的高碳水化合物的摄入，外加各种精制加工食品才是损害我们健康的元凶。《一日两餐》的作者想要为我们讲解这背后的理论原理「高胰岛素血症」，以及我们为什么需要「限制碳水」和「间歇性禁食」来帮助我们减掉身体多余的脂肪，降低糖尿病、心脏病的发生概率，获得一个健康、快乐和精力充沛的生活。\n现代人类正在面临胰岛素抵抗的危机 胰岛素是胰腺分泌的一种代谢激素。在正常情况下，人体摄入的碳水化合物会转化为葡萄糖，这些糖少部分会直接消耗，绝大多数则会听从胰岛素的安排，从血液去往目的地。其中一部分会去肌肉或肝脏转化为糖原储存，另一部分则去往脂肪细胞转化为甘油三酯。\n现代的食品工业和饮食让人们疯狂地摄入碳水化合物，导致胰岛素得持续地维持工作，来处理血糖的负担。而长时间地持续分泌胰岛素，会最终导致细胞上的受体对胰岛素不再敏感，这也就进入了胰岛素抵抗的状态。很多人也把这种状态称作代谢综合征，会带来一些显著的健康问题：高血压、高血糖、高血脂和腹部脂肪多。同时，还会导致氧化应激、慢性炎症和糖化，而这些被医学界认为是心血管疾病、癌症和衰老的关键驱动因素。\n作者认为，当我们深陷“血糖-胰岛素过山车”的模式中，就会丢掉我们相当重要的一个能力——「代谢灵活性」，即可以根据身体的需要去燃烧各种能量来源的能力，尤其是储存的脂肪。拥有这种能力则让我们整日体感良好，后续的各种健康风险也会大大降低。\n因此，简单地说，我们要避免胰岛素抵抗，重新获得代谢灵活性。\n怎么重获代谢灵活性？ 现在，为了达成避免胰岛素抵抗，重获代谢灵活性的目标，我们当然需要改变日常饮食习惯。但是！可以忘掉计算热量等等的麻烦事，我们只需要关注以下几条：\n避免缺乏营养的精加工食品（尤其是含糖饮料和精制碳水）； 减少进食，开始间歇性禁食； 第一步：从饮食中剔除现代的有害食品 作者认为，现代三大有害食品是添加糖、精制谷物和精炼种子油。这些碳水不仅缺乏营养，同时还大量刺激体内胰岛素分泌，诱导进入胰岛素抵抗的状态。\n但可惜的是，我们现在的饮食文化中这些已经随处可见。米、面、饼等等已经成为绝大多数人家的主食必备，而商店的饮料货架上的绝大多数都是含糖或者代糖饮料，同时各种的甜品和零食也在刺激和引诱我们的食欲。想要完全摆脱这些真的有点难。\n但我们需要好好反思一下这“三巨头”所带给我们的：\n精制谷物：大量的、纯粹的碳水不仅缺乏营养，还增加了我们对糖类的代谢负担，分泌过量胰岛素的危害逐渐浮现； 加工糖和代糖：额外添加，只为满足口舌之欲，实质上摄入体内也在增加代谢负担，让胰岛素过量工作； 精制种子油：非天然的化学组成，通过与天然脂肪分子相似的特性蒙骗了身体，整合进了脂肪细胞中，导致脂肪很难作为燃料燃烧，阻碍了代谢灵活性； 那我们应该多吃什么呢？作者认为我们应该重视以下这些营养密度更高的食物：\n肉、鱼、禽、蛋、海洋生物、动物肝脏、带骨肉、骨头汤 蔬菜：含有抗氧化剂、类黄酮、类胡萝卜素等植物营养素、维生素； 新鲜的应季浆果 坚果：含有蛋白质、脂肪酸、抗氧化剂、维生素及矿物质等； 天然乳制品：避开加工的含糖酸奶、风味酸奶等等； 黑巧克力：富含抗氧化剂和矿物质，尽量选择可可含量至少 70% 的产品，避开牛奶巧克力或半糖巧克力这些糖分炸弹 饮料：水、不加糖的茶和咖啡 以谷物为基础的饮食体系实质上是食品工业推销风行的一种不好的饮食习惯。\n麸质和谷物中的反营养物质，作者认为日常司空见惯的嗳气、腹胀、便秘、短暂性腹痛和偶尔性腹泻其实都是身体对这些“非营养物质”中的毒素的不良反应。\n精炼种子油、精制谷物和精制糖\n通过日常禁食、规避精制碳水化合物和种子油这样的生活方式行为，来降低膳食导致的胰岛素产生，从而向你的基因发出“燃烧脂肪”而非“储存脂肪”的信号。当你成功降低胰岛素时，你就能将储存的身体脂肪作为首选的能量来源，而不是将你摄入的热量当作首选，进而燃烧脂肪。\n第二步：开始间歇性禁食 间歇性禁食是一个比较新的进食理念，即我们人类其实无需早、中、晚三顿饭，甚至期间不停吃零食来满足全天能量。因此，我们可以空出一段完整的时间来减轻身体的消化负担，来让身体有选择地燃烧脂肪，获得代谢灵活性。\n最自然的方式，就是等到饥饿感自然到来的时候才进行进食。通过尽可能长的时间，身体能够产生抗氧化剂（比如谷胱甘肽），同时对优化内部解毒过程，让大脑和身体燃烧脂肪和酮类物质，促进线粒体的功能，同时促进线粒体的合成和效率，可以说，从代谢、免疫和认知功能都有一定的好处。\n更加推荐的初步间歇性禁食方式就是 16-8 禁食法，即每天在 8 小时的进食窗口完成所有食物摄入，然后在剩下的 16 个小时保持禁食状态。比如我就准备通过稍晚的早餐和午餐在 9：00-17：00 完成进食，剩下的时间则留给禁食状态。\n除此之外，在执行过程中还要注意：\n关注身体的饥饿和饱腹的信号，能够更好地筹划自己的饮食进程； 推迟第一顿饭的时间，直到饥饿的时候在再吃，不必拘泥于早餐的理念； 尽量少吃零食，吃零食会阻碍脂肪燃烧，增加胰岛素的波动，还会增加每日的总热量摄入； 在到达自己的消化窗口时间可以刷牙，让口腔保持清爽，不再受到食物的诱惑； 作者时时告诉我们，只要坚持这样的饮食方式 21 天，就会有明显的健康收获。我也打算开始践行作者提倡的方式，同时使用日志记录执行情况和感受，1个月后在此分享一下，间歇性禁食到底如何。\n最后，聊一下为什么不推荐这本书 虽然作者想要传递一些新鲜的饮食理念，但是书中的一些部分实在令人不悦：\n拼凑内容：书中使用了相当多的篇幅来讲解需要配套的运动、睡眠、习惯养成法，甚至冷暴露等等的健康生活方式，但是基本都是东拉西拽，从别人那里拿来的，比如小A的《A书》里讲了什么什么，小B的《B书》又说了什么什么，既然没有自己的东西，也就不用强行塞到书中充数； 强行带货：你会读到在讲解酒类的部分开始推荐 XX 牌的红酒，到了睡眠部分又在推荐 XX 牌的床垫，又让读者去油管检索 XX 教练的运动视频……这些内容的出现无疑降低了科普的说服力度。 这大概也是造成豆瓣评分偏低，短评不满的部分原因。因此，大可不必阅读此书，文中的相关理念也可以通过其他渠道继续深入了解。\n","id":16,"section":"posts","summary":"总结 《一日两餐》想让我们通过限制碳水摄入和间歇性禁食这两个重要手段来摆脱未来的胰岛素抵抗； 书中的大量内容更像是别处观点的拼凑，同时时不时夹杂","tags":["分享","读书笔记","阅读","健康科普"],"title":"【把书读薄04】一日两餐：找回身体代谢灵活性的新饮食理念","uri":"https://www.xzywisdili.com/2022/09/2022-09-26-bookreview04/","year":"2022"},{"content":"总结 《锻炼》通过人类进化学和现代医学研究两方面，对广泛流传的各种健康迷思进行了验证和解释； 关于久坐、睡眠、运动损伤、运动与衰老等等话题，你一定能看到种种新奇的视角和极具说服力的验证； 评分：4.8/5.0，非常难得的健康科普阅读体验，部分章节之后会重新翻看。 社会上流传了许多健康迷思：「久坐会带来健康风险」、「每晚睡眠达到8小时才是最佳状态」、「仅靠步行，无法减肥」等等内容。我们有时候接受了这些理论，仅仅是因为很多人（包括专家）都这么讲，但却从来没有思考这些说法究竟有没有道理、原理是什么。而丹尼尔·利伯曼想用《锻炼》这本书尽量把这些健康迷思讲清楚。身为哈佛大学的进化生物学教授，他带来了进化生物学的视角，从人类的进化讲起，到现代医学研究阐释的原理。看完这本书，我是真的感觉自己打开了眼界，又被里面的原理和验证所说服。\n避免非必要的身体活动才是进化的结果 首先有一个最基本的观点（当然可能各位已经在生物课上学习过），即「人体在静息状态下依然在消耗能量。」更重要的是，这部分能量并不低，静息代谢消耗的能量要占据每日总热量消耗的 60% 到 75% 之多。\n静息状态的能量消耗主要来自于各个器官的辛勤工作。比如心脏在不停收缩、泵血当然需要能量；肠胃在消化，肝肾在调控和过滤血液当然需要能量；大脑在思考，更需要能量……静息状态，虽然我们看上去什么都没有做，但是各种维持生命的生理活动一直在进行，并且消耗能量。\n而在进化过程中，人体也学会了怎么分配能量。在从类人猿进化到人类的过程中，我们深知能量的重要性（当然，没能量就会嗝屁）。而在能量有限的情况下，分配的优先级就很重要了。而人体实际上可以把能量花在以下5个方面：身体成长、生命维持（静息代谢）、能量存储（例如，以脂肪形式存储）、运动、繁衍。\n在进化生物学的理论中，「生命维持」和「繁衍」是这五项里最最优先的两项，这也符合我们的认知：没了生命维持就嗝屁了，而繁衍是刻在基因里的种族任务……所以，其实作者想表达的结论就是：\n「避免非必要的身体活动，才是进化的结果。」\n换句话就是，「我们生而懒惰」。当然，在这个语境下，「懒惰」已经不是一个贬义词了，更象征着人类在漫漫进化长河中的一种实用的适应。\n那么问题来了，既然所谓的「懒惰」是人类的正常选择，而「久坐非常不利于健康」的说法也流行已久了，这两个观点又该如何理解呢？\n久坐是怎么带来健康风险的 其实关于久坐有两个已经公认的事实：\n现代人每天久坐的时长要比过去更长。 统计显示现代人平均每天坐的时长占非睡眠时间的 55%-75%，即 9-13 个小时； 站姿或者其他活动要比坐姿消耗更多的能量。 很好理解，因为牵涉到肌肉的伸缩和协调。 这样一来，久坐和健康风险的关系链就呼之欲出了。久坐让“运动”这一部分的能量节省了下来，而能量不会凭空消失，就会存储成为脂肪的形式；脂肪细胞的膨胀招致了白细胞前来并向血液释放了大量炎症细胞因子，形成了慢性轻度炎症。简单一句话总结：久坐会让你变胖，而变胖会带来健康风险。\n现在的问题是，怎么解决这样的健康风险？研究告诉我们，只要保持了久坐的习惯，哪怕你坚持在下班时间进行锻炼，也依然于事无补，这部分健康风险是无法抵消的。所以只剩下了两个选择：\n间断性起身做轻量运动； 折中的方法：“不安分” 地坐着 这里的原理在于，保持这种“不安分”的活动会让肌肉保持收缩，然后燃烧掉血液中糖和脂肪的能量；另外，肌肉受到的刺激，能够带动抗炎反应，消除体内的炎症。\n当然，这只是开胃菜，后续精彩纷呈 久坐只是作者论述的第一个话题，而从此篇也逐渐渐入佳境。在接下来，作者对于「8小时睡眠理论」、「步行减肥理论」、「奔跑的速度与耐力」、「锻炼如何延缓衰老」等等话题一一进行了讲述。而且你能明显地感觉到，每一个话题都并非泛泛而谈，而是经过了作者深入的思考。\n让我印象最深，也最有趣的例子，莫过于「体育与人类自我驯化」。\n体育其实是人类自我驯化的产物 这里先要理解两个概念：「应激性攻击」和「主动性攻击」\n高应激性攻击：你只要惹我，我不过脑子立马就干你，行为不受控制； 低主动性攻击：完全由大脑主导和控制，通过策划行动、互相协作、谋求时机来进行攻击。 在还未完全进化的类人猿形态，其实就是高应激性攻击和低主动性攻击。但是进化过程中，拥有团队协作能力进行捕猎的人类和受到异性青睐的人类逐渐在自然选择中赢得胜利，慢慢地变成了低应激性攻击和高主动性攻击。\n这个过程其实和野狗驯化成家狗的过程有一定相似性，换句话说，就是「人类的自我驯化」。\n而早期的体育活动，很大程度上其实就是人类在训练自己的狩猎技巧、生存技巧和团队协作能力，这样又能使种族更好地生存和繁衍。你会发现，其实体育有一个非常关键的特征：就是对于应激性攻击行为的自我控制。绝大多数体育规则中，场上队员对对手的粗暴的应激性攻击都是违反规则的。\n对锻炼有一个正确的认知 其实通读完本书的最大收获就在于，对「锻炼」这件事的理解更深刻了。\n锻炼是近代诞生的产物，我们从没有进化出锻炼的本能，但是锻炼本身对我们身体是有益处的。规律性的身体活动是延缓衰老和延长寿命的最佳方式。当我们再回过头重新审视这一切，我们就不会对那些不喜欢锻炼的人进行苛责，同时喜欢锻炼的人也能明白锻炼到底给我们带来了什么。不过分夸大锻炼的功效，但也需要承认锻炼的益处，我认为这才是看待「锻炼」这件事最正确的态度了。\n","id":17,"section":"posts","summary":"总结 《锻炼》通过人类进化学和现代医学研究两方面，对广泛流传的各种健康迷思进行了验证和解释； 关于久坐、睡眠、运动损伤、运动与衰老等等话题，你一","tags":["分享","读书笔记","阅读","健康科普"],"title":"【把书读薄03】锻炼：丹尼尔·利伯曼教授对健康迷思的一一解答","uri":"https://www.xzywisdili.com/2022/09/2022-9-21-bookreview03/","year":"2022"},{"content":"总结 介绍了 Logistic 回归模型构建过程中的一种变量筛选方式——先单后多； 配合对应的代码实例和结果解释，让方法更加清楚，同时便于日后复习查看。 在 Logistic 回归模型的构建过程中，面对的第一个问题是变量筛选，即筛选哪些作为预测因子进入到最后构建的模型中。其中最为基础的方法是 「先单后多」，顾名思义，即先进行单因素分析，再将单因素分析中具有统计学意义的变量再一起纳入多因素模型中。这样的方法最为简单和实用。\n接下来会通过 R 语言代码的实例来演示整个过程。数据采用的是 [Framingham 十年冠心病风险数据](Framingham_CHD_preprocessed_data | Kaggle。\nStep 1 数据的读取和预处理 第一步，当然是读取数据并且通过 names 、summary 和 str 分别查看数据集的变量名称、基本统计信息和变量的类型。尤其是后两者，能够提供变量的数据类型和统计信息，有助于发现异常的数据，在分析之前发现原因提前纠正。\n# 导入数据，命名为 data library(readr) data \u0026lt;- read_csv(\u0026quot;framingham.csv\u0026quot;) # 查看变量名称 names(data) # 查看基本统计信息 summary(data) # 查看变量类型 str(data) 接下来对数据做基础的预处理。对于医学数据，需要注意的预处理主要是对于分类变量的转换：\n将分类变量转换为 factor； 二分类变量处理或者不处理不影响结果，多分类变量则一定要进行处理，为了保险和便利起见，尽量都进行处理； 转换中，使用 labels 标清楚各个分类代表的含义，之后不容易发生混淆。 # 标明连续型变量和分类型变量 contin_vars \u0026lt;- c(\u0026quot;age\u0026quot;, \u0026quot;cigsPerDay\u0026quot;, \u0026quot;totChol\u0026quot;, \u0026quot;sysBP\u0026quot;, \u0026quot;diaBP\u0026quot;, \u0026quot;BMI\u0026quot;, \u0026quot;heartRate\u0026quot;, \u0026quot;glucose\u0026quot;) discre_vars \u0026lt;- c(\u0026quot;male\u0026quot;, \u0026quot;education\u0026quot;, \u0026quot;currentSmoker\u0026quot;, \u0026quot;BPMeds\u0026quot;, \u0026quot;prevalentStroke\u0026quot;, \u0026quot;prevalentHyp\u0026quot;, \u0026quot;diabetes\u0026quot;) # 处理分类变量 data$TenYearCHD \u0026lt;- factor(data$TenYearCHD, levels=c(0, 1), labels = c(\u0026quot;未来10年无冠心病风险\u0026quot;, \u0026quot;未来10年具有冠心病风险\u0026quot;)) data[discre_vars] \u0026lt;- lapply(data[discre_vars], factor) Step 2 单因素分析 一般而言，单因素可以采用的统计分析方法包括：t 检验、卡方检验和秩和检验。t 检验需要满足数据符合正态分布，如果不满足则需要考虑秩和检验这种非参数检验。另外卡方检验则一般面向分类的数据，而非定量数据，比如性别（1代表男，2代表女，数字无比较意义）。\n# 对连续变量进行 t 检验 # 对分类变量进行卡方检验 library(tableone) table1 \u0026lt;- CreateTableOne(vars=c(contin_vars, discre_vars), data=data, factorVars=discre_vars, strata=\u0026quot;TenYearCHD\u0026quot;, addOverall=FALSE) result1 \u0026lt;- print(table1, showAllLevels=FALSE) write.csv(result1, \u0026quot;result1.csv\u0026quot;) 这时，可以看到 tableone 的输出结果。可以看到，按照连续型变量和分类变量展示了不同统计结果（Mean(SD)/占比）。并且在最后一列给出了统计学的检验结果 P 值。\n这里可以补充几个参数的说明：\naddOverall：在列上展示所有出 Overall 的情况。 showAllLevels：如果设置为 TRUE，展示所有变量的所有水平； nonnormal：不满足正态条件，可以指定确切进行秩和检验的变量，之后在结果会显示中位数、第一四分位数和第三四分位数； exact：不满足卡方条件，可以指定确切概率检验的变量。 Step 3 单因素的 Logistic 模型分析 在完成上述分析之后，可以开始进行多因素模型构建。有时，我们会先进行单因素的 Logistic 模型分析，然后再做多因素的 Logistic 模型构建。这样的好处在于，我们可以观察到对于某个单个变量的 Crude OR值（俗称的粗 OR值）和调整后 OR 值（adjustment OR）。\n这里举一个变量的例子：\n# 单因素 logistic # 如果存在分类变量，需要设置哑变量 # 自变量: 年龄 model \u0026lt;- glm(TenYearCHD~age, data=data, family=binomial()) summary(model) summary(model)$coefficients # 计算 OR 及其可信区间 exp(cbind(\u0026quot;OR\u0026quot;=coef(model), confint(model))) 这里结果显示了自变量 age 的参数值、标准误、z 值以及 P 值。可以看到，P 值明显小于 0.05，说明「年龄」这个自变量在单因素 Logistic 回归里是有意义的。\n再来看下面的 OR 值，为 1.079 (1.067, 1.090)，可以解释为，年龄每增加 1 岁，未来10年的冠心病发生风险是之前的 1.079 倍。\n对于一个分类变量「糖尿病」来说：\n# 自变量: 糖尿病合并症 model \u0026lt;- glm(TenYearCHD~diabetes, data=data, family=binomial()) summary(model) summary(model)$coefficients # 计算 OR 及其可信区间 exp(cbind(\u0026quot;OR\u0026quot;=coef(model), confint(model))) 与上述同理，不过这里的 OR 值 3.398 (2.250, 5.064) 可以解释为，具有糖尿病合并症人群未来 10 年的冠心病发生风险是无糖尿病人群的 3.398 倍。\nStep 4 多因素的 Logistic 模型分析 常规方式 根据上面单因素分析的结果，如果某变量的 P 值一般小于 0.05，就可以将该变量纳入多因素模型中。实际应用中，也可以将纳入标准放宽，即 P 值小于 0.1，就纳入多因素模型。\n# 多因素模型分析 model \u0026lt;- glm(TenYearCHD~var1+var2+var3+...+varn, data=data, family=binomial()) summary(model)$coefficients exp(cbind(\u0026quot;OR\u0026quot;=cord(model), confint(model))) 这里就不展示结果了。得到的 OR 值可以认为是调整后的 OR 值。解释的结果和上面也大同小异。\n有时候会发现，部分之前单因素分析时存在统计学意义的变量，在多因素中则不存在统计学意义。这很可能是因为存在混杂偏倚，而多因素分析在一定程度上控制了混杂偏倚。\n另一种非常规的方式 假如我们已经确定了要研究的是某个因素（就叫做主因素吧）和因变量，也就是结局之间的关系，我们应该如何筛选协变量呢？\n如果某个协变量与因变量之间单因素分析的 P 值小于 0.1，并且当纳入主因素和协变量同时进行分析后，由于协变量的存在，主因素的系数相比之前只有主因素时，变化超过了 10%，那么就说明这个协变量其实是对主因素的系数有一定程度的影响，应该纳入到多因素分析中。这里可以采用下面的代码：\nuni_methods \u0026lt;- function(xvar) { model \u0026lt;- glm(TenYearCHD~diabetes, data=data, family=binomial()) coef \u0026lt;- coef(model)[2] form \u0026lt;- as.formula(paste0(\u0026quot;TenYearCHD~diabetes+\u0026quot;, xvar)) model2 \u0026lt;- glm(form, data=data, family=binomial()) coef2 \u0026lt;- coef2(model2)[2] ratio \u0026lt;- abs(coef2-coef1)/coef1 if (ratio \u0026gt; 0.1) { return(xvar) } } xvar \u0026lt;- c(contin_vars, discre_vars) xvar \u0026lt;- xvar[-which(xvar==\u0026quot;diabetes\u0026quot;)] lapply(xvar, uni_methods) 这里我们确定变量 diabetes 作为我们的主因素，然后筛选其他协变量。只要 ratio ，也就是系数的变化幅度大于 10%，就可以在最后的结果中显示出来。\n参考资料 Logistic 相关课程讲座 R语言逻辑回归（Logistic回归）模型分类预测病人冠心病风险 Framingham 数据集 ","id":18,"section":"posts","summary":"总结 介绍了 Logistic 回归模型构建过程中的一种变量筛选方式——先单后多； 配合对应的代码实例和结果解释，让方法更加清楚，同时便于日后复习查看。 在 Logistic 回归模","tags":["R语言","临床数据分析","医学预测"],"title":"【医学预测01】Logistic 回归构建变量筛选：先单后多","uri":"https://www.xzywisdili.com/2022/09/2022-09-07-rlogistic01/","year":"2022"},{"content":"总结 《开启高质量沟通的第一分钟》目标是让职场工作沟通的双方可以通过一段简短的引言快速进入良好的沟通节奏； 书中凝练了一套沟通结构：沟通框架（背景 + 意图 + 关键信息）+ GPS 概述法（目标 + 问题 + 解决方案）； 评分：4.5/5.0，前半部分讲述框架细节非常有价值。 职场工作交流中，最希望的对话方式是简明扼要，沟通双方同时理解背景，并且谈话目的清晰。但是现实中往往出现谈话者讲了很久，聆听者却始终不明白其意图和表达主题的情况。因此，本书提炼了一个在正式交谈前的引言框架，用大约 1 分钟的时间，让职场沟通进入一个良好且高效的沟通节奏。\n引言可以用两个部分讲完 书中介绍的引言可以凝练成以下的这条公式：\n沟通框架（15秒，最好3句话讲完） 背景：希望讨论的主题，让对方的注意力集中； 意图：希望对方接收到你的信息之后怎么做； 关键信息：类似内容提要，凝练总结所有内容中最重要的部分。 GPS 概述法（45秒） 目标（Goal） 问题（Problem） 解决方案（Solution） 沟通框架 = 背景 + 意图 + 关键信息 这里，「沟通框架」部分需要重点拆解一下。\n「背景」，目的是让对话的双方能够第一时间聚焦在同一个主题中。一般而言，内容为：要讨论的项目名称、要讨论的某个客户或订单、要讨论的某个流程环节等等，比如「我认真查看了你发送给我的开发需求文档。」\n「意图」，就是直截了当地说清楚，希望对方接收到你的信息之后怎么做。一般包括：需要提供意见、需要对方决定、向对方进行汇报或告知等，比如「文档中有一些细节需要您进一步核实。」\n「关键信息」，是谈话对象必须知道，最为重要的那句话，也是最重要的中心思想的概括。尽量需要凝练到简短的一句话里，比如「需求列表的排期不符合我们的工作安排。」\n这样进行组合就能形成一个比较凝练的沟通框架：「我认真查看了你发送给我的开发需求文档，文档中有一些细节需要您进一步核实。里面的需求列表的排期不符合我们的工作安排。」这样，用 3 句话就能够让对话双方进入明确接下来的沟通重点。\nGPS 概述法帮助沟通双方着眼于解决方案 GPS 概述法则更为简单，也更容易记忆，包括三个部分：目标（Goal）、问题（Problem）和解决方案（Solution）。\n举一个简单的例子：我想要拿到对岸的旗子（目标），但是中间有一条河无法跨越（问题），认为需要建桥跨过障碍（解决方案）。\n从本质上来说，几乎所有的工作沟通都是为了解决问题。所以 GPS 概述法的核心也是为了让谈话聚焦在解决方案上，无论是找到解决方案还是对现有方案进行调整或者修改。\n实际应用：先分列点，再组合起来 在了解了上面讲述的沟通的各种组件之后，如果想要实际应用，可以先把每个组件罗列出来，然后再组合成沟通的引言。举一个书中提到的实际例子：\n一位技术分析师在学习了政府关于支付数据防火墙的指南后，与IT负责人就该指南进行讨论。采用沟通框架和 GPS 概述法进行构建：\n背景：我认真了解了新信息安全政策。 意图：我们必须有所行动。 关键信息：我们的防火墙不再符合要求。 目标：新行业规范要求所有商业交易必须有5道防火墙，以确保支付数据的安全。 问题：我们当前的软件只能提供4道防火墙。 解决方案：我们必须制订软件升级计划并且提交领导层审批。 然后进行组合：「我认真了解了新信息安全政策。我们的防火墙不再符合要求，我们必须有所行动。新行业规范要求所有商业交易必须有5道防火墙，以确保支付数据的安全。可是我们当前的软件只能提供4道防火墙。我们必须制订软件升级计划并且提交领导层审批。」\n这样就是一个非常优秀且高效的沟通方案。\n这是一种实操性相当强的沟通技巧 在本书的后半部分，则介绍了多个主题的沟通法则，以及在发邮件、邀请函和主持会议等等场景下如何利用这种沟通技巧。 包括结尾还精心准备了一些场景和习题让读者练习。 总体来说，通过书中的多个案例分享和实操指导，能够感觉到是一种可以快速应用到实际工作中的技巧。 真正应用起来的话，我感觉难点主要还是在考验自身概括与凝练的能力。 如何通过寥寥数语能够让沟通双方对于接下的解决方案有一个共性的认识而不产生误解，这也需要多加修炼。\n","id":19,"section":"posts","summary":"总结 《开启高质量沟通的第一分钟》目标是让职场工作沟通的双方可以通过一段简短的引言快速进入良好的沟通节奏； 书中凝练了一套沟通结构：沟通框架（背","tags":["分享","读书笔记","阅读"],"title":"【把书读薄02】开启高质量沟通的第1分钟","uri":"https://www.xzywisdili.com/2022/09/2022-09-05-bookreview02/","year":"2022"},{"content":"总结 《拼凑真相》提出了 10 条所谓的数据法则，在我看来只有 6 条比较实用； 书中使用了很多历史上的事例作为论据，但是对于每种观点的详细论证不够； 《拼凑真相》评分：3.0/5.0，不推荐专门抽时间阅读。 在现今的网络时代，各种传统媒体和自媒体生产的观点层出不穷。这其中的大多数都是为了调动读者情绪，进而博取流量的错误观点。低端一点的可能仅仅是利用聊天记录和阴谋论炮制观点。高端一点的就是通过参杂数据和统计分析，看似在理，实际上则通过部分手段营造假象。《拼凑真相》这本书就是在引导读者，在面对这些纷繁的数据观点时，如何做到距离真相更近一些。\n十大法则其实只有 6 个比较实用 《拼凑真相》这本书的副标题是「认清纷繁世界的十大数据法则」。但通篇读下来，感觉只有 7 个比较实用。\n控制情绪、放下个人偏见。有时候看到与自己观点相符的分析结论，就会更容易偏信而忽略其中的问题；看到相悖的观点则反之。这时候需要先平复心情，放下偏见地去审视结论的得出过程是否有漏洞； 举例：习惯喝咖啡的人群了解到咖啡的相关健康风险时候，第一反应偏向于不太相信。\n慎重结合自己个人经验。如果遇到和自己个人实际体验不一致的情况，不要轻易采信任何一方，而是分别审视官方的数据来源和统计方法是什么，我自己的个人体验是否有局限性。 举例：自己乘公交车十分拥挤，但是官方表示平均乘车人数不是很高。可以审视自己乘公交车是否集中在某个时段，官方统计的时段区间是什么。\n确定每个统计量背后的准确含义。这一点非常重要。通常我们会看到一些陌生的统计量，或者比较空洞的大词，不妨调查一下这些词的准确含义是什么。 举例：同一个统计量“新生儿死亡率”可能在不同的城市或者医院，由于对新生儿的定义（周数）不同，导致最后的统计数据也发生了差异。\n注意指标之间是否有可比性。某些统计量之间并不能直接比较，强行比较当然会带来错误的结论。 举例：比较城市A和城市B的相对安全性，因为两个城市人口密度不一样，不能直接用凶杀案数量比较。而用一个比率，比如每百万人的凶杀率更合理。\n考虑统计样本是否全面，理解幸存者偏差。部分样本由于某些原因根本没有发声的机会，导致能让你看到的是特定的一批样本。读者可以反问：有没有漏掉一些样本？ 举例：记者到候车大厅采访大家有没有买到返乡的火车票。\n不要迷信大数据和相关算法。大数据在大多数情况下算法和数据来源并不透明，而小数据统计往往更容易进行核实和检验。 举例：谷歌的流感趋势预测，仅凭算法预测流感发生趋势，但是却错误地将“冬季球赛”的检索和“流感发生”进行了错误关联，并且没有剔除，导致后期的一些预测发生了较大误差；\n最后作者表明了：\n图表是双刃剑，容易通过别有用心的设计产生误导； 提倡让民众自发产生好奇心，学习相关的专业知识，具有一定的判断力； 同时关注官方统计机构发布的相关数据和结论，保护捍卫统计公正性的学者，唾弃那些故意哗众取宠、操弄数据的“假科学家”。 但是很明显，作者没有讲解图表常用的误导方式。而后两点也更像是作者个人政治观点的主张。从读者的角度来说，并不具有很强的实操性。以至于让我觉得，可能只是单纯地想要凑齐 10 条法则这个数，而强行找补的内容。\n应用到实际中的手段：批判式提问 读完此书，我认为之后再遇到一些观点的表述时，最好用的方法就是：批判式提问。\n举个例子，当我们看到这样一条：「经过对学生的数据分析，我们发现喜欢吃椰子的学生的成绩比较好」的时候，我们就不会轻易相信，而是可以一步步提出以下问题：\n喜欢吃椰子的定义是什么？按照食用频率？ 成绩的定义又是什么？语数外成绩？还是水果知识大赛成绩？ 学生数据的样本是什么？随机抽取的某所学校？还是椰子学园的学生？ 统计方法是什么？是否有影响成绩的其他因素？是否控制了这些因素？ 我身边的生活经验，是否存在这样的现象？上述结论和我的生活经验有何异同？ 只要能够多思考，多提出问题，再去找原来观点的“茬”，我想应付绝大多数自媒体的信口胡诌，应该是绰绰有余了。\n","id":20,"section":"posts","summary":"总结 《拼凑真相》提出了 10 条所谓的数据法则，在我看来只有 6 条比较实用； 书中使用了很多历史上的事例作为论据，但是对于每种观点的详细论证不够； 《拼","tags":["分享","读书笔记","阅读"],"title":"【把书读薄01】拼凑真相：指导读者如何面对各种诡计多端的媒体把戏","uri":"https://www.xzywisdili.com/2022/08/2022-08-29-bookreview01/","year":"2022"},{"content":"我们在很多情况下都会想要可视化一天中的周期时间来探索发现可能存在的某种模式。 比如根据犯罪数据发现在一天中的高发时段，又或者某个app的用户活跃时段的表示，又或者患者在院外一天中的健康作息的可视化，都不可避免会碰到要把数据在周期性的时间上进行展示。 而目前这样的可视化方向大概有两个：线性展示和环形展示。\n线性展示 线性展示是目前使用方式相对比较多的可视化方式，如下图分别展示的对于人群购物时间的可视化和运动传感器回收数据时间分布的可视化：\n如此使用折线图和条形图来对数据在一天中的时间分布的优势显而易见：\n符合读者的直觉； 绘图比较简单； 但是这种方式也有一些局限性，即没有办法很好体现出时间的连续性。 它们的 X 轴出于限制，都不可避免地选择两个时间点作为图表的开始和结束。 而这个时间点的选取也需要根据数据的实际情况来进行。 比如通用的时间制会规定一天的24小时开始于 0:00，结束于 23:59，但对于很多活动并非如此（比如部分职业的加班或者犯罪往往在午夜时分保持连贯，甚至有可能延伸到凌晨2-3点）。 有时候图表所显示的内容强行断开有时候也会让部分想要探索该处连续性的读者没有办法直观看到。 此外，大众对于上午、下午、徬晚和晚上都在心里有一个普遍共识的观念，这也没有办法在线性图中直观看到，除非作图者显性标注出来。\n以上这些种种的问题想要解决其实比较棘手。曾有人使用双层时间图来进行展示：\n上半部分依然是正常的线性可视化，但是作者将上半部分反转再进行平移，想要以此来同时展示白天（上半部分）和夜间（下半部分）的时间趋势，从一定程度上解决连续性表现不足的问题。但是这样在一定程度牺牲了可视化的直观性，用户可能会觉得上下是两个系列的数据；另外也没有从根本解决问题。\n所以有些人开始尝试另一种可视化思路。\n环形展示 不知道环形展示方式的最初灵感是不是来自钟表的表盘。但钟表代表时间，使用钟表表盘式的圆形尝试来进行一天内趋势的可视化似乎也合情合理。 我自己也在 Excel 里面尝试了一下。 只要将数据转换合理，再使用 Excel 里面内置的雷达图，就能做出近似环形的可视化效果。 但是很快就发现了问题：传统的符合人类直觉的钟表表盘是 12 小时制的，而一天有 24 个小时。 如果强行将 24 个小时挤入一个环形圆盘也会显得非常怪异，比如 Magic Eye 的设计中，\u0026ldquo;Morning\u0026rdquo; 到了右下象限而 “Evening” 到了左上象限：\n这时候有人开始尝试这些妥协式的设计，即使用双表盘、在一个表盘中利用不同颜色来展示上12小时和下12小时、又或者用内圈和外圈来代表 AM 和 PM：\n甚至还有一种使用无限符号的可视化方式，想要表现完整的一天：\n可以看到，环形的方式进行可视化展示的优点包括：\n能够带给读者直观的“上午”、\u0026ldquo;下午\u0026quot;以及“晚上”这种结合生活时间段的感受； 部分解决了连续性的问题； 但是又带来了新的问题：\nAM 和 PM 本身需要颜色或内外圈多维展示，如果有更多维度或系列的数据，就会显得更加杂乱； 作图难度相对于线性方式来说更高； 数据映射到极坐标下，增长和减少的趋势可能也会随之扭曲。 综合以上，我觉得还是优先考虑使用线性的方式进行展示一天内的数据分布情况更为合理。 更为简便的作图方式，展示作图者想要展示的趋势或者模式，就已经足矣。 如果使用环形可视化很容易耗费更多的精力，反而只能做到华而不实的效果，也可能被误导，很难真正探索出数据背后真正的时间分布情况。\n","id":21,"section":"posts","summary":"我们在很多情况下都会想要可视化一天中的周期时间来探索发现可能存在的某种模式。 比如根据犯罪数据发现在一天中的高发时段，又或者某个app的用户活","tags":["分享","可视化"],"title":"一天内数据分布的可视化","uri":"https://www.xzywisdili.com/2022/07/2022-07-28-timeseriesplot/","year":"2022"},{"content":"感谢 CXW 同学的邀请，能一起在跨年那天晚上看一场现场的爵士大乐团演出。超棒的音乐，超棒的食物和环境，送走 2021 的最后一天和新的 2022 年（当然今天才发，一定不是因为我太懒哈哈）。\n这样的爵士演出如果用两个字总结就是「惬意」。所有乐手似乎都有一定的自由度，但又似乎紧紧地联系为一个整体，构成整个和谐的舞台。就好像跳舞的舞者，每个人可以有自己独特的动作，而且组合在一起依然赏心悦目。同时，表演的不同曲目又给到了不同乐手的 solo 段落，其它乐手作为陪衬。轮流表演，相辅相成，实在是太有趣了。就这样一首接着一首，时间飞速流逝……\n我也想到了过去几年的跨年，都是和 LXZ 同学出去到一家固定的日料店，吃到将近关门。这似乎也成为了近几年的保留节目。每次跟他的畅谈都能感受到他的一些坚定的力量，也让人对生活还是抱有热情。2021 毕业虽然分道扬镳，也照例第一时间问好。也希望他 2022 一切都好。\n","id":22,"section":"posts","summary":"感谢 CXW 同学的邀请，能一起在跨年那天晚上看一场现场的爵士大乐团演出。超棒的音乐，超棒的食物和环境，送走 2021 的最后一天和新的 2022 年（当然今天才发，一","tags":["分享","记录"],"title":"2022 跨年快乐","uri":"https://www.xzywisdili.com/2022/01/2022-01-04-happynewyear/","year":"2022"},{"content":"从一本书说起 最近的机缘巧合之下读到一本书，叫《如何成为一个会学习的人》。 一来每隔一段时间就需要读一读这种方法书打打鸡血，二来也听说日本人写这种书是有一手的。 但读完还是比较失望的，除了作者一句话一段的叙述方法和他缺少论证某种方法有效的逻辑之外，最大的缺点还是介绍的绝大部分方法基本还是陈词滥调。\n其中作者最核心的观点是要找到学习的乐趣，让自己沉迷其中，然后能够全身心专注其中。之后展开的各种方式都是围绕这一个核心所展开的。 这个核心虽然表面听上去无可指摘，但是实际上在东亚的此类应试教育中真的太难应用了。 除去少部分学霸人群之外，绝大多数的人在面对语数外理化生史地生等等数门课时，几乎很难对每一门都感兴趣。 可能一部分人只对其中某些课感兴趣，成为偏科的学生；而甚至有人所有课都不感兴趣，这样的话在学校里学习当然是一件极为痛苦的事。 当然，我的意思不是说这些学生是有问题的，相反，我觉得现在的课程安排可能需要一些思考。\n我所畅想的课程安排 可能很多人在上学时都或多或少想过这样一个问题：我学的这些未来会有什么用？而大部分的老师会告诉你“不要想这些乱七八糟的”。 甚至我还听到过“无用之用，方为大用”这样的玄学说法。但经历了初中高中到大学毕业，我感觉到的事实就是，当初上学学的很多课，是真的没有用、用不到的。 如果教育的目的是让一个人成为自我完整的人、对社会有用的人，我个人认为以下这些课是最重要的：\n写作与表达**：主要是教导如何通过口头和书面准确通顺地表达自己的意思，能够让读者清晰地获取信息； 情绪和压力管理：让学生可以感知到自己现在的情绪和可能面对的压力，并且能够自我控制； 人体和健康基础：基础的医学健康知识，了解真正健康的生活方式； 性教育和两性关系：正确的性别生理知识和性别观念，以及男性和女性良性相处的相关观念； 体育运动：绝对不可缺少的一环，学生应该获得足够的户外体育活动时间； 思想品德：一个善良、正义的人的必备素质； **逻辑学基础 数学基础 英语 这些是我认为一个人应该提前学到，并且会受益终生的内容。所有这些在步入社会都能用的到。相反，这些在现在的教学安排是极为匮乏的，体育运动仅限于课间跑操和一周两节的体育课。 健康的两性相处观念和基本的逻辑教育完全缺乏；而在未成年人抑郁和自杀的增加的同时，情绪和压力管理也没有跟上。诚然，这些项目都很难考核，也不适合现行选拔性考试的应试教育； 也许我的这些想法也像是痴人说梦。但我真希望有一天我们真的不需要这样的应试教辅书和方法书了。\n","id":23,"section":"posts","summary":"从一本书说起 最近的机缘巧合之下读到一本书，叫《如何成为一个会学习的人》。 一来每隔一段时间就需要读一读这种方法书打打鸡血，二来也听说日本人写这","tags":["分享","记录"],"title":"读完一本应试方法书的碎碎念","uri":"https://www.xzywisdili.com/2021/12/2021-12-18-bookthoughts/","year":"2021"},{"content":"最近看到了一种新的减重饮食方法（本身不是很新，只是我最近才看到），感觉容易执行且负担并不是很大，所以打算开始试试。这种饮食方法是间歇性断食，简称「轻断食」。其主要分为两种类型：\n限时进食：即每天只在有限的几个小时内进食，比如「186 断食法」等 全天断食：即定期断食一整天，比如「52 轻断食」等 可以说，全天断食是比限时进食更进阶的一个版本，其中最有代表性的就是「52 轻断食」，即一星期当中任选不连续的两天控制饮食，轻断食的那天，女性最多摄取 500 大卡，男性则最多摄取 600 大卡。所以，所谓「断食」的含义并不是不进食，而是在原先的饮食结构上，进食更少的分量。并且也不只是单纯依赖断食减重，还是需要定期进行运动，增加热量消耗。\n这种「轻断食」和传统的饮食减重方式相比，优势主要有以下三点：\n对比持续节食：当你吃得太少，饮食摄取热量低于身体基础代谢率时，身体就会启动保护机制，减少不必要的热量消耗，降低基础代谢率，同时身体还会燃烧肌肉来获得能量，导致肌肉的流失；而肌肉的流失又会降低基础代谢率，形成连锁反应，也就造成「易胖体质」；而轻断食可以摄入足够维持基础代谢的热量，再通过运动等使每日活动热量超过基础代谢率，身体便会燃烧脂肪进行减脂； 对比低糖饮食：低糖饮食虽然能达成减肥效果，但是长期执行可能对身体造成伤害，尤其当身体缺乏糖分时，脂肪无法正常燃烧会产生酮体，而大量酮体不但对大脑有伤害，对心脏、肝脏和肾也会造成损害；而轻断食提倡保持均衡饮食结构，糖分也会正常摄入，可以长期持续保持； 对比白开水断食、流体断食和蔬果断食等：轻断食保持均衡的饮食结构，包含全类食物和六大类营养素的全面摄入，不用特别禁食某种食物； 这种饮食方法简单且更有吸引力，能够长期执行，参与者也更容易坚持和保持这样的饮食习惯，比如在一周中比较忙碌的两天，就可以尝试这样的饮食方法。 而在「52 轻断食」的基础上，有对饮食结构更加明晰的「12345 饮食法」，即正常饮食的 5 天里：1 餐 500 大卡，其中蛋白质占 25%、淀粉 35%、脂肪 40%。这里每一项的具体含义如下：\n一餐 500 大卡：一天的总热量就控制在 1200-1500 卡以内，可以维持身体的基本新陈代谢，再搭配运动消耗热量燃烧脂肪，就能变瘦； 25% 的蛋白质：可以避免肌肉流失，尽量选择优质低脂蛋白质，比如鸡蛋； 35% 的淀粉：吃对糖比不吃糖更能维持减重效果而且健康，但此处糖并非精制糖类，推荐吃全麦类淀粉； 40% 的脂肪：保证身体需要的必需脂肪酸，帮助稳定血糖，增加饱腹感。 而在需要轻断食的两天里，将原先的一餐，分到整天吃，就可以保证一天只摄入 600 大卡。当然，想要减重不单单只依赖饮食，同时需要进行充足的运动。 我已经准备开展「52 轻断食」减重实践了，目前选定周一和周六进行断食。希望能坚持下去取得好的效果。\n","id":24,"section":"posts","summary":"最近看到了一种新的减重饮食方法（本身不是很新，只是我最近才看到），感觉容易执行且负担并不是很大，所以打算开始试试。这种饮食方法是间歇性断食，","tags":["分享","记录"],"title":"最近在尝试的一种新的减重饮食法","uri":"https://www.xzywisdili.com/2021/11/2021-11-29-newdietmethod/","year":"2021"},{"content":"上周五的时候，我去旁听了数字疗法峰会北京站的一些分享。数字疗法是今年兴起的一个新的行业，蕴藏着无数的机会和可能，当然也已经有很多的公司已经投身于此，做出了一些产品。听完了这次的分享，我有以下三点感想：\n1 目前赛道的产品瞄准的方向比较集中 目前做数字疗法的公司很多都在做关于脑科学、认知科学和解除成瘾方面的一些诊疗工具。 比如通过移动 APP 里的认知小游戏来对患者的认知状态进行评估，或是通过软件来对已经患有相关疾病的患者进行辅助治疗。 而检索国外的产品也发现，目前数字疗法也多集中于认知科学领域和控制糖尿病人血糖方面。这一方面说明可能数字疗法确实在这些领域取得了一定的效果，也有可能说明数字疗法在其他种类疾病的辅助诊断和治疗方面的开拓依然不够。\n2 目前展现产品已初具雏形，但仍需要打磨 能够发现目前展示出来的产品已经能够投入到临床的使用，但我认为还是有进步的空间。 比如针对不同的患者，如果提供个性化的数字疗法方案；如何利用已覆盖的患者数据和结局来对产品进行快速的更新迭代；如何有效地将患者家属和医生的参与感提高并调动起来，这些方面依然没有看到更多的进步。\n3 「数字疗法」的大众认知程度和普及度不高 这一点尤为明显。当我告诉同为医学硕士或博士的同学的时候，他们都不由得反问：「什么是数字疗法？」更遑论普罗大众了。「数字疗法」这个名词目前在国内市场的大众认知程度和普及程度并不高，这样一定会导致大家对其的接受程度依然不高。这一方面可能需要各家公司通力协作进行一定时间的市场教育，另一方面在做此类产品的时候可能需要改一个更加通俗易懂的名词。\n「数字疗法」的时代可能才刚刚到来，十分希望看到有更多的人投入到这一领域，有足够有效且漂亮的产品问世，非常期待 。\n","id":25,"section":"posts","summary":"上周五的时候，我去旁听了数字疗法峰会北京站的一些分享。数字疗法是今年兴起的一个新的行业，蕴藏着无数的机会和可能，当然也已经有很多的公司已经投","tags":["分享","记录"],"title":"听完数字疗法产业峰会北京站的一些想法","uri":"https://www.xzywisdili.com/2021/10/2021-10-18-thoughtsofdtx/","year":"2021"},{"content":"见识到了有人如何通过玩文字游戏吸引流量了。\n从一则新闻说起 昨天半夜在微博上看到了这么一则新闻：巴黎奥运会将削减一些项目，帆船、竞走、棒球、空手道和拳击都将面临删减和调整，同样也包括4个举重项目。\n但是这个新闻经过一些媒体和大V之口就变成：「巴黎奥运：取消多个中国夺金热门项目，新增霹雳舞」。再经过微博大 V 一转载，事情的逻辑就变成了：中国在东京奥运举重表现出色，于是小心眼的西方国家们嫉妒，于是法国主谋取消了举重项目。这也激起了很多网友对法国进行辱骂攻击。\n但我觉得奇怪，就简单检索了一下：\n首先，这事并不是新闻，而是去年12月就已经提出，时间顺序上就驳斥了中国举重队在东京奥运斩获数枚金牌和削减举重项目的因果关系；其次，取消原因也和中国此次奥运举重队的傲人成绩并无联系，主要是国际举重协会管理之混乱程度令人咋舌：\n内斗严重，曾经出现过 3 天更换 3 个主席的荒唐事，遭到了国际奥委会的多次警告； 兴奋剂审查不力，18 名举重运动员通过他人冒名顶替逃避检测 贿赂裁判和相关官员，举联主席都带头腐败，1040 万美元现金无法解释 通过这几份公开报道，足见国际举联已经烂到根上，引起了国际奥委会的强烈不满。基于这样的背景，国际奥委会早有提议在巴黎奥运会上削减举重项目。\n有人可能要问：那为什么最近这则新闻又被拿出来了？\n这是因为国际奥委会最近投票通过了一项关于《奥林匹克宪章》的修订，简单说就是扩大了自己的权力，当发现某项体育类别的管理机构问题很大时，就可以削减或者取消这项运动以示惩戒。\n当然，这整件事对于我国和我国运动员都是不利的，因为举重确实是我国的传统强项。但我想就算网友们感到不满，也应该搞清楚事件的来龙去脉，或许可以攻击国际举重协会管理混乱，甚至攻击国际奥委会的一刀切制度不合理。但是对法国人和巴黎奥运会进行辱骂这点上，实在是骂错了人。\n警惕媒体和大 V 对于新闻的「加工」 东京奥运会期间爆发了不少全民关注的热点新闻，也有不少新闻闹出了这样那样的「乌龙」或者「反转」，但其实我觉得这主要有几点原因：\n媒体和大 V 二次加工新闻：主要表现在修改大标题、断章取义、强行联系。保持新闻的客观中立已经不再是这些媒体恪守的信条，怎么样博取流量，尤其是利用大众情绪博取流量才是他们最在意的。大众想要看到什么，他们就炮制什么样的新闻，至于被事实打脸或翻车也不会伤筋动骨，因为网友的关注焦点一下子就过去了； 大众已经深陷信息茧房：这点我深有体会。因为检索上述新闻消息根本不需要任何难度，直接用相关关键词在百度或者谷歌检索，就能看到过去的新闻稿。这说明了现在获取资讯的网友越来越懒于去探求事实，“不就是一则新闻嘛，他发出来了，我跟着骂两句，也就刷过去了”。可能大部分人抱着这样的心理，但殊不知自己的潜意识和观念已经被这些媒体通过这样的方式塑造成了另一种样子。 过去我们常说互联网万物互联，透过「窗户」就能看到世界。但其实警惕，有人在你看出去的那扇窗户上捣鬼，只给你展示你想要的。\n","id":26,"section":"posts","summary":"见识到了有人如何通过玩文字游戏吸引流量了。 从一则新闻说起 昨天半夜在微博上看到了这么一则新闻：巴黎奥运会将削减一些项目，帆船、竞走、棒球、空手","tags":["分享","记录"],"title":"看看一些现代“媒体”是怎么愚弄大众的","uri":"https://www.xzywisdili.com/2021/08/2021-08-13-seehowmediafoolus/","year":"2021"},{"content":"昨天因为一些原因，也体验了一把在 2021 年寄一封平信的整套流程。 正好值此毕业之际，给家里的爸爸妈妈写一封感恩家书。 先找了一张信纸然后按照打好的草稿誊抄了一遍（写字练字养成正确的书写姿势实在是太重要了， 不然就会像我一样写三四行，手便会非常酸痛）。\n昨天下午，带着信纸去到了旁边的中国邮政，下午大厅里空空荡荡的，没有一个顾客。 在保安对我例行测体温扫健康码之后，一位工作人员非常热情地招待了我。 得知我要寄平信之后，也直接给我取了一个信封和邮票。\n最后算上称重从北京寄回陕西，总共花费了￥1.3。当然，寄平信的花费是必须使用现金，不能用电子支付的。\n在互联网的发展和对信息发送接受速度的追求下，确实能感觉到寄信这件事已经快要淡出社会的舞台了。 明明在我小时候还司空见惯的东西，到 2021 年，竟然从写信到贴邮票都带有一种复古感，让我还是觉得有些奇妙。 不过，我觉得偶尔摆脱互联网，在线下进行一些实际的任何活动，也确实能体会到生活在这个社会的实感。 这种实感还是挺重要的。\n","id":27,"section":"posts","summary":"昨天因为一些原因，也体验了一把在 2021 年寄一封平信的整套流程。 正好值此毕业之际，给家里的爸爸妈妈写一封感恩家书。 先找了一张信纸然后按照打好的草稿","tags":["分享","记录"],"title":"在 2021 年寄一封平信","uri":"https://www.xzywisdili.com/2021/07/2021-07-05-writesendletter/","year":"2021"},{"content":"我平时算是少数派的忠实读者，但仍觉得 少数派的编辑好物推荐栏目有过度倡导消费主义的嫌疑。 当然，这绝不是对少数派本身立场的任何批评，因为豆瓣消费降级小组和 豆瓣不要买小组也都是从该网站接触到并受益良多。 抵抗消费主义，只购买自己需要的东西是我个人坚信并践行的生活方式。 接下来也简单列出我个人不要购买的一些物件，顺便也使用一下久违的 Markdown 表格。\n物品 理由 含糖饮料 皮肤会变差，浪费钱，游离糖对身体不好 瓜子 嗑起来就什么都做不了，肠胃容易难受 实体书 慎重，非必要不要买，搬家巨麻烦（电子版代替） 纸笔本 用的少了，大学攒了很多，搬家的时候都要丢掉，没必要添置 手机游戏月卡 提供体力和原石的虚拟资产没有意义（迟早会弃游） 小麦欧耶 容量和营养价值对比，溢价太高，口味也一般 免洗手消毒凝胶 好用，但贵，可以用香皂代替（可以用很久） 手机壳 一套足矣，不要喜新厌旧买很多 自热火锅 口感一般，吃一次房间味道很重，油腻 奶茶 / 茶饮 溢价，并不健康，容易腻 线上买课 善用检索，看看是否有公开分享的免费替代版本 至于想要的东西，之前有个清单，但也删的几乎不剩了。 一个纠结的点是耳机。曾经室友推荐我购买的一条森海塞尔 MX375 已经用了 5 年仍然正常服役， 而且真的对我来说完全称心如意的一条耳机。但现在无线耳机大行其道的今天，逼着你必须购置一对无线耳机。 之前购买的 Redmi 的无线佩戴起来有些不适，但先用着吧。可能在遥远的未来再换掉。\n另一个纠结的点就是手头这台 15 mid 的老 MBP。也算服役了5年半，虽说还能用，但是运行速度和功耗发热 都明显处于“老年”状态。也是实在纠结该拿他怎么办。等之后找到一个合适的契机更新吧。\n别的真就没有什么想买的了。\n","id":28,"section":"posts","summary":"我平时算是少数派的忠实读者，但仍觉得 少数派的编辑好物推荐栏目有过度倡导消费主义的嫌疑。 当然，这绝不是对少数派本身立场的任何批评，因为豆瓣消费","tags":["分享","记录"],"title":"我个人不会再买的东西","uri":"https://www.xzywisdili.com/2021/07/2021-07-02-thingsneverbuy/","year":"2021"},{"content":"最近开始关心 AI 技术在医疗领域的一些应用。 随着老龄化社会的进程，节省医疗系统的冗余成本，做到更好的健康管理势必更为重要。 而从现有积累的和未来获得的人群大数据势必也将把医疗探索和相关制药研发带入到一个新的时代。\n但同时这个进程也有太多问题和阻碍：缺乏相关跨界人才的问题，数据隐私和伦理的问题，模型可解释性的问题和 人群对 AI 的信用度和接受度的问题等等……\n不过，我还是相信，新技术的到来能帮助我们在医疗系统中解决更多的问题。\n所以，最近搜罗了这个方向的一些书籍，准备开始爆啃：\n当然，如果有更多的心得和收获也会在博客和大家及时分享。 可能唯一的遗憾是平时没有一起交流讨论的伙伴。\n","id":29,"section":"posts","summary":"最近开始关心 AI 技术在医疗领域的一些应用。 随着老龄化社会的进程，节省医疗系统的冗余成本，做到更好的健康管理势必更为重要。 而从现有积累的和未来获","tags":["分享","记录","读书"],"title":"最近要开始啃书了","uri":"https://www.xzywisdili.com/2021/06/2021-06-28-starttoreadbooks/","year":"2021"},{"content":"如果大家有从 NASA 获取气象数据的需求的话，只要在某度搜索，一定会看到下面的这些链接：\n他们可以批量获取单个城市和经纬度范围内的数据，而且代码也写得很好，但有两点问题：\n他使用的数据库是 NASA 为作物生长模型提供的气象数据，包括辐射、最大最小气温、降雨、风速，而不包括很多研究需要用到的气温和相对湿度的数据； 他将空值用了前后5日的平均值代替，有时候可能不需要这么处理 针对以上两点问题，我只能去寻找 NASA 官网进行数据检索和下载，而 NASA 提供的数据获取工具也十分方便。 只需要打开 NASA 的 Data Access Viewer，就可以看到左边的界面：\n左边的工具选项都十分明确：\n数据来源：就选择“SSE-Renewable Energy”就好； 选择日值、年值或是气候学的平均处理； 选择目标城市的经纬度即可，当然也可以使用标记工具直接在地图上点出； 输入目标的起止日期； 输出文件的格式 下面就是选择目标变量，我选择的是这两个，也就是气温和相对湿度：\n接下来就可以提交和下载了。\n至于数据的说明，可以点击这里，里面说明了数据的来源、分辨率和可获取的日期跨度。当然，如果要进行批量获取的话，可能需要高人来写相应的代码了。\n","id":30,"section":"posts","summary":"如果大家有从 NASA 获取气象数据的需求的话，只要在某度搜索，一定会看到下面的这些链接： 他们可以批量获取单个城市和经纬度范围内的数据，而且代码也写得","tags":["分享","记录"],"title":"如何从 NASA 下载气象数据","uri":"https://www.xzywisdili.com/2021/06/2021-06-16-downloadclimate/","year":"2021"},{"content":"电子病历的普及度已经很高。但如何从电子病历中提取信息还是比较令人头疼。 首先病历内容众多，从主诉到体格检查，从家族史到用药史，一份完整的病历涵盖的信息众多。 其次，病历内容是由医生按照段落填写的，很难在数以万计的病历中找到通用的标准进行提取。\n当然，如果医院的病历系统是按照表格进行组织并且保存，比如我之前实习过的社区医院，将每个人的电子病历 保存为 HTML 格式，就可以使用 python 的 BeautifulSoup 进行提取。当然，如果只能得到大段 文字的话，大概需要借助人工智能里面的自然语言处理来做更多的工作的。\n当然，如果任务比较简单的话，我们还是有取巧的方法的。比如要从病历记录里面提取出来身高、体重、收缩压和舒张压 等等，如：“血压114/56mmHg，体重53.3kg，胎心146次/分，宫高25cm，宫缩无。” 这样的描述中提取出舒张压：56，收缩压：114，体重：53.3 等等数据，我们可以借助正则表达式解决：\nlibrary(readxl) library(tidyr) library(stringr) data \u0026lt;- read_excel(\u0026quot;data.xlsx\u0026quot;, sheet = 1) extract_blood_pressure \u0026lt;- function(diag_str) { res \u0026lt;- str_match(diag_str, \u0026quot;血压[:： ]*[0-9]*/[0-9]*[ ]*[(mmHg)|(mmhg)]+\u0026quot;) blood_pressure \u0026lt;- unlist(str_extract_all(res[, 1], \u0026quot;[0-9]+\u0026quot;)) return(as.numeric(blood_pressure)) } extract_height \u0026lt;- function(diag_str) { res \u0026lt;- str_match(diag_str, \u0026quot;身高[:：]*[0-9]*/[0-9]*[ ]*[cC][mM]\u0026quot;) height \u0026lt;- unlist(str_extract_all(res[, 1], \u0026quot;[0-9]+\u0026quot;)) return(as.numeric(blood_pressure)) } extract_weight \u0026lt;- function(diag_str) { res \u0026lt;- str_match(diag_str, \u0026quot;体重[ ]*([0-9]{1,}[.]*[0-9]*)[ ]*[kK][gG]\u0026quot;) weight \u0026lt;- unlist(str_extract_all(res[, 1], \u0026quot;[0-9]{1,}[.]*[0-9]*\u0026quot;)) return(as.numeric(weight)) } 其实就是医生在写这些数据的时候，绝大部分情况下都有这样的固定模式，可以让我们通过正则表达式 提取。当然，我们甚至可以针对更多数据，包括心率、体温等等，给出对应的正则表达式。我写的是针对我得到 的数据给出的解决方案，可能不具有普适性，甚至在语法上也不一定是完美方案，但思路和代码仅供大家参考。\n另外，关于这个主题有几个比较实用的网站推荐：\n正则表达式匹配验证 R 语言正则表达式语法汇总 正则表达式规则入门 ","id":31,"section":"posts","summary":"电子病历的普及度已经很高。但如何从电子病历中提取信息还是比较令人头疼。 首先病历内容众多，从主诉到体格检查，从家族史到用药史，一份完整的病历涵","tags":["分享","R 语言"],"title":"从电子病历提取数据","uri":"https://www.xzywisdili.com/2021/06/2021-06-15-extractnumbers/","year":"2021"},{"content":"R 语言中的 data.table 包可以理解为 data.frame 的高级版本，它比较适合适合用来处理大型数据集。\n载入包并且读取一些数据，当然也可以使用 nrow 参数来决定读取多少行：\nlibrary(data.table) mydata \u0026lt;- fread(\u0026quot;some_kind_of_data.csv\u0026quot;) mydata \u0026lt;- fread(\u0026quot;some_kind_of_data.csv\u0026quot;, nrows = 10) data.table 包提供了一个非常简洁的通用格式：mydata[i, j, by]，可以理解为：对于数据集mydata，选取子集行i,通过by分组计算j。只需要记住，i是用来在行上进行操作的（比如筛选行），j 是用来在列上进行操作的（比如选择列或者根据计算创建新列）。对比dplyr等包来说，data.table的运行速度更快。\n选择列 如果只需要选择列的话，可以只代入 j，但需要记住给 i 留出位置：\nmydata[, j] 首先，通过列名选择列是，是否需要打双引号是一个值得考虑的问题，因为两者各有优劣。 data.table 支持打双引号的原生 R 的做法：\ndata1 \u0026lt;- mydata[, c(\u0026quot;columnA\u0026quot;, \u0026quot;columnB\u0026quot;, \u0026quot;columnC\u0026quot;, \u0026quot;columnD\u0026quot;)] 但也可以使用不打双引号，这样会更方便（就好像 tidyverse 里面的 select），只不过需要用到 list：\ndata1 \u0026lt;- mydata[, list(columnA, columnB, columnC, columnD)] 这里，第一个小技巧就来了，我们可以使用一个点代替 list：\ndata1 \u0026lt;- mydata[, .(columnA, columnB, columnC, column)] 在 data.table 的中括号里，.() 就是 list()的简写形式。\n如果你想使用一个已经存在的列名向量，比如：\nmycols \u0026lt;- c(\u0026quot;columnA\u0026quot;, \u0026quot;columnB\u0026quot;, \u0026quot;columnC\u0026quot;, \u0026quot;columnD\u0026quot;) 直接套用是行不通的：mydata[, mycols]，而是需要在这个列名向量前面加两个点：\ndata1 \u0026lt;- mydata[, ..mycols] 当然，你可以借用命令行里面 .. 的含义去理解，可以认为是从括号里面的命名空间上升到全局变量了。\n计算 data.table 行数 下一个标记是 .N，代表行数。可以用这个标记来得到数据集的总行数和使用 by 分组后的各亚组行数。 比如我们返回 iris 数据集的总行数和按照 Species 分组的行数：\ndiris \u0026lt;- as.data.table(iris) diris[, .N] # 150 diris[, .N, Species] # Species N # 1: setosa 50 # 2: versicolor 50 # 3: virginica 50 当然，使用多个变量进行分组也是可以的，同样用到 .：\nmydata[, .N, .(columnA, columnB)] 并且你还可以在后面使用 order，让行数结果从高到低排列：\nmydata[, .N, .(columnA, columnB)][order(columnA, -N)] 这样的语句顺序也非常符合数据处理的逻辑。 当然，如果你使用习惯了 dplyr，你喜欢的写法 data.table 也可以模仿：\nmydata %\u0026gt;% count(columnA, columnB) %\u0026gt;% order(columnA, -n) mydata[, .N, .(columnA, columnB)][ order(columnA, -n) ] 向 data.table 增加列 比如我们想要在 iris 数据集增加一列，看品种是否是 Virginica，如果是则为 TRUE，否则为 FALSE， 那么可以这样写：\ndiris[, IfVirginica := ifelse(Species %like% \u0026quot;virgin\u0026quot;, TRUE, FALSE)] 这个 %like% 的用法有点类似于 SQL 里面的 LIKE。 而 := 的用法可能在其他编程语言中比较常见，这里代表通过增加新列改变了目前的数据集 diris， 而并非存入另外的新数据集。\n当然也可以同时增加两列：\ndiris[, `:=`( IfVirginica := ifelse(Species %like% \u0026quot;virgin\u0026quot;, TRUE, FALSE), IfVensicolor := ifelse(Species %like% \u0026quot;vensi\u0026quot;, TRUE, FALSE) )] 更多的操作符 %between%：找到目标的取值区间进行筛选\nmydata \u0026lt;- diris[Species == \u0026quot;virginica\u0026quot; \u0026amp; Sepal.Length %between% c(4, 6)] %chin%：类似原生 R 的 %in%，但只面向字符变量且速度更快\nmydata \u0026lt;- diris[Species %chin% c(\u0026quot;virginica\u0026quot;, \u0026quot;vensicolor\u0026quot;)] fcase() 函数 最后要介绍的 fcase() 函数和 SQL 里面的 CASE WHEN 语句和 dplyr 里面的 case_when() 语句 类似，基本语法是 fcase(condition1, \u0026quot;value1\u0026quot;, condition2, \u0026quot;value2\u0026quot;)，比如下面的例子：\ndiris[, newSpeices := fcase( IfVirginic \u0026amp; !IfVensicolor, \u0026quot;Species1\u0026quot;, !IfVirginic \u0026amp; IfVensicolor, \u0026quot;Species2\u0026quot;, IfVirginic \u0026amp; IfVensicolor, \u0026quot;Both\u0026quot;, !IfVirginic \u0026amp; !IfVensicolor, \u0026quot;Neither\u0026quot;)] 最后，更多特殊符号的用法可以参考 help(\u0026quot;special-symbols\u0026quot;)，虽然文档可能讲得不是很清楚，但依然全面。\n","id":32,"section":"posts","summary":"R 语言中的 data.table 包可以理解为 data.frame 的高级版本，它比较适合适合用来处理大型数据集。 载入包并且读取一些数据，当然也可以使用 nrow 参数来决定读取多少行： library(data.table) mydata \u0026lt;-","tags":["分享","R 语言"],"title":"新发现的 R 里关于 data.table 的一些神奇用法","uri":"https://www.xzywisdili.com/2021/06/2021-06-15-newrtips/","year":"2021"},{"content":"今天整理数据的时候，需要使用数据集 2 对数据集 1 里缺失的部分进行填补，而两者重复的部分，优先保留数据集 1 里的。\n这个问题其实很简单，只需短短 2-3 行：\nresult \u0026lt;- data1 %\u0026gt;% left_join(data2) %\u0026gt;% mutate(value = ifelse(is.na(value.x), value.y, value.x)) 但我遇到的问题稍微多一点点难度，日期的序列不是连续的话，例如数据集 1：\n# date value # 2014-01-01 12 # 2014-01-03 14 # 2014-01-05 17 数据集 2：\n# date value # 2014-01-02 13 # 2014-01-03 16 # 2014-01-04 15 那么希望得到的结果是：\n# date value # 2014-01-01 12 # 2014-01-02 16 # 2014-01-03 14 # 2014-01-04 15 # 2014-01-05 17 我们会需要先生成一个日期的模板：\ntemplate \u0026lt;- tibble( date = seq(as.Date(\u0026quot;2013/1/1\u0026quot;), as.Date(\u0026quot;2017/12/31\u0026quot;), \u0026quot;days\u0026quot; ) template %\u0026gt;% left_join(data1, by=\u0026quot;date\u0026quot;) %\u0026gt;% left_join(data2, by-\u0026quot;date\u0026quot;) %\u0026gt;% mutate(value = ifelse(is.na(value.x), value.y, value.x)) 当然，如果数据维度上升，比如多了一个城市维度的话，只需要在模板上更改一下也能解决了：\ntemplate \u0026lt;- tibble( city = rep(city_codes, each=len1), date = rep(seq(as.Date(\u0026quot;2013/1/1\u0026quot;), as.Date(\u0026quot;2017/12/31\u0026quot;), \u0026quot;days\u0026quot;), len2) ) template %\u0026gt;% left_join(data1, by=c(\u0026quot;date\u0026quot;, \u0026quot;city\u0026quot;)) %\u0026gt;% left_join(data2, by-c(\u0026quot;date\u0026quot;, \u0026quot;city\u0026quot;)) %\u0026gt;% mutate(value = ifelse(is.na(value.x), value.y, value.x)) 这里需要注意生成模板的两个变量重复的次数和参数设置，另外下面的合并键也要加入新的变量。\n","id":33,"section":"posts","summary":"今天整理数据的时候，需要使用数据集 2 对数据集 1 里缺失的部分进行填补，而两者重复的部分，优先保留数据集 1 里的。 这个问题其实很简单，只需短短 2-3 行","tags":["分享","R 语言"],"title":"对数据进行填补","uri":"https://www.xzywisdili.com/2021/06/2021-06-09-mergeandkeepdata/","year":"2021"},{"content":"终于在今年经历了毕业季，虽然坎坎坷坷，但也还算顺利，即将要从北京大学医学部毕业了。 然而在这之间走的各种流程和细节着实让人无比心累，实在是不吐不快。 明明有很大的提升空间，却还是抱残守缺，实在是与北大医学部的名校形象不太相符。\n老旧的线上管理系统 难以想象，北医在 2021 年还在使用只有 IE (对，就是那个在 95 年推出，在 15 年被微软宣布要放弃的 Internet Explorer 浏览器) 才能顺利登陆并且完成各项内容提交的研究生教育管理系统。只要使用现代浏览器登陆，那必然是只能显示一片白色：\n这带来的后果便是，从抢课、查成绩，到毕业申请、提交材料，任何使用苹果 mac 系统和最新版 windows 的用户都 无法顺利处理研究生期间的教务相关事务。虽然北医的生活服务平台已经更新成了现代化的新版应用，并且支持所有平台（包括手机 APP），但研究生教育管理系统迟迟不更新的原因实在不得而知。\n另外，一级二级菜单混乱，反应延时，信息显示不清晰直观等问题就不在这里一一赘述。\n行政人员的办公方式和态度槽点繁多 这一点应该是绝大部分毕业生的共识。毕业所需程序复杂，材料繁多，行政人员准备了一份 100 多页的 PPT 来解释说明全程。 整个文件字号大小无区分，找不到一级二级标题，各种颜色色块堆叠，实在让人无法找到重点。就从这一页上来看，目之所及 5 种颜色。想要完整清晰地领会这页的意思，应该是要花费不少时间：\n如此复杂繁琐，难以理解，想必很多学生都有问题要去询问相关的行政人员。但相关人员态度敷衍，摆脸色， 有时候明明可以直接告诉你问题的答案，就是不说，让你自行去查 PPT。而到最终收材料的那天，每个人需要上交 10 多份纸质材料。没想到两个行政人员对每个同学每一份文件逐一检查，队伍直接排到了走廊尽头。整个排队流程至少要让你等待 1 个半小时！我在想，如果找一间会议室，按照要交的文件分流成列，找几个本科学生检查模板化公式化的材料，行政人员去检查更复杂的提交材料。整个过程使用共享文档登记提交进度，通过一份就在该项下面打勾，效率至少比现在这样要翻几倍。\n无纸化进程还很远 无纸化办公的概念一直在普及并被实现，在现今，很多手续的办理，甚至合同的签署都可以完全在线化，电子化。 然而，整个毕业流程距离无纸化办公还太远。整个流程光各种申请表格、提交材料，就大大小小至少 20 多份。 而从论文写完，送审，到答辩，再到最终提交，光纸质版的大论文都要至少打印 13 本之多。 其实，仔细想想，里面明明有很多不需要签字或者盖章的文件完全可以移交成在线电子表格的形式进行提交。 就连导师签字的内容也可以通过在线的教务系统进行审批和签字（当然，这么老旧的教育系统当然是不具备这个功能的）。\n当然，这些只是我在这所学校经历最后阶段的一点感想。可能这些问题如此难以解决和实现有其背后的原因，但我还是真心 希望这所学校可以变得更好一点。\n","id":34,"section":"posts","summary":"终于在今年经历了毕业季，虽然坎坎坷坷，但也还算顺利，即将要从北京大学医学部毕业了。 然而在这之间走的各种流程和细节着实让人无比心累，实在是不吐","tags":["学习","分享"],"title":"毕业流程之不吐不快","uri":"https://www.xzywisdili.com/2021/06/2021-06-09-graduation/","year":"2021"},{"content":"这篇博客计划持续更新。这里的网站或者是实用的工具，或者是收藏的实用知识，而且每一个都是我自己用过或者看过的。 其中不少还是我高频使用的，甚至域名也已经背下来了。\n工具类网站 经纬度批量查询 世界范围内免费地图下载 查看时差的网站 提供代理 ip 池 各种知名品牌配色参考 电脑装机官方网站合集 快递时效查询 深度学习助力翻译网站 DeepL 在线编辑修改 GIF 图 免费在线 OCR 工具 给定地图区域的面积和最大容纳人数 我最常用的 PDF 转图片网站 上传 word 生成手写字体文档，方便打印的【萝卜工坊】 下载无水印抖音视频 一站式解决 gif 相关问题（什么都有） 免费美区 Apple ID B站视频解析下载 看不懂的网络缩略词速查工具 学习类网站 杀手级词云 英文句式写法查询 编程通用算法可视化 和小浩学算法大全 统计类图表汇总 python3网络爬虫汇总 经济类数据图表（值得学习） 积累 Tips 科研·学术 学生在家远程学术 生活·厨房 夏日便当拯救计划 通过这些小改变，帮家中的年夜饭增添一份健康 屯粮做饭 关于洗涤、熨烫和收纳衣服的常识 工作日简单A+B煮饭 TDEE 热量计算器，得到每日摄入热量 打印机选择指南 职场·工作 写给职场新人的 PPT 演示制作指南 PPT 制作全流程要点 办公技巧分享 2020 年 7 月中华人民共和国县以上行政区划代码 其他 bilibili up主推荐 ","id":35,"section":"posts","summary":"这篇博客计划持续更新。这里的网站或者是实用的工具，或者是收藏的实用知识，而且每一个都是我自己用过或者看过的。 其中不少还是我高频使用的，甚至域","tags":["工作","分享"],"title":"推荐且收藏的好网站","uri":"https://www.xzywisdili.com/2020/12/2020-12-28-recommendwebsite/","year":"2020"},{"content":"\n写在前面 A little words before 我的英语水平相当有限，但我想把赵粤接受《时尚COSMO》的采访内容分享给可能看不懂中文的海外粉丝朋友。 里面有一些专用的中文词汇，如果你不明白的话，可以点击我附上的链接查看这些词语的具体含义。 如果您发现了我翻译中的问题，或者您能提供更高质量的翻译，我非常感谢。\nMy English is not very good, but I want to share Zhao Yue\u0026rsquo;s interview on COSMO emagazine with overseas fans who may not be able to read Chinese. There are some special Chinese words in it. If you don\u0026rsquo;t understand, you can click the link I attached to see the specific explanation. If you find any mistakes in my translation, or if you can provide a higher quality translation, I would be very grateful.\n赵粤的采访翻译 Zhao Yue\u0026rsquo;s interview translation 她是夏夜最后的凉风 She is the last cool breeze of summer night\n赵粤说话有一种流畅的秩序，很少磕巴也很少重复，想见她的头脑是分外灵光的。在塞纳河度过的七年，以隐性或显性的方式存在于她身上，比如将陌生气氛翻译成热络自然的能力。她就是那种第一次见面的人就可以喊她全名的人啊。夜晚闹哄哄的大堂里，最后一项工作也即将完成。编辑喊：“赵粤，我们采访才做一半呐。”赵粤回，“我还记得刚才聊到哪。”就是一个沁人心脾的人。\nZhao Yue speaks in a fluent order, seldom stuttering or repeating. She has a brilliant mind. The seven years spent in SNH48 group have existed on her in a recessive or explicit way, such as the ability to turn unfamiliar atmosphere into warm and natural. She is one of those people who can call her full name when they meet for the first time. In the noisy lobby at night, the last work is about to be finished. The editor yelled, \u0026ldquo;Zhao Yue, we\u0026rsquo;re only halfway through the interview.\u0026rdquo; Zhao Yue replied, \u0026ldquo;I still remember where I talked just now.\u0026rdquo; She is such an elegant person, talking to her seems to be able to smell the fragrance of flowers.\n赵粤身上随遇而安的侠性、天真不泯的热血、自洽得一团和气，都是成长过程中积攒的人格。你会觉得这个姑娘，还蛮君子的。\nZhao Yue\u0026rsquo;s happy-go-lucky chivalrous nature, blood surging and harmonious personality are all accumulated during her growth. You\u0026rsquo;re gonna think this girl\u0026rsquo;s a junzi.\n武侠的启蒙要从我小时候说起，我姥爷把我带大的，他是个老顽童，特别喜欢玩游戏。《仙剑奇侠传》的坑是我姥爷待我入的，他打98版仙剑，我经常在旁边看。仙剑的世界还都蛮梦幻的，我小时候拿姥爷的太极剑，就会有仗剑走天涯的梦想在。喜欢动漫是在高中，喜欢看《火影忍者》。在舞蹈学校里每天练习，看动漫的话就会鼓舞到我，连上课都会非常有动力。\nThe first time I knew \u0026ldquo;Wuxia\u0026rdquo; was in my childhood. My grandfather brought me up. He was an old urchin, especially fond of playing games. My grandfather took me to play the 98 edition of The Legend of Sword and Fairy. When he played that video game, I used to watch it. The world of sword and fairy is very dreamy. When I was a child, I took my grandfather\u0026rsquo;s Taiji sword, and I had the dream of running around the world with the sword. I began to like watching anime in high school, especially Naruto. At that time, I practiced dancing in school every day, watching anime can motivate me in class.\n武侠距生活较远，喜欢武侠的人可能内心保存着一份天真，但我也是有功利心的。我的功利心可能体现在平时练习上面，你想要到达某一个高度的话，你肯定要不断地练习，肯定要有一个动力在，也可以把它算作功利心。但是如果这件事我努力了的话，我平时对待它的心态就是无所求。放平心态，只要自己努力了就不后悔。\n\u0026ldquo;Wuxia\u0026rdquo; is far away from real life. People who like \u0026ldquo;Wuxia\u0026rdquo; may have a naive heart, but I am also utilitarian. My utilitarian may be reflected in my daily practice. If you want to accomplish a goal, you must practice constantly. This preseverance can also be regarded as utilitarian. But if I work on it, I usually treat it with a mindset of not wanting anything. Keep a calm mind, as long as you work hard, you will not regret.\n功利和天真怎么选？\nWould you rather be utilitarian or naive?\n你看你怎样比较开心。我可能跟自己的好朋友在一起会比较开心，所以在《创造营2020》里，我比较多想的是如何让我们七个人走得更远一点。当时的想法很单纯，就想跟大家在一起。后来才察觉没有把握住比赛的节奏，赛制一直催促着我往前，队友也催促着我往前，命运让我往前。\nIt depends on which one you choose to be happier. I will be more happy when I\u0026rsquo;m with my good friends, so in Chuang 2020, I think more about how to make the seven of us (seven idols from 48 group) go further. At that time, the idea was very simple: I just wanted to be with them. Later I realized that I didn\u0026rsquo;t grasp the rhythm of that competition. Both the rules of competition and my teammates had been urging me to move forward. And fate made me move forward, too.\n期间我经常听到的一种说法是，“河妹自带粉丝。”我认为是前面几年所做的努力，现在得到的回报吧。\nDuring that period, I often heard a saying that \u0026ldquo;Idols from SNH48 groups bring their own fans.\u0026rdquo; I think it\u0026rsquo;s the effort made in the past few years. Now it\u0026rsquo;s time to get the reward.\n大家看到有人跟随我进入到《创造营2020》，但是没有看到二期生剧场台下观看的人数甚至没有台上人多的时候。剧场状况不佳，我们淋着雨去发传单，为了观众过来看公演的时候。我们不断地往外出去，为了让更多河外的人结识的时候。大家看到现在的我有一部分粉丝，也是之前的我们努力来的吧。\nMany people saw that fans followed me into Chuang 2020, but they didn\u0026rsquo;t know that the number of audience in the theater was even smaller than the number of second-generation members of SNH48 groups on the stage before. At that time, our theatre was in a bad condition, so we went out in the rain to hand out leaflets to passers-by, hope someone will come to watch our show. We kept going out, in order to let more people who don\u0026rsquo;t know SNH48 groups get to know us. As you can see, I have some fans now, which is also related to my previous efforts.\n我有时候也会焦急，想获得更多人的认可。但我也会想，没关系你现在就这么认为吧，过段时间我肯定会让你认可的。不论是“出村”还是“出圈”，我内心认为的顺序始终是，并不是出圈之后就会有更多人认识你，而是你获得了更多人的认可才能出圈。\nSometimes I am anxious to get more recognition. But I also think, it\u0026rsquo;s OK, you think so now, I\u0026rsquo;m sure I\u0026rsquo;ll let you recognize it later. On the way of \u0026ldquo;becoming widely known\u0026rdquo;, I think the right order is always that after you get out of the circle, more people will know you, but you can get more people\u0026rsquo;s recognition before you leave the circle. 这里好难！！！\n你向来都是风轻云淡的人吗？\nHave you always been a light-hearted person?\n我小时候是个遇事崩的人。十八岁的时候出道，在我们的队年龄是下位圈的，姐姐们都非常保护我。那个时候的我很脆弱，受到人和事的影响就会马上不淡定，开始哭或情绪崩溃。父母一直跟我讲，无论发生什么事情一定要静下心来想该怎么去解决，崩溃对于解决事情没有一点好处，反而会让结果变得更糟糕。经过了很长一段时间的磨练，我现在做什么事情都非常淡定。我想，只要人活着，我就肯定能解决它。\nWhen I was a kid, I was a person who broke down easily in situations. I entered SNH48 group at the age of eighteen, and was one of the youngest in our team. My teammates were very protective of me. At that time, I was very vulnerable, would immediately start crying or having emotional breakdowns when I was affected by people and things. My parents have always told me that no matter what happens, we must calm down and try to solve it. Collapse is not good for solving the problem, it will only make the result worse. After a long period of training, I am very calm in what I do now. I think, as long as people live, I can definitely solve it.\n我爸有一个形容，你爬上了一个很陡峭的山，你得要下来，你怎么下来？你是绝对有办法可以下来的，你不可能一直呆在上面下不来。人在求生的状态下一定可以想出办法，但是你人要淡定。\nMy dad had a metaphor, if you are on the top of a very steep mountain, and have to get down, how? You definitely have a way to get down. You can\u0026rsquo;t stay up there all the time. In the state of survival, you can figure out a way, but you have to calm down.\n但也会有失误……\nBut sometimes I make mistakes……\n《超新星全运会》射箭比赛上脱靶的那一箭，其实是闭错了眼睛。应该是闭上左眼，我闭成了右眼（笑）。我的心率一直很稳，包括对手前一箭射出十环，我顶住了压力回馈了十环。但脱靶以后我觉得我好傻，想消失在赛场上面，不敢看队友的方向，结束后向他们说了抱歉，自己一个人跑到后台去哭。\nIn the archery final of \u0026ldquo;Super Nova Games: Season 3\u0026rdquo;, I missed one arrow because I closed wrong side of my eyes. I supposed to close my left eye, but I closed my right eye that time (laugh). My heart rate was very steady. After the opponent scored 10 with one arrow, I also resisted the pressure and responded with an arrow of 10 scores. But after I missed that arrow, I thought I was so stupid that I want to disappear on the field and I didn\u0026rsquo;t dare to look at my teammates. After the final end, I said sorry to my teammates and ran to the backstage alone to cry.\n但我也没有放弃，射箭这件事我要死磕到底。你在哪个地方摔得有多惨，你爬起来的样子就有多精彩。\nBut I\u0026rsquo;ll never give up on archery. How badly you fall somewhere, and how wonderfully you get up in the same place.\n我小时候经常为一件事情感到难过，我又不是这样的人，你们怎么这么想我？如今除非是很亲近的人误解我，被离你很远的人误解就看淡。我想如果我不了解一个人，人家在我耳旁风言风语，我可能也会想那个人会不会怎么样。大多数人会选择一种去听别人口中的TA的判断方式，也没有一个对错吧，君子和而不同。\nOne thing I used to feel bad about as a child was the misunderstanding I would receive from others. Nowadays, unless it\u0026rsquo;s someone very close to me, I don\u0026rsquo;t really care about being misunderstood by people who live far away from me. I think if I don\u0026rsquo;t know someone well and people are whispering in my ear, I may also wonder what that person will be like. Most people will choose a way to judge other people. There is no right or wrong on this thing. A junzi gets along with others, but does not necessarily agree with them.\n我最感谢的是帮助过我的人。这七年里我遇到过很多次瓶颈，但很幸运每次都能遇到引导我的人，给我不断创新的机会。让我觉得 ，噢，天无绝人之路。\nI\u0026rsquo;m very grateful to all those who have helped me. In the past seven years, I have encountered many bottlenecks, but I am lucky to meet people who guide me and give me the opportunity to innovate constantly. It makes me feel, oh, there\u0026rsquo;s always a way out.\n在过去与现在面前，赵粤却是个连贯性很强的人呢。很难得吧，成长并不是一帆风顺的，有人对过去的自己挥刀，削铁如泥。赵粤却能将行装背好，顺流而下。她很侠义，亦很洒脱。\nIn front of the past and the present, Zhao Yue is a person with a strong sense of continuity. It\u0026rsquo;s not always easy to grow up. Some people are wielding knives at their past selves, cutting iron like mud. Zhao Yue, on the other hand, is able to carry her outfit on her back and go further. She is very chivalrous, free and easy.\n好像一叶小船，撑着一河的清辉，也渡着彼岸的人。\nShe\u0026rsquo;s like a small boat, holding the light of a river and ferrying people on the other side.\n","id":36,"section":"posts","summary":"写在前面 A little words before 我的英语水平相当有限，但我想把赵粤接受《时尚COSMO》的采访内容分享给可能看不懂中文的海外粉丝朋友。 里面有一些专用的中文词","tags":["追星","分享"],"title":"赵粤的《时尚COSMO》杂志翻译","uri":"https://www.xzywisdili.com/2020/09/2020-09-30-zhaoyuecosmo/","year":"2020"},{"content":"最近在研究计算真实空间两个点的距离， 比如已知 A, B 两点的经纬度坐标，计算两点之间的距离。 在 R 语言中，已经有很多大神提供的空间地理数据包，经过搜索发现最备受推崇的是 geosphere 包。\n使用方法也非常简单：\nlibrary(geosphere) df \u0026lt;- data.frame(lon = c(lon1, lon2), lat=c(lat1, lat2)) distance \u0026lt;- distGeo(df[1, ], df[2, ]) 得到的距离结果单位是 m。\n这个包的最大优势是计算速度很快，比如这里有 geosphere 包的 distGeo 方法和 sp 包的 spDistsN1 方法的 Benchmark 对比。\n附赠内容 批量转换地址为经纬度工具 百度地图经纬度拾取系统 R 语言实现 46 种距离算法 ","id":37,"section":"posts","summary":"最近在研究计算真实空间两个点的距离， 比如已知 A, B 两点的经纬度坐标，计算两点之间的距离。 在 R 语言中，已经有很多大神提供的空间地理数据包，经过搜","tags":["R 语言"],"title":"R 语言计算空间距离","uri":"https://www.xzywisdili.com/2020/07/2020-07-16-rdistance/","year":"2020"},{"content":"最近热播，也引起热议的电视剧无疑是秦昊主演的隐秘的角落。而该剧的原著小说——紫金陈的《坏小孩》也和剧一起霸占了近期的豆瓣书影音热度榜前二。我作为一个紫金陈的读者，曾经对其改编剧作《无证之罪》抱以极高的评价。于是也第一时间观看了该剧，又在今天读完了他的原著小说，心情也从原先的惊喜和期待变成了失望。\n不知道谁扣给紫金陈老师“推理之王”的帽子，在我看来，推理从来都不是他作品的重心。 《谋杀官员》《长夜无明》和《无证之罪》都是开头从一个非常离奇的命案开始抓住观众的好奇心，慢慢抽丝剥茧出这则命案背后的大背景和相关人物。他笔下的世界往往更偏向于丛林世界，不乏心狠手辣的恶徒，也有心怀信仰的正义之士。 在相关人物经历了一番互相争斗，周旋和算计之后，故事达到高潮，最后通过强有力的收尾让读者们纷纷意犹未尽。 他的作品风格更像美剧《冰血暴》那样的黑色犯罪片。人物塑造和多线剧情的构建也是紫金陈老师的强项。\n在《坏小孩》中，紫金陈老师作出了自己勇敢的创新，塑造了一个弑父，弑友的冷酷无情，而又狡猾的少年杀手形象。 他富有情绪又善于伪装，懂得利用，狠得下心，可以想见，这样的形象在过往的中国文学中是比较具有冒犯性，也是比较匮乏的。我们可能更多会在日本的犯罪小说中看到这样的人物形象。 但是这样的创新也带来了一些弊端，紫金陈老师把大量的笔墨用在三个小孩与张东升的日常和所谓的“周旋”上。 书中人物从一开始就和盘拖出，有些行动更是显现出了狠劲，却丧失了合理性（朱晶晶和王瑶等等段落）。 最后的日记策略和严良对戏虽说挽回了一些，但是依然能看出这本书并没有发挥出紫金陈老师完全的功力。\n而到了改编剧上，只能说情况变得更糟了。明显导演和编剧不能照搬小说 9 人死亡的剧情，只好在原先的框架下修修补补。 这样一来，剧作的重心偏向了对朱朝阳和严良（剧中和书中不是一个人，书中刑警为严良，福利院溜出男生为丁浩）的家庭背景展开，尤其是朱朝阳在离异父母之间的关系展开占到了大量篇幅。 最关键的是，我们的两个小少年变成了其心也善的正面形象，所有的罪恶都留给了张东升一个人。 尤其是朱朝阳抱着濒死的父亲掩面痛哭的时候，相信读过原著的小伙伴都只能无奈地摇摇头。 而我认为，这样改编的结果只能是丢掉了原著里最精华，最亮点的部分，实在让人有些失望。\n但是依然也要夸奖剧作的摄影风格，给我们呈现了一座透着海风味道的沿海小城。 里面的一些镜头也颇有想法，比如第六集结尾小船透过云层，再拉回到海上浮尸的镜头让我印象非常深刻。 三位小演员的演技也是出奇地好，都表演出了各自角色的特点，在秦昊面前也丝毫不落下风。 另外，12 集的设定也应该给个大大的好评，真心不希望国产剧动辄拍出个几十集。\n接下来，紫金陈老师的《长夜难明》也会出剧版，将在今年播出。兴奋地期待的同事，也希望不要改得太多，因为那本原著涉及的剧情更加阴暗，且富有政治性。希望国产剧发展得越来越好，产出更多精品。也希望国产犯罪推理类型的小说也能打出一片天。\n","id":38,"section":"posts","summary":"最近热播，也引起热议的电视剧无疑是秦昊主演的隐秘的角落。而该剧的原著小说——紫金陈的《坏小孩》也和剧一起霸占了近期的豆瓣书影音热度榜前二。我","tags":["记录","分享"],"title":"聊聊《隐秘的角落》","uri":"https://www.xzywisdili.com/2020/06/2020-06-20-thebadkids/","year":"2020"},{"content":"最近在研究分段样条回归和线性混合效应模型，看到了几篇好文可以分享。\n样条回归（附 python 代码） [分段回归的拐点连续性](https://yihui.org/cn/2012/04/break-points-in-regression/ 最全使用 R 语言中lme4 包完成线性混合效应模型分析 但还是没搞懂分段线性混合效应模型，要学会接受自己的平庸和愚蠢。\n","id":39,"section":"posts","summary":"最近在研究分段样条回归和线性混合效应模型，看到了几篇好文可以分享。 样条回归（附 python 代码） [分段回归的拐点连续性](https://yihui.","tags":["记录","分享"],"title":"分享几篇统计相关好文","uri":"https://www.xzywisdili.com/2020/04/2020-04-28-papershare/","year":"2020"},{"content":"这一次新冠肺炎，大众们也被迫接受了各种流行病学的知识教育。 但生活中不仅仅是新冠病毒，还有很多种其他病毒、细菌以及他们所引起的流行病。 对于大众来说，也有迫切的需求想要很好地预防流行病，或者在面对流行病时能够做到合适的处置。\n我曾在疾控中心有过一段实习经历。我认为有一个最省心最简单的办法：关注你当地疾控中心的公众号！\n原因有如下几点：\n健康教育也是现在疾控中心的工作内容之一，基本各地的疾控中心都会开设公众号，向大众科普一些健康教育知识。而这些预防流行病的知识都权威可靠； 流行病可能具有地域性和季节性。比如登革热和疟疾就多发生在南方的夏季，诺如病毒和流感多发于冬春季节。甚至还有一些区域有特殊的地方病。我想，不能苛求每一个公众掌握或者记得那么多种流行病的特征，易感人群和预防方式； 这些内容往往经过编排，有图片和加粗颜色字体进行要点强调，更为清晰易懂； 不会标题党，也不会有广告，你关注之后它偶尔出现在你的视线里也不会让你厌烦。 举个例子，比如我当年实习过的地区所开设的公众号「健康龙岗」（这里并非打广告嘻嘻）： 在 2019 年 12 月 16 日便更新流感高发季节的注意事项，对于易感人群，症状，个人预防与防护和接种疫苗的相关情况都介绍得相当清楚。\n在 2019 年 8 月 21 日，也对深圳市出现的可能高危地带进行了公布：\n传染病的流行是一种健康安全的风险，而疾控中心就是最直接也最靠谱的风控部门。 至少疾控的公众号能让你及时地了解当前的地点和时间下，你最应该做出怎样的预防措施。 衷心祝愿大家远离病魔，少生病，身体健康。\n","id":40,"section":"posts","summary":"这一次新冠肺炎，大众们也被迫接受了各种流行病学的知识教育。 但生活中不仅仅是新冠病毒，还有很多种其他病毒、细菌以及他们所引起的流行病。 对于大众","tags":["记录"],"title":"一个最推荐的预防流行病的方法","uri":"https://www.xzywisdili.com/2020/02/2020-02-29-commonwaytoprevent/","year":"2020"},{"content":"一个简单地汇总。\n这次疫情，以丁香园为代表，微博，支付宝和腾讯跟上，都建立了自己的疫情实时播报页面。布局也都大同小异，最上面是确诊、疑似、重症、死亡和治愈五个最为重要的数字。下面是表现各个省份地区的疫情地图，和治愈情况、新增情况等的时间趋势。再下面是最新的新闻通报。\n中文维基上则记录了从报道开始每一天每一个省的详细情况，这里有日期和地区尺度上最为翔实的数据和增长率，并且每一个数字都有对应的引用来源。看起来相当规范。\n联合早报报道新加坡疫情时，采用了如下这种堆积的趋势图，可以更方便地观察确诊病例中不同情况人数的比例。\n另一个报道新加坡疫情以面板的形式展现了所有数字、趋势。其最大的特点是在右侧\u0008显示了每一个病例，点击就可以显示病例的性别，年龄，所处地区和确诊日期等等内容。\n微博上以 @江南剑心2 为代表的一批志愿者团队试图梳理每一个病例的情况，呈现一个地区亲属关系，聚集关系，是否为输入和非输入性。\n希望一切早点过去。\n","id":41,"section":"posts","summary":"一个简单地汇总。 这次疫情，以丁香园为代表，微博，支付宝和腾讯跟上，都建立了自己的疫情实时播报页面。布局也都大同小异，最上面是确诊、疑似、重症","tags":["统计图"],"title":"大家是怎么展现疫情的","uri":"https://www.xzywisdili.com/2020/02/2020-02-22-convirusgraph/","year":"2020"},{"content":"这次最大的收获是完全过了一遍文本分析，其中包括：\n文本清洗 词频分析 绘制词云 文本分类模型 我无意在博客中写出我是怎么做的或是贴出任何代码，因为网络上有太多太多相关的教程了，文本的，视频的。 当今世界，只要不是特别冷门的知识，想学习还是特别方便的。再一次歌颂互联网。 当然，虽然只是很粗浅地了解和实践，但至少自己能在一天之内做完一整套出来，还是比较有成就感的。 于是权且把最后的成果贴在这里。\n","id":42,"section":"posts","summary":"这次最大的收获是完全过了一遍文本分析，其中包括： 文本清洗 词频分析 绘制词云 文本分类模型 我无意在博客中写出我是怎么做的或是贴出任何代码，因为网络","tags":["记录"],"title":"记录自己用一天时间完成的成果","uri":"https://www.xzywisdili.com/2020/01/2020-01-10-newwork/","year":"2020"},{"content":"今天，官方公布了此次「武汉肺炎」的病毒病原体检测结果， 结论是这是一种新型的冠状病毒。那么，现在广大普通群众最关注的重点应该有两个：\n该病毒是否有人传人性质？ 该病毒的来源究竟是哪里？ 对于第一点，这里权且整理一下多方的观点，仅供参考：\n武汉市卫生健康委员会通报，个案发病日介于去年12月12日至12月29日，均在武汉市医疗机构隔离治疗，追踪的密切接触者也从121人增至163人，目前追踪者均无发烧等异常症状，且目前未发现明显人传人现象，也没有医护人员感染。武汉疾控中心主任李刚称，不明原因的肺炎病例数量可能还会有所增加。\n香港大学感染及传染病中心总监何柏良上周在接受多家媒体采访时，将武汉近期爆发不明肺炎疫情，定义为“新型病毒”，极有可能人传人。香港中文大学医学院呼吸系统学讲座教授许树昌也表示，病毒会否人传人仍有待观察，绝不可排除相关可能性。\n台湾卫生福利部疾病管制署副署长庄人祥表示，冠状病毒的潜伏期最长可达14天，1月1日至14日间若有新病例传出都可能是休市前就感染的，但若1月14日以后仍持续出现新病例，可能代表此病毒具有人传人能力；若没有新病例出现，或许可研判和动物感染源有关。\n所以，该病毒是否具有「人传人」确实还不清楚。担心会「人传人」的人可以购买 N95 口罩预防，而不担心的也至少应该持续关注相关新闻和最新进展。对于第二点，病毒的来源更是需要把第一点弄清楚之后进行更为细致的排查。因此我们还需要继续等待官方的结论。\n","id":43,"section":"posts","summary":"今天，官方公布了此次「武汉肺炎」的病毒病原体检测结果， 结论是这是一种新型的冠状病毒。那么，现在广大普通群众最关注的重点应该有两个： 该病毒是否","tags":["记录"],"title":"新型冠状病毒","uri":"https://www.xzywisdili.com/2020/01/2020-01-09-virus/","year":"2020"},{"content":"2020 年新年伊始，本应是好好鼓走干劲，大展宏图的一年。 没想到刚开年，就对自己失望至极。\n今年的一门小类课，班上一共7名同学。 期末考核是每人做一份PPT概括总结这学期学过的所有内容，也就是俗称的「pre」，每人限时10分钟。 本来考核早早的一周之前就布置了下来。 本来不是一项很难的任务，但是拖延症一直让我迟迟没有开始做 PPT，甚至都没有去想。\n等到昨天晚上，才告诉自己必须要开始做了。 却也是发呆到晚上10点钟。不得已，匆匆完成，赶工到早上 7 点，草草睡了 2 个小时，爬起来接着做。 终于在汇报前的一个小时，完成了。 但实在也是没有办法好好熟悉一下内容，过一遍 PPT。\n没想到老天也是不给面子，直接抽中第一个上台。 讲的也是磕磕绊绊，甚至才讲到我准备的 PPT 的一半，时间就已经到了。 老师也是不留情面，让我打住。我只好灰溜溜下台。 看着别人精心准备的内容，我心里实在是羞愧得紧。\n依然记得之前的一次汇报。那门课要求分组汇报选定的书目。 我当时是我们组 6 个人中唯一一个在一周内读完《论自由》的人。 之后索性就我一个人搜集，整理资料，制作 PPT，最后汇报。 我还记得很清楚，当时特别得意地用《V字仇杀队》的剧照作为我 PPT 的封面。 当时也准备了 5 个历史案例和故事，还有段子若干，娓娓道来，虽然也超时了，但并没有超时多久。\n毫无疑问，现在退步太多了。 我一直以来自认为不是一个自甘堕落的人，但不可否认惰性的恶魔犹如温水煮青蛙一样把我快煮烂掉了。 虽然如此，我也始终承认，一切都还有解法。\n","id":44,"section":"posts","summary":"2020 年新年伊始，本应是好好鼓走干劲，大展宏图的一年。 没想到刚开年，就对自己失望至极。 今年的一门小类课，班上一共7名同学。 期末考核是每人做一份P","tags":["记录"],"title":"别向小徐学习","uri":"https://www.xzywisdili.com/2020/01/2020-01-02-pathetic/","year":"2020"},{"content":"看到一条来自@peekaboo的推特：\n對李逍遙來說，一直留在十里坡，就算讓你學會劍神好了，你付出最大的代價是什麼？\n才不是時間，時間其實他媽的一點都不重要。\n是他將會錯過趙靈兒和林月如。\n#不要留在十里坡 #上路吧去遇見你們本該遇見的人\n一是对这款 20 年前，小学玩的仙剑奇侠传的怀念；更重要的是这段话让我深有感触。 别做「十里坡剑神」，2020年，是时候上路了。\n","id":45,"section":"posts","summary":"看到一条来自@peekaboo的推特： 對李逍遙來說，一直留在十里坡，就算讓你學會劍神好了，你付出最大的代價是什麼？ 才不是時間，時間其實他媽的","tags":["记录"],"title":"别做「十里坡剑神」","uri":"https://www.xzywisdili.com/2019/12/2019-12-30-bestoftheyear/","year":"2019"},{"content":"想来懒狗如我已经20天没有更新博客，对于之前每周一更的规划也已经食言了。 转眼想到博客的时候，就已经离2020年只剩3天。 不如写一个年度总结来送走离开的这一年，这次比较简单，就分为收获和遗憾两点来写。\n收获 外行管理员 由于组里项目的需要，购置了一台戴尔的服务器。 而安装和配置服务器的任务就交给了我和子川同学。 从安装系统，到死活连不上网，再到买PCI网口安装， 再到和学校信息中心沟通，再到ip，域名和路由器的配置， 中间实在经历了太多的困难和挫折。 到现在还算顺利地成为了一名三流服务器管理员。 我想这是我人生中难得的经历了。\n一份实习 从暑假一开始，我就去了中关村美年健康产业研究院实习。 这是美年集团成立的一家偏研究性质的公司。 没想到时光飞逝，如今已在此处实习了半年之久。 从一开始笨拙地登陆不上公司的服务器，到能独立完成了不少份数据分析和报告。 这其中，还和老师，师兄师姐们一起完成了和新华社合作的体检大数据糖尿病地图。\n能认识一群厉害的老师和师兄师姐，实在是太棒了。\n一份副业 从今年的 9 月份开始，我阴差阳错的开始了一份副业。 先是被拉入了一个 QQ 群，在接了一单之后，和中介菠萝君加了微信，从此就开始在日常接单了。 所谓的「接单」，其实就是给世界各地的大学生们辅导作业，或是写写代码。 每一单视难易程度的不同，大概平均就是几百元。 特别感谢菠萝君这几个月的照顾和机会，还经常吹我的彩虹屁。 但最大的问题在于，由于时差的缘故，我有时需要熬夜到很晚，所以准备在明年放缓这份副业的节奏，或者干脆另找一份副业。\n开始理财 这可以算做半份遗憾，半份收获。遗憾在于这么晚才开始理财，但是收获是终于开始理财了，总比未来再开始好。 通过研究和比较，选择了一种最适合我这种普通人的理财方式。首先，准备好三个盒子：\n第一个盒子需要存放 3-6 个月满足生活开销的钱，我放在支付宝的余额宝里面； 第二个盒子则用来做长期投资，我选择的是定投指数基金； 第三个盒子则是为自己的梦想礼物存钱。 每个月有收入的时候，会先保证第一个盒子放满，然后收入的 30% 会放在第二个盒子，剩下的钱就进入了第三个盒子。 同时我也开始记账，会发现每个月花销要比我预想的多了不少。还是需要控制很多花钱的地方。\n除此之外，今年我的老神舟光荣下岗，换了一台惠普地暗影精灵5，也体验了荒野大镖客2和纪元1800。感觉好爽。\n遗憾 由于我看过的电影和书都在豆瓣上标记过，刚才熟了一下，发现居然 2019 年看过了 129 部电影和剧集，却只读了 14 本书。 去年和前年读书比较多的原因是，平时会有很多的通勤时间，而通勤时读书就能轻松读到很多本。 反而现在最远也只有骑车的路途的时候，倒是不会特意抽时间读书了。\n另外还有好多遗憾，没能坚持背单词，没有戒掉饮料，没有好的作息，难以抵挡很多诱惑， 还没有对未来地清晰规划，还没有找到女友。\n希望 2020 年能更加进步。现在要继续准备下周地结课 pre 了。\n","id":46,"section":"posts","summary":"想来懒狗如我已经20天没有更新博客，对于之前每周一更的规划也已经食言了。 转眼想到博客的时候，就已经离2020年只剩3天。 不如写一个年度总结来","tags":["记录"],"title":"2019 年度总结","uri":"https://www.xzywisdili.com/2019/12/2019-12-29-annualsummary/","year":"2019"},{"content":"昨天晚上师姐速报：「师弟，帮忙算个东西，用SAS写」。\n一开始对我来说可能不是什么难事，打开文件一看，陷入了深深的沉思。 用一个截图说明：\n每一行代表一个研究对象，每个研究对象有 pm1 到 pm7 这 7 天所接触的 PM2.5 浓度（实际文件里面是 365 天）。 我们需要计算连续 2 天，连续 3 天，\u0026hellip; ，连续 7 天接触 PM2.5 浓度大于 35 的次数。 而且如果连续 3 天接触 PM2.5，那么只能算一次连续 3 天次数，而不能再重复计算连续 2 天次数。 换句话说，这里的每一个连续的天数都必须刚刚好好。\n也许有些写程序的小伙伴会觉得非常好写，但对我来说，想要使用 SAS 解决这个问题，直觉上还是比较费劲的。 昨晚在参加选调生宣讲的时候，就一直在想。回宿舍立马坐下敲了出来。\n首先，导入数据。将数据里每个值和 35 进行比较，先转换成由 0， 1 组成的数据集。 方便下一步的运算。\nDATA example; infile \u0026quot;G:\\04_code\\PM2.5数据.csv\u0026quot; DSD MISSOVER firstobs=2; input id pm1-pm356; RUN; DATA example_35; set example; array pm {365} pm1-pm365; do i = 1 to 365; pm[i] = pm[i] \u0026gt; 35; end; drop i; RUN; 接下来，就是最关键的部分。 这里是利用 SAS 在 DATA 步读取数据的同时，插入一个循环。 这个循环长度就是 365，把每个研究对象的数据由头读到尾，在读的过程中分析数值 1 的连续情况。\nDATA PM_days(keep=id day2 day3 day4 day5 day6 day7); set example_35; array pm {365} pm1-pm365; do i = 1 to 365; if pm[i] eq 1 then do; count+1; end; if pm[i] eq 0 | i eq 365 then do; if count eq 2 then day2+1; if count eq 3 then day3+1; if count eq 4 then day4+1; if count eq 5 then day5+1; if count eq 6 then day6+1; if count eq 7 then day7+1; count = 0; if i eq 356 then do; output; day2 = 0; day3 = 0; day4 = 0; day5 = 0; day6 = 0; day7 = 0; end; end; end; RUN; PROC SQL; create table PM_result as select * from example, PM_days where example.id = PM_days.id; quit; PROC PRINT data=PM_result(obs=10); RUN; 说实话，这段代码写下来还真有点 C 语言的感觉。 当然在循环中，对于 day2-day7 的操控和赋值可能还有改进的空间，即把这些变量也编入一个数组。 但既然只要求到了最多 7 天，这样其实也足够了。\n最后赶在 10 点之前，交给了师姐，师姐表示很欣慰。 不过，我同样感受到了 SAS 实在是一种充满个性的数据分析语言。 打个比方，写 python 和 R 像是骑马，写 SAS 就像骑河马。 而且，此时此刻最让我难受的是，Markdown 也和河马过不去。\n","id":47,"section":"posts","summary":"昨天晚上师姐速报：「师弟，帮忙算个东西，用SAS写」。 一开始对我来说可能不是什么难事，打开文件一看，陷入了深深的沉思。 用一个截图说明： 每一行","tags":["SAS","编程"],"title":"工具人时刻","uri":"https://www.xzywisdili.com/2019/12/2019-12-09-smalltool/","year":"2019"},{"content":"时间匆匆忙忙到了 2019 年的 12 月，我自己也是匆匆忙忙的事情。至于即将到来的年终总结什么的，就随缘了。 最近忙的事，既然暂时告一段落，就有必要开个博客总结一下。 所以这一篇可能内容会比较杂乱反复。\n地图和 shiny 玩出新花样 在 shiny 中，选择想要绘制的美国州名，再单独绘制这一个州的情况，达到如下图的效果：\n当然，在众多的美国地图文件中，如果选择边缘没有那么精细的版本，就能让渲染速度更快一点。 shiny 实在是一个好用好玩的东西。\nR 也能写简单的网络爬虫？ 有一个项目是需要整理 2015 到 16 赛季 nba 球员的场均得分、场均助攻等数据。 我们可以直接在 篮球数据网站 进行爬取。 这事本来比较适合直接拿 python 跑，但出于一些原因，还是得用 R。 但意外地发现，其实也不是特别复杂，解析，xpath 查找也都不在话下，核心爬取部分也就不到 30 行代码。\nlibrary(tidyverse) library(xml2) # 得到某个球员的查询地址 # 举例，输入\u0026quot;Derrick Rose\u0026quot;，函数返回\u0026quot;/players/r/rosede01.html\u0026quot; get_player_url \u0026lt;- function(player_name) { query_url \u0026lt;- \u0026quot;https://www.basketball-reference.com/search/search.fcgi?hint=\u0026amp;search=\u0026quot; url \u0026lt;- paste0(query_url, str_replace(player_name, \u0026quot; \u0026quot;, \u0026quot;%20\u0026quot;)) player_url \u0026lt;- url %\u0026gt;% read_html() %\u0026gt;% html_nodes(xpath='//div[contains(@class, \u0026quot;search-item-url\u0026quot;)]') %\u0026gt;% html_text() return(player_url[1]) } # 得到某个球员的数据 # 举例：输入\u0026quot;Derrick Rose\u0026quot;，函数返回该球员相关的 Age, Tm等等数据 get_player_data \u0026lt;- function(player_name) { print(player_name) base_url \u0026lt;- \u0026quot;https://www.basketball-reference.com\u0026quot; player_url \u0026lt;- get_player_url(player_name) if (is.na(player_url)) {url \u0026lt;- paste0(base_url, \u0026quot;/search/search.fcgi?hint=\u0026amp;search=\u0026quot;, str_replace(player_name, \u0026quot; \u0026quot;, \u0026quot;%20\u0026quot;))} else {url \u0026lt;- paste0(base_url, get_player_url(player_name))} player_data \u0026lt;- url %\u0026gt;% read_html() %\u0026gt;% html_table() %\u0026gt;% .[[1]] %\u0026gt;% filter(Season == \u0026quot;2015-16\u0026quot;) %\u0026gt;% select(Season, Age, Tm, Pos, PTS, AST, TRB, STL, BLK, FG, FT, TOV) %\u0026gt;% mutate(Player = player_name) return(player_data) } 病例和对照的匹配 在病例对照研究中，对于一定数量的病例，要在对照组中选择一定比例的对象作为对照，这个过程叫做匹配（Match）。 而匹配一般会以性别，年龄等特征作为变量。 这次的匹配任务是使用年龄进行匹配，但查了网上的一些资料，大都是你要用什么什么，很少告诉你怎么用的。 自己操练了以下，用 MatchIt 可太简单了：\nlibrary(MatchIt) # 匹配 match_it \u0026lt;- matchit(group~年龄, data=data, method=\u0026quot;nearest\u0026quot;, ratio=1) a \u0026lt;- summary(match_it) # 提取匹配好的数据 df_match \u0026lt;- as_tibble(match.data(match_it)[1:ncol(data)]) 各种选项和奥秘都在 matchit 函数中，包括 method，ratio 等等。\n荒野大镖客2的游玩体验 这周通关了Rockstar Games 的新作 荒野大镖客2 的一周目。 本来应该专门用一篇博客来聊一些通关的感想，但是看到网路上已经有不少文章，各种深度，各种角度都有。 自己就也不用煞有介事地再去聊了。\n对我而言，R星最大的特质在于他会让你关注社会，关注现实。 纵观几部作品，表面上看，他在打造一个越来越真实，细节越来越丰富的开放世界。 但是从剧本层面来讲，他通过游戏让你：认识你自己、关注社会的现状和趋势、找到自己的归宿。\n比如亚瑟，是一个忠心耿耿的帮派打手，同时又具有情感和正义感的西部牛仔。 而美国文明和秩序的边界在不断向西部扩张，传统的牛仔和帮派文化注定走下历史舞台。 无论亚瑟如何反抗，都不可避免地成为牺牲。 在这个无比悲壮的故事背后，可以看到R星一直以来对美国历史和社会的观察，而非仅仅把玩家从现实世界抽离。 R星之于育碧，小丑之于漫威，都是一样的道理，我喜欢讨论现实的东西。\n另外，最近还做了一个时间序列分析，但没有时间写了。另外，新开了游戏《纪元1800》，这游戏简直太上头了。 连续两天玩到快两点，今天不能再这样了。\n","id":48,"section":"posts","summary":"时间匆匆忙忙到了 2019 年的 12 月，我自己也是匆匆忙忙的事情。至于即将到来的年终总结什么的，就随缘了。 最近忙的事，既然暂时告一段落，就有必要开个博客","tags":["R 语言","可视化"],"title":"最近我都做了些什么","uri":"https://www.xzywisdili.com/2019/12/2019-12-04-whathaveidone/","year":"2019"},{"content":"在R语言中绘制地图，尤其是可交互式的地图的另一个利器是 leaflet 包。 这次我会借着我刚刚完成的一个小项目来讲解这一内容，目标是仿制美国 CDC 官网上的一张图——是的，我们的「战场」从中国转移到了美国。\n数据准备 官网上有相关数据的下载。除此之外，我们还需要美国的 shapefile 文件。 一般来说，国外的地图文件去 GADM 上下载问题就不是很大。 接下来，对于这一任务来说，可以通过 ArcGis 方便地将包含各州数据 csv 文件 join 到地图文件上。 这一步唯一要注意的就是地图文件对应字段的类型（一般来说是 Text）要与各州数据类型一致。\n在这一系列的上一篇文章中，我当时将这一 merge 过程在 R 里用代码写了出来。 这两种方法都可以，如果需要批量绘制地图的话，当然还是之前代码的方法更好。\nleaflet 绘制地图 leaflet 实在是绘制地图的一大利器。它不仅强大，还足够简单。 在 Youtube 上面也有太多相关的教学视频，每一讲大概2-3分钟，看起来也不累。 使用 40 行左右的代码就可以生成我们需要的地图：\nlibrary(leaflet) library(rgdal) library(sf) library(htmltools) library(htmlwidgets) USA_data \u0026lt;- st_read(\u0026quot;map2/USA_data.shp\u0026quot;) # 4个分类的配色定义 pal \u0026lt;- colorFactor( palette = c('#fff5f0', '#b4b4b4', '#fb6a4a', '#fcbba1'), domain = US$category ) # 每个州显示的标签内容 USA_data$label \u0026lt;- paste(\u0026quot;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;\u0026quot;, USA_data$NAME, \u0026quot;\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026quot;, \u0026quot;Category: \u0026quot;, USA_data$category, \u0026quot;\u0026lt;br\u0026gt;\u0026quot;, \u0026quot;Percent Change From 2016-2017: \u0026quot;, USA_data$change, \u0026quot;\u0026lt;br\u0026gt;\u0026quot;, \u0026quot;Statistically ignificant: \u0026quot;,USA_data$significan, \u0026quot;\u0026lt;br\u0026gt;\u0026quot;, \u0026quot;2016 Number: \u0026quot;, USA_data$X2016number, \u0026quot;\u0026lt;br\u0026gt;\u0026quot;, \u0026quot;2016 Rate: \u0026quot;, USA_data$X2016rate, \u0026quot;\u0026lt;br\u0026gt;\u0026quot;, \u0026quot;2017 Number: \u0026quot;, USA_data$X2017number, \u0026quot;\u0026lt;br\u0026gt;\u0026quot;, \u0026quot;2017 Rate: \u0026quot;, USA_data$X2017rate) # 绘制地图并保存 my_map \u0026lt;- leaflet(USA_data) %\u0026gt;% addPolygons(color = \u0026quot;#444444\u0026quot;, weight = 1, smoothFactor = 0.5, opacity = 1.0, fillOpacity = 0.5, fillColor = ~pal(category), highlightOptions = highlightOptions(color = \u0026quot;white\u0026quot;, weight = 2, bringToFront = TRUE), label=lapply(USA_data$label, HTML), labelOptions = labelOptions(textsize = \u0026quot;15px\u0026quot;)) %\u0026gt;% addLegend(position=\u0026quot;bottomright\u0026quot;, pal=pal, values=~category, title=\u0026quot;Category\u0026quot;, opacity=1) %\u0026gt;% setView(lng=-98.134, lat=38.053, zoom=4) saveWidget(widget=my_map, file=\u0026quot;my_map.html\u0026quot;) 这段代码的核心部分在于一开始使用 sf 包的 st_read() 读取 shapefile 格式的地图文件。 之后使用 leaflet 生成对应的地图对象，再使用类似 ggplot 里面的绘图思路加上各个图层。 addPolygons 方法里面提供了很多参数和选项，包括鼠标悬浮时的高亮选择，悬浮时候的标签和标签的样式。 这样，我们就可以在 USA_data$label 中定义好想要的标签内容，再结合标签样式达到图中的效果了。\n阿拉斯加和夏威夷 当然，如果你照着上面的代码绘制，会发现结果和上图有一些不同。 那是因为按照现实地图中的经纬度绘制，阿拉斯加和夏威夷会分别出现在地图的左上角后左下角，导致整个地图显示十分不和谐。 但我们也不能直接不绘制这两个州。 所以一般通用的方式是将这两个州经过一定的坐标变换放置在图中的合适位置。\n而不得不说国外有专门处理这一地图的 R 包：albersusa。 这个包里提供了 usa_sf() 这一函数，可以取用美国大陆的指定州，实在好用。 这一部分代码我就不再贴出。\nleaflet 与 R shiny leaflet 与 R shiny 结合起来使用也实在是异常顺滑，逼格满满。 最让人舒服的一点是，R shiny 也异常清晰易懂，也存在大量官方和民间教程可供学习。 我可以在地图的左上角放置选项框，并对选中分类的州进行高亮显示：\n最后，不得不感叹，在 leaflet 包的强大加持下，交互式地图可视化也能变得如此轻松愉快。 这竟然也让上一期的 ggplot 落后得有点像上个时代的产物。不过当然这是一句玩笑话。 一切都越来越有意思了。\n","id":49,"section":"posts","summary":"在R语言中绘制地图，尤其是可交互式的地图的另一个利器是 leaflet 包。 这次我会借着我刚刚完成的一个小项目来讲解这一内容，目标是仿制美国 CDC 官网上的一张图","tags":["R 语言","可视化"],"title":"我在 R 里画地图（二）","uri":"https://www.xzywisdili.com/2019/11/2019-11-18-rmap2/","year":"2019"},{"content":"netflix 推出了一款新剧 政客（The Politician）。 这部剧在国内并未激起太多波澜，也几乎未见有人对其讨论。得益于昨晚好友山神推荐，我打开了第一集竟没有停下来，一口气看完了第一季。\nkeep real 政客是瑞恩·墨菲利用高中生竞选学生会主席的环境语境之下所撰写的赤裸裸的政治讽刺剧。 很多人将此剧视为男主人公佩顿是如何打造形象，击败竞选对手，从而成功的传统励志剧。但事实并非如此。\n在剧集一开始，佩顿和里弗面对委员会答辩之时，佩顿的慷慨陈词颇有政治家满分回答的感觉，但里弗却走下演讲台， 向观众娓娓道来自己曾经抑郁想要自杀的极富情感性的经历。后来剧情急转直下，里弗面对佩顿在家中开枪自杀。 到这时我才明白，原来里弗和佩顿其实就是男主人公内心的一体两面。自杀正象征着佩顿正演化为一个冷血政客。\n之后剧集更是高潮迭起，仿佛一刻也不想让观众停下。学院同级生的选票成了佩顿心中唯一所关注的东西，他要为此打造形象，笼络人心。 编剧也给我们端上了几盘硬菜：与女友编造狗血爱情剧树立自己的受害者形象博取同情，拉拢假癌症患者英菲尼迪作为自己的选举搭档，在家庭内更是略施巧计击败了两个哥哥…… 在整个过程中，政治野心在佩顿心中远大于一切。在假戏真做后，他在走廊里没有回头看女友一眼。而在副主席事件中， 他更是对于英菲尼迪正在通过伤害她的身体把她伪装为癌症患者的事实丝毫不关心，反而最关心这件事的暴露是否会影响自己的仕途。 而最为经典的莫过于在本季仅为 28 分钟的第五集，以选民视角将佩顿的伪善面目展现的淋漓尽致。他真的不会关心你想要的究竟是什么，他只是满足于利益和选票的交换而已。剧情也证明了他是一个只会空谈而并非能做实事的人。且先不说上任后诸多不切实际的提案被否定，我们来看看他在竞选之中所做的两件事。 第一件是献血事件，其本质完全是为了调查英菲尼迪是否是自己竞选图中的隐患；而第二件则是禁枪法案，佩顿动用了家族的财产力量做足了表面功夫， 但可惜后面剧情中两把关键性的枪的出现又直接打了佩顿的脸。可以说，从投票前夕的佩顿身上，我们看不到一点真的东西。 他对英菲尼迪，对母亲，对身边人的所有关怀都让人忍不住打一个问号。\n而剧作中也设置了两个与佩顿形成鲜明对比的角色。一个是露西饰演的艾斯垂德。导演对这个角色的设置颇为巧妙。 她的竞选动机来源于对里弗的追念和对佩顿利用里弗竞选的极度厌恶。但由于不足够强的胜负心，再加上她善良的本性，导致她看起来愚蠢而又脆弱。 而她又是一个对错分明的人，竟然将自己多年经营不义之财的父亲举报入狱。而父亲终于在被逮捕之际大声喊出对她自豪的肺腑之言。 众所周知，艾斯垂德这样一个善良的人是不堪一击的，也是难以在这种政治斗争中幸存的。另一个人便是里卡多， 他可能没什么智力，脑子还保留着一些原始冲动，外加他的日常样貌也和佩顿精心打扮过的外表完全相反，但他对英菲尼迪毫无保留的爱构成了他一切行动的动机。 事实上，正是他对于英菲尼迪那封旧录像带的曝光才真正拯救了她的前途命运。但这位迷途少女却还是对佩顿深信不疑。（我能理解编剧对于这段情节的设置，但这里显得未免有些生硬。）而艾丝翠德和里卡多的出逃性“私奔”则是剧中为数不多的人性闪光，可惜也寓意着真的东西会被所有或虚伪的现实扫地出门。\n细节上的亮点 最让人印象深刻的莫过于片头，一切成绩单、布料的裁剪、家庭绊脚石的清理等等全都以微缩景观的形式呈现。 而男主先是以木偶的形式出现，经过打磨和润色，再套上衣服，形成了表面光鲜的形象也是将政治的本质揭露无疑。 政客在其中，可不就是扮演木偶的角色吗。\n而让我记忆犹新的亮点则是剧中频频提到的中国普通话。我认为这一细节更多的是指代与中国交好的尼克松。而尼克松也在波兹曼的著作《娱乐至死》中被当做例子提及。 即在尼克松和肯尼迪竞选总统的时候，电视辩论首次被引入竞选。此处也是作为政客个人形象愈发影响，甚至在当选与否中起决定性因素的一个暗喻。\n而最后一集结尾的设置也非常有趣。导演在为第二季铺路，需要交代本州竞选情况和政治风貌。 但这些内容并没有使用旁白或一板一眼的方式，反而很巧妙地设置了一个颇有学历的实习生，初入竞选办公室并与其上司进行对峙的一场戏完成。 所有情况交代得明明白白。其中还穿插着 windows 95 和 98 的小细节也十分有趣。\n另外也可以明显地感觉到主创团队对于其中人物的服装选择和搭配也花了不少心思。比如团队里那个负责数据的酷酷的中性 boy 从头到尾就是一身淡蓝衬衫和西装裤的搭配，而爱丽丝则比第一夫人穿得还更第一夫人，而最后与佩顿渐行渐远，和其他男人订婚订婚之时，着装风格也发生了变化。\n未来展望 在第一季将近结尾，可以明显看到佩顿的政治梦出现极大挫折之际，里弗以灵魂形象屡屡出现，再加上一位神秘亚裔人士的点拨之下，佩顿试图重新追回爱情。 也许在学生会主席系列事件的重创之下，佩顿的野心与梦想受到了冲击，使得他开始审视自己的内心。\n上文也提到过，在结尾剧作镜头一转，通过一个实习生视角讲明了本州的政治风貌。 我想，也许正如一些豆瓣网友所预测的那样，第二季的内容就会是佩顿带领着他的竞选团队对地方议员进行冲击。 但我并不希望只看到佩顿是如何诡计多端地一路过关斩将，实现他的政治野心的。 一来，录影带，毒杀，刺杀，背叛等等戏码都已在第一季悉数登场，老调重弹只会令观众缺失惊喜感。 二来，政治场上的尔虞我诈已经被纸牌屋拍到难以超越，我想政客自己对标的也并非那样的美剧。 我更希望看到的是佩顿自我性格的补全，和竞选小队中各个成员追随佩顿的理由。 当然，我也十分期待主创团队能在第二季给观众们带来更多的剧情冲击。\n","id":50,"section":"posts","summary":"netflix 推出了一款新剧 政客（The Politician）。 这部剧在国内并未激起太多波澜，也几乎未见有人对其讨论。得益于昨晚好友山神推荐，我打开了第","tags":["美剧","影评"],"title":"政客生活 101","uri":"https://www.xzywisdili.com/2019/10/2019-10-26-politicians/","year":"2019"},{"content":"上一周如果用一个词来概括，那就是“荒废”。 感觉自己越来越难以控制自己的情绪，经常陷入到一种无来由的失落之中，脑子里胡思乱想。 这样似乎越来越难以集中精力，已经有点影响自己的工作和学习。 “生活就像巧克力”的比喻太温柔了，要我说倒像是“糖果”，打开糖果纸，你可能会吃到空气，吃到钢珠球，吃到臭蛋，吃到变质的糖果，就是不会吃到甜甜的糖。\n总结一下上一周完成的工作：\n继续修改问卷小程序，这就是一项不停返工，反复打磨的工作，至于打磨的结果如何，还得等个把月； 在之前绘制地图的基础上，更新了2.0版本，而且已经迫不及待地在昨天的 post 里面写清楚了整体的绘制思路； 整个周末，完成了系里奖学金答辩审核材料的所有工作； 马马虎虎地看了一些 QGis 的资料，GIS 之路任重道远； 继续坚持健身，继续坚持早睡早起，但一直也没戒掉含糖饮料； 很遗憾还是没抽出多少时间读书，也没怎么更新博客。 这一周继续努力吧。\n","id":51,"section":"posts","summary":"上一周如果用一个词来概括，那就是“荒废”。 感觉自己越来越难以控制自己的情绪，经常陷入到一种无来由的失落之中，脑子里胡思乱想。 这样似乎越来越难","tags":["记录","周记"],"title":"一周总结（9.16-9.22）","uri":"https://www.xzywisdili.com/2019/09/2019-09-24-weeknote/","year":"2019"},{"content":"前言 大约一个月前，接到一个任务：绘制全国的患病率地图，就草草做了一个初版，当时还做了一个 ppt 和组里的小伙伴分享。 直到后来，才知道这种图有一个专业的称呼：choropleth maps，中文名字是分层设色图。 可以看到在 google 中检索 choropleth maps 得到的结果：\n单在画图这一方面讲，其实这是一个老生常谈的话题，也有数不胜数的工具和包。但问题的关键在于：绝大多数国外提供的中国地图并不规范。 关于中国地图的规范问题，在姜大伟的知乎专栏 使用中国地图的正确姿势 中有比较翔实的介绍。我也根据此做成了其中一张幻灯片：\n所以问题摆在了我们的面前：\n找到一张可供使用的规范的中国矢量地图； 根据这张地图绘制我们想要的分层设色图。 提前提醒：本文代码直接复制粘贴不能够运行，请到文末下载分享的全部程序和地图文件！\n寻找标准地图 在网络上，使用最多的是一份名为 bou2_4p.shp 的地图矢量文件，这份文件来自哪里已不可考，似乎是 2012 年国家提供的 1：400 万地理信息地图，但是后来又关闭了开放。 这份地图应该是目前问题最少的地图，藏南、台湾、南海诸岛也都存在。但是时间已经来到了 2019 年，这份 12 年的地图是否那么无懈可击呢？\n答案是否定的。根据 b 站 up 主“地理人_zxl”的视频 ArcGis更正老式中国基础地理信息数据错误，通过 天地图 和 bou2_4p 的比对，还是可以发现新疆的边界存在一些问题（红线为 bou2_4p，底图为标准地图）：\n同时，bou2_4p 的地图也没有澳门特别行政区。通过上面视频中提供的方法，在 ArcGis 中对 bou2_4p 进行修改，就可以得到修正版本的 bou2_4p 了。 这就是我们接下来要使用的标准地图。\n使用 R 绘制地图 首先要说明，在我个人使用过的工具和包里面，还是 ArcGis 这种专业 GIS 软件绘制地图最为顺滑快捷。 那为什么要使用 R 语言呢？其实还是出于以下这几个原因：\n不只我要出图，别人也要进行地图制作； 免费开源，安装便捷，上手简单； 只需改动患病率数据，运行代码就可出图 再次提醒，以下我不会贴出完整代码，只会贴出一些核心的重点代码。完整的代码会分享在文章末尾。\n基础绘图 首先，我们通过 rgdal::readOGR 读取修正过后的中国地图，然后对数据框进行转换，再和患病率数据 da 进行合并。\n患病率数据 da 长什么样子呢？你一看便懂：\nNAME rate 安徽省 15.89 北京市 12.85 福建省 15.14 黑龙江省 NA 这里的患病率数据中用 rate 写明了每个省的患病率大小。然后通过 ggplot 绘制出我们需要的基础制图。\nchina_map \u0026lt;- rgdal::readOGR(\u0026quot;map//bou2_4p.shp\u0026quot;, use_iconv = TRUE, encoding = \u0026quot;UTF-8\u0026quot;) xdata \u0026lt;- china_map@data xs \u0026lt;- data.frame(xdata, id=as.character(seq(0, 924))) china_map1 \u0026lt;- fortify(china_map) china_map_data \u0026lt;- left_join(china_map1, xs, type = \u0026quot;full\u0026quot;) china_data \u0026lt;- left_join(china_map_data, da, by='NAME', type=\u0026quot;full\u0026quot;) ggplot(china_data, aes(long, lat))+ geom_polygon(aes(group=group, fill = rate), color = 'white', size = 0.25) 我们可以在这一步对 x 轴和 y 轴的范围进行裁剪，修改地理坐标投影系，并修改绘图主题，隐藏 x 轴和 y 轴等等。下一步再进行进一步的美化。\n美化底图 接下来就是进行地图的进一步美化，比如我们想要更改色阶的颜色。这里的 p1 就是我们上一步保存的底图。 第二步的核心在于 scale_fill_gradientn，可以指定无限数目的渐变色了。 而 na.value 则是指定缺失值省份的颜色。\np2 \u0026lt;- p1 + scale_fill_gradientn( colours=c(\u0026quot;#4C79B0\u0026quot;, \u0026quot;#9FD2AE\u0026quot;, \u0026quot;#F4C4B2\u0026quot;, \u0026quot;#BE1E56\u0026quot;), na.value = ’#D3D3D3' ) p2 顺带一提，想要选取好看的颜色，可以试试在 google 图片里面搜索 choropleth map。 如果看到顺眼的颜色，可以直接得到色彩的 hex 值复制到自己的图里。\n绘制九段线 九段线的地图文件来自 l9.shp，这里我们想要把九段线绘制在整个地图的右下角，并且小图中出现的广东等省份也是染好色的。 思路就是把整张图和九段线全部画出来，再通过经纬度截取出我们想要的那部分：\nl9 \u0026lt;- rgdal::readOGR('map\\\\l9.shp') l91 \u0026lt;- fortify(l9) china_map \u0026lt;- rgdal::readOGR(\u0026quot;map//bou2_4p.shp\u0026quot;, use_iconv = TRUE, encoding = \u0026quot;UTF-8\u0026quot;) xdata \u0026lt;- china_map@data xs \u0026lt;- data.frame(xdata, id=as.character(seq(0, 924))) china_map1 \u0026lt;- fortify(china_map) china_map_data \u0026lt;- left_join(china_map1, xs, type = \u0026quot;full\u0026quot;) china_data \u0026lt;- left_join(china_map_data, da, by='NAME', type=\u0026quot;full\u0026quot;) p9 \u0026lt;- ggplot(china_data, aes(long, lat))+ geom_polygon(aes(group=group, fill = rate), color = \u0026quot;white\u0026quot;, size = 0.25) + coord_map(\u0026quot;albers\u0026quot;, lat0=27, lat1=45) + scale_fill_gradientn( colours=plot_color, guide = \u0026quot;none\u0026quot; ) + geom_line(data=l91, aes(x=long, y=lat, group=group)) + coord_cartesian(xlim=c(105,125),ylim=c(3,30)) 得到九段线小图后，再通过 grid 包把这个小图放置在原图的右下角。\nlibrary(grid) vie \u0026lt;- viewport(width=0.225, height=0.15, x=0.78, y=0.15) print(p9) print(l9, vp=vie) 这样，就达到了如下图这样的效果：\n添加省份名字标签 在第一个版本做好之后，老板说让我可以加上省份名称的英文标签。 我想，不如就再增加一个添加省份名称标签的功能，中英文都整上。 这里，如果熟悉 ggplot 的朋友，应该会觉得很简单。 只要准备好了两份数据文件，分别是 cn_text 和 en_text，里面记录每个省的标签名字和标签的位置。 然后使用 geom_text 即可完成：\ntext_da \u0026lt;- read.csv(\u0026quot;map\\\\cn_text.csv\u0026quot;) p3 \u0026lt;- p2 + geom_text(data=text_da, aes(label=text)) 在 geom_text 里面，修改文字的字体，大小，颜色自然也不在话下。 除此之外，只要把 geom_text 替换成 geom_shadowtext，就可以使用带白边的标签，效果如下：\n在我的破电脑上，如果单次开始运行整个绘图程序，到出图保存，大概需要 10s 左右。 这可能是一个劣势，当然也不是不能接受的。\n后记 想来这篇也拖了很久的时间，着实不太应该。 本来想把所有内容写在一篇内容中，但是想想我也不易写，读者也不易读，实在是不太好。 于是我只把最最核心的内容记录在博客中，主要是陈述思路，而删减了很多内容。\n这里，我把所有地图文件，程序和说明文档打包在了这里。 我已经尽量做到简单易用，这个系列的第二篇就写写怎么画插值热力图。 之后我还会继续研究绘制地图的相关方法，有新的技巧，感想和技能还会更新。\n","id":52,"section":"posts","summary":"前言 大约一个月前，接到一个任务：绘制全国的患病率地图，就草草做了一个初版，当时还做了一个 ppt 和组里的小伙伴分享。 直到后来，才知道这种图有一个专","tags":["R 语言","可视化"],"title":"我在 R 里画地图（一）","uri":"https://www.xzywisdili.com/2019/09/2019-09-23-rmap/","year":"2019"},{"content":"晚上在全屏看视频的时候，不小心按到了一个键，直接调用了电脑的前置摄像头， 满屏出现我的大脸，差点没把我吓到凳子下面。\n过去的一周前半程处于期待中秋节的喜悦中，后半程就是一个人的中秋了。 当然也有不少值得总结的内容：\nHPV 的地区分异和 ALT 的标准制定两个研究都完成了中期的分析，两位博士老师也有别的工作，就先告一段落了 动脉粥样斑块的相关危险因素的 cox 比例回归模型上周已经跑完，这周也能完成一个收尾 本来上周就要更新的“我在R里画地图”迟迟没有写完，中秋又发现旧的地图文件出现了一点问题，亟待修正 利用每天晚上过了一遍 matplotlib 包的内容，其实这种包学起来还可以，只要记住哪种图使用哪个函数，等到用的时候查文档就能解决绝大多数问题了 继续和服务器与小程序扯皮，成功在中秋之前更新了一版新的，但中秋期间就被小伙伴又提了不少问题，还好今天都已经总结反馈 假期重温了 蝙蝠侠：黑暗骑士 和 泰坦尼克号，又看了 出租车司机 和 亢奋，让我更热爱电影艺术了 这周在读 金雀花王朝，虽然才读了一半，但英格兰王室的宫斗大戏已经让人印象深刻，小说话的叙事也属实精彩 这也是健身的第二周，感觉并没有什么变化，全身上下依然酸痛，白天也还不是很有精神 经过一周的调整作息，每天都能在12点躺在床上，但是依然不能及时睡着，经常躺到快两点才能入眠 今天看到一个观点，说人一天的注意力是有限的，而且从项目A转移到项目B上会耗散很多精力，比如现代生活的罪魁祸首即时通讯软件。 我想要试一试抵制“即时”，只在每天的几个固定时间，比如8点，12点，16点，20点才看绿色聊天软件。\n另外的疑惑是精力管理，每天顶着咖啡和红牛并非长远之计，规律作息和定期锻炼是必须的，但也需要寻找到别的方法。 最后给自己提一点要求，希望能保持博客的更新，哪怕忙的时候，每天写一二百字也好。\n未来的一周，加油！\n","id":53,"section":"posts","summary":"晚上在全屏看视频的时候，不小心按到了一个键，直接调用了电脑的前置摄像头， 满屏出现我的大脸，差点没把我吓到凳子下面。 过去的一周前半程处于期待中","tags":["记录","周记"],"title":"这一周（9.9-9.15）","uri":"https://www.xzywisdili.com/2019/09/2019-09-16-weeknote/","year":"2019"},{"content":" You talking to me（你在和我说话吗）？\n看过 出租车司机 之后，我本想把朋友圈的封面换成 坐在出租车内的德尼罗，但奈何绿色聊天软件对于图片糟糕的裁剪只能作罢。\n本来，中秋就是团圆的最强节日符号。但是马丁・斯科塞斯和德尼罗会把你的脑子洗刷得一干二净。 很少有如此能将孤独者刻画得如此真实的影像。虽然这个世纪，每个人都声称自己或多或少有点心理问题， 但真正能对崔维斯感同身受的，恐怕少之又少。毕竟孤独症也并非千篇一律。\n影片中，崔维斯坐在出租车里，与外面灯红酒绿的纽约大街形成了两个世界。 雨后闪烁着霓虹灯的街道在迈克尔・查普曼的镜头里面如同梦境一般。 有时，崔维斯只是一个观察者，一言不发地听着后排落魄的男人宣泄情绪； 有时，他又想涉足车外的世界，让这个社会留下自己的一点印记。 我倒不统一他是正义感爆棚，如同超级英雄一般试图拯救罪恶与堕落的社会， 他就是想为自己找一个出口。\n说到出口，不得不提到那个老掉牙的话题：人生的目标到底是什么？ 这个问题重要到每个人都必须面对，且或早或晚给出自己的回答。 我们支持大众所支持的，反对大众所反对的，追求大众所追求的，吸收大众所吸收的。 我们连自己的人生都没有捋清楚，却不停地在为别人出谋划策； 我们顶着自私利己的蠢脑袋，却一直在为人类的大命题寻找解答； 我们假装问题已经有了答案，用美食，旅游，健身麻醉自己，甚至像 亢奋 里面吸得满嘴冒泡。 所有人都一定会找到属于自己的那个出口，有时候是工作，有时候是酒精。\n在某个午后，坐在会议室里面，几个熟悉的面孔在不停的讲话。 总有一种感觉，身处一室，却相隔千里。 需要不停地问自己：我为什么在这里。 你需要不停地欺骗自己，蒙蔽自己，才能心安理得地在寻常的日子完成应该完成的工作。 所以，最令我好奇的，无非就是，崔维斯坐在竞选办公室门口的出租车里，盯着贝西看的时候， 心里到底在想什么。也许这个答案可以在马上上映的 小丑 里找到。\n出租车司机这部电影会成为目前唯一一部我封存硬盘中，却再也不会打开看第二篇的电影。\n","id":54,"section":"posts","summary":"You talking to me（你在和我说话吗）？ 看过 出租车司机 之后，我本想把朋友圈的封面换成 坐在出租车内的德尼罗，但奈何绿色聊天软件对于图片糟糕的裁剪只能作罢","tags":["记录","电影"],"title":"计程车人生","uri":"https://www.xzywisdili.com/2019/09/2019-09-15-taxidriver/","year":"2019"},{"content":"悲报：2019年的我，只能做6个标准俯卧撑了。\n对，没错，在中秋节这天晚上，如此适合聚会的日子，我去健身了。 还好回来的路上抬了头，没有错过月亮。 十五的月亮没有辜负这个日子，云也适时地散开。 一个人走在安静的校园里独享这一份静谧与美好，说实话有一点奢侈。 (原谅月亮被我的 ip8 拍糊了)\n想起大一那年，中秋每年都是刚入学的第一个节日。 那年中秋还记忆犹新，几个高中的小伙伴一起压校园里的马路， 谈着未来和理想。之后的三年，就基本没有中秋的特殊记忆了。 去年的这个日子，我在深圳，但也恰好是要和深圳说再见的时候。 深圳是我特别喜欢的城市，绿化良好，空气清新。 当时也是月朗风清，整块圆月悬在夜空。 没想到回到北京也还能这么清楚地看到这么大的月亮。\n随后，吃了月饼，还跟风画了一个月饼。椰蓉的，特别好吃。 一个美好的夜晚。\n","id":55,"section":"posts","summary":"悲报：2019年的我，只能做6个标准俯卧撑了。 对，没错，在中秋节这天晚上，如此适合聚会的日子，我去健身了。 还好回来的路上抬了头，没有错过月亮","tags":["记录"],"title":"好一个中秋","uri":"https://www.xzywisdili.com/2019/09/2019-09-13-midautumn/","year":"2019"},{"content":"刚刚以老年人的配速跑完了2km，不过实在太喜欢这种在雨后大口呼吸空气的感觉了。 清新的空气确实能让人头脑更加清醒，整个身体也更有活力。 也不知是不是古人也有同感，如此推崇“气”的概念。\n最近工作上遇到了不少难题，让人甚是头疼。 当然在办公室，这种情绪只能隐于平静的表面之下。也只好回宿舍写篇博客排解一番了。\n这份文档到底还要再改几遍？ 场景：学校项目组和外包公司\n事件：我更改完一版 word 文档，微信发送给老师审核；老师通过审阅和批注改完重新发送给我。 我改好后再发送一版给外包公司。\n结果：文件夹堆满了数个版本的文件，老师群里突然问这里不是之前XX版本改过了吗？就只能一版一版去翻，最后大概率死无对证。 微信电脑端记录也残缺不全，只能在手机上慢慢翻找。\n办法：\n使用在线文档，最好是 google docs（又不是谁都能上 google，再说大家也没有使用共享文档的意愿） 用个全平台同步的沟通工具，哪怕邮箱吧（Telegram？好吧，又是访问限制。邮件？“好麻烦啊”） 代码确实看不懂 场景：办公室\n事件：经常需要看别人的代码，又经常给别人分享代码。\n结果：看到别人的纯代码，自己得先研究个一个小时才能看出个所以然来； 自己给别人分享代码，就算心里写了注释，感觉也不踏实。\n办法：\n没啥说的，求求大家写一点注释吧（肯定比没有注释要舒服） 我有时自己还要额外写一个 HTML 文档来解释 其实以后可以顺便也更新到博客上面 连网页都能松鼠症 事件：自己在查资料，往往遇到一些比较好的网页，但内容想之后有时间再看，于是轻松一点收到 OneTab 里面，这下可好：\n唉，不知不觉就又堆积了3百多个页面。“稍后阅读”可真是不能过度使用了。 不多说了，清理去了。\n","id":56,"section":"posts","summary":"刚刚以老年人的配速跑完了2km，不过实在太喜欢这种在雨后大口呼吸空气的感觉了。 清新的空气确实能让人头脑更加清醒，整个身体也更有活力。 也不知是","tags":["记录"],"title":"一点碎碎念","uri":"https://www.xzywisdili.com/2019/09/2019-09-12-thoughts/","year":"2019"},{"content":"今天本来想总结一下这阵子使用 R 代码画地图的一些心得，但写到半夜 12 点半才写到一半，无奈今天无法更新。 眼睛打架之时开始翻看 eBooksPlan 的 Telegram群组。\neBooksPlan 是我一直很欣赏的一个社群，因为群主会稳定持续地分享很多优秀内容，但群组内一直有不讨论政治议题的群规。 今日，群里几个人又争论起了政治话题，似乎是有关“民主和言论自由”云云。管理员出面打断了讨论并封禁了两名带头的辩论者。\n我对于这样的处理结果不置可否，但却也心生疑惑，于是只是凭借好奇询问：\n我知道读书群不允许讨论政治的群规。但我们如何单纯地讨论读书，而绝不涉及政治。拿“每周一书”举例，这周的选书是《过剩之地：美式富足与贫穷悖论》。我个人对这本书特别感兴趣，但是否意味着我们无法在群里探讨书中可能提及的绝大多数主题？抑或是说我们只能讨论文本层面的内容？\n就抛开那本书而言，很多生活的方方面面最后都不可避免地涉及到了经济、政治。 我们可以显而易见地发现，遵守群规的众人只能无休止地使用贴纸，争论 kindle、iPad 和纸质书阅读的利弊，或者求某本书的电子书资源。 这样的群倒也食之无味。而针对我的问题，管理员是这样回复的：\n討論主題，問題若僵持在辯，討論什麼…？ 你、我都知道，許多人對於議題來講，只一心求辯贏。誰能控制自己，好好說理…？ 如果各位都能夠控制自身的情緒，在論述之上，誰會惹得最後惡言相向？ 看過太多群組討論到最後，不是一句傻B，就是一句爹爹來著的。\n我对于群主的回复颇有几分认同，也理解了群规的道理所在，但心里感觉有一点遗憾。 我们从小的政治课都只是灌输政治观点，却从来没有培育讨论议题的能力，甚至似乎都没有被传授如何开放性地谈论问题。 网络空间里层出不穷的“辩手”一言不合就无赖式撒泼，或是脏话频出，确实让其他所有人的讨论兴致荡然无存。\n","id":57,"section":"posts","summary":"今天本来想总结一下这阵子使用 R 代码画地图的一些心得，但写到半夜 12 点半才写到一半，无奈今天无法更新。 眼睛打架之时开始翻看 eBooksPlan 的 Telegram群","tags":["记录"],"title":"再也敏感不起来","uri":"https://www.xzywisdili.com/2019/09/2019-09-11-thoughts/","year":"2019"},{"content":"“开学”这个词像一层乌云笼罩了我至少十余载的青春岁月，9月1日的到来也宣示着暑假的结束，对于学生来说自然苦不堪言。 但今年我却没有太大感觉，一是毕竟已经读研，暑假也在赶工干活，开学只是偶尔去上几次专业课，其余并无不同； 二是自己也不可能给自己放假，胆敢随意抽出一周时间去外地旅游散心，甚至回家看看。\n这段时间真正聊以慰藉的就是高强度关注威尼斯电影节了。作为一个DC粉，看到期待已久的电影《小丑》的各种资讯一点点放出，更是对我心理的一次次撩拨。 昨天看到消息，《小丑》拿下了这次的金狮奖，自然非常欣喜，DC终于实现了翻身。 但是仔细一想，10月份这部电影就会在北美、港澳台，乃至越南上映，而无缘中国大陆，说不定还要眼巴巴地等待两个月的资源，实在又苦不堪言。 除此之外，《婚姻故事》《我控诉》《自助洗衣店》《星际探索》也加入了我的想看片单，也算是对自己耐心的一次极大考验了。\n先总结总结上一周完成了什么工作吧。\n继续和小程序、服务器、公司和项目组两方“扯头发”式扯头发，期间还经历了一次服务器的宕机，不过还好修复了； 初步完成了 HPV 的地区分异的项目研究，鲍博说要更替一下城市的相关指标，这一更又要此去经年了； 学习了一半 python 里面的 matplotlib； 继续学习 SAS 里面对于 sql 语句的使用方法； 读完了来自日本的工具书《为什么精英都是 Excel 控》，里面有一处着实好笑：说他们为了使用\u0026quot;F2\u0026quot;键检查公式不误触\u0026quot;F1\u0026quot;键弹出帮助浪费时间，把键盘上面的\u0026quot;F1\u0026quot;键都拔掉了，幸好我的电脑\u0026quot;F1\u0026quot;都是截图的快捷键； 终于看完了奈斯博的《焦渴》，也是见证了哈利再次出生入死，搏命拿下了吸血鬼罪犯； 购置了一把沙发椅，不贵也就130块，但是真还挺舒服的； 没有来由地去和几个高中同学喝了一顿酒，被曹总灌得人仰马翻； 我希望下周开始践行的几项，首先要在12点左右上床睡眠，戒掉含糖饮料（咖啡我是戒不掉了），尽量喝水，抽时间多锻炼身体。 接下来，尽量开始今早着手进行meta分析的相关分析，不要再继续拖延下去了，得加快学习节奏了。\n","id":58,"section":"posts","summary":"“开学”这个词像一层乌云笼罩了我至少十余载的青春岁月，9月1日的到来也宣示着暑假的结束，对于学生来说自然苦不堪言。 但今年我却没有太大感觉，一","tags":["记录","周记"],"title":"这一周（9.2-9.8）","uri":"https://www.xzywisdili.com/2019/09/2019-09-09-weeknote/","year":"2019"},{"content":"今年已经步入了9月，也正式宣告这个“伪暑假”的落幕，新的学期就要开始了。之所以说是“伪暑假”，着实是因为自己没给自己放假，整整一个暑假都在实习。不过整个实习安排还好，从早上9点工作到下午5点半，倒不算特别辛苦，但是每天大部分时间也算投资在工作上面了。\n工作纪实 回顾这一周，似乎也没有做太多事情：\n过了一遍 python 的 numpy 和 pandas 包 和科研项目组里的服务器斗智斗勇，更新了一版小程序 整理了中国气象和污染物的数据 钻研出了绘制正规中国热力地图的画法 剩下时间都用来跑 SAS 代码了（这个 HPV 的项目已经让我整整心累了 3 周） 读了 1 本书，看了若干美剧和电影 庆祝曹老板考上清华，又欢迎马老板从东京归来 尝了一次味千家的葱油拌面（我感觉真的挺好吃的，里面的蛤挺鲜） 入手了 Sony 的 WH1000 XM3，第一次体会到了降噪耳机的强大之处 因为公司就在学校宿舍旁边，早上和晚上也就是骑车子往返，虽说方便，但倒是少了通勤时候安安静静看书的时间。这直接导致我这一周的第二本书拖拖拉拉，还没有读完。可能每天也只有在运行 SAS 代码和睡觉前能瞥上两眼，仔细想想这样的自己也是有点好笑。\n审视与敲打 实话实说，每天去实习之前，我总要花费不少的时间浏览端传媒，少数派 和 自己订阅的 今日热榜。这些井喷式的新闻和八卦究竟对我们是否有所裨益这个问题我没有想明白，但是我知道的是我特别特别喜欢看，一天不看心里就会发痒。这样网站的设计者千方百计地延长我们的在线时间，我也不止一次再各处看到有人提倡“远离一切瀑布流的社交媒体”。虽然心里认同这样的理念，但我却从没有一次真正戒除过。我决心，至少完成一天额定的工作之后，再浏览各类咨询。说不定累了之后就不想看了。\n在工作中比较困惑的地方在于，如何进行文件的存档管理。每天由于不停地重新跑程序，数据更新迭代也比较快。而数据又经常通过word文档和excel表格进行上交和分享。众所周知，git 是不适用于传统的word和excel的，因此就需要使用比较传统的方法：在文件名上标注作者、日期和版本号。这样下来，本地就会堆积不少的各种版本文件，实在令人难以处理。下周我要花一定的时间，哪怕抽出半天的时间好好检查和整理一下现在的文件。还要整理扔在有道云里面的各种来源的笔记。\n另一点令我不满的是，现在的生活可以说几乎是 100% 的“坐立生活”。白天久坐一天之后，到了晚上也没有精力去运动，也是坐在宿舍里，这样导致我自己更没有精神。这样长久会形成一种恶性循环，加剧自己的腰背疼痛的问题。因此，我在考虑要不要购置一把更舒服的椅子或者靠垫，然后抽出时间开始运动。白天坐着不动一段时间之后一定要起身一段时间，舒展舒展。\n新学期即将开学，不知道还能实习多久，不过珍惜每一天。\n","id":59,"section":"posts","summary":"今年已经步入了9月，也正式宣告这个“伪暑假”的落幕，新的学期就要开始了。之所以说是“伪暑假”，着实是因为自己没给自己放假，整整一个暑假都在实","tags":["记录","周记"],"title":"一周总结（8.26-9.1）","uri":"https://www.xzywisdili.com/2019/09/2019-09-02-weeknote/","year":"2019"},{"content":"很早就读过埃勒里·奎因的悲剧系列，始终以为该系列仅 X, Y, Z 三本。 没想到偶然得知还有一本《哲瑞·雷恩的最后一案》，果断花了一天时间读完。 合上书时，感受到了无比的震撼，心情难以平静。\n结合起前三本，突然明白，埃勒里奎因兄弟，是想在侦探小说的领域，完成一个可以媲美莎士比亚四大悲剧文坛地位的侦探系列。 而这个系列的主角，就是哲瑞·雷恩。\n「从人们不再寻找圣杯之后，我所从事的航院便是这地球上最活力洋溢的一种。今天早晨，我六点半起床，现在吃早饭前习惯性地游两英里泳，再坐上早餐桌满足我高涨的食欲； 接着，我试戴了奎西手制的新假发，那是昨天完工的，奎西自认为是得意之作；然后我和我的导演科罗波特金、我的舞台设计师佛瑞茨联络，再一封封享受我受到的大量信函； 最后，我进入莎士比亚所在的时代，徜徉在那神奇而辉煌的古老岁月中……就这么一个平凡的日子里，你也觉得这样是很美好的一天吧？」\n在《X的悲剧》初登场的时候，哲瑞雷恩身为莎翁戏剧的演员，举手投足之间都是一个标准的老绅士。 而在探案过程中，他更像一块温润的玉石，不会咄咄逼人，也不会滔滔不绝地卖弄，更没有神经质气质。 在书中的有些场景，你甚至不会察觉到他的存在。但到每本书的结尾，他总会带来一场逻辑和理性的推理风暴，完美地折服所有在场的人物和书前的读者。\n平实的语言，铺开所有线索，最后用严谨完整的逻辑收尾，更是让《X的悲剧》和《Y的悲剧》两本达到了推理小说天花板的境界。 这是我对这一系列的原有印象，直到我看到了这一本《哲瑞·雷恩的最后一案》。\n此书没有令人眼前一亮的诡计和手法，关键的密码线索和闹钟线索也只是平平，但在最后十页的结尾中，系列剧情来了一次彻底的升华。 哲瑞·雷恩这位洞察世事、秉持正义的智者在面临理想和显示冲突的时候，做出了他的选择，也终结了自己的生命。 在推理之外对人性和道德的探讨，不仅赚足了读者的眼泪，也成功将这个系拔高为毋庸置疑的神作。哲瑞·雷恩也成了我最喜爱的侦探形象。\n下一本读什么好呢？\n","id":60,"section":"posts","summary":"很早就读过埃勒里·奎因的悲剧系列，始终以为该系列仅 X, Y, Z 三本。 没想到偶然得知还有一本《哲瑞·雷恩的最后一案》，果断花了一天时间读完。 合上书时","tags":["阅读","推理"],"title":"哲瑞·雷恩：悲剧系列终章","uri":"https://www.xzywisdili.com/2019/04/2019-04-22-drurylane/","year":"2019"},{"content":" 本文是流行病学学习笔记的第一篇，上课时间为 2019 年 2 月 25 日。\n基本原理 队列（cohort）在流行病学中指有共同经历或有共同状态的一群人。 队列研究（Cohort study）指选定暴露于及未暴露于某因素的两组人群，随访观察一定的时间， 比较两组人群某种时间的结局（一般指疾病的发病率或死亡率），从而判断该暴露因素与发病或死亡有无关联及关联大小的一种观察性研究方法。\n病因和危险因素研究是流行病学研究的重要任务，病因研究的逻辑顺序应该是先有病因存在，然后又疾病发生。 所以队列研究的特点是由因到果。\n危险因素（risk factor）指能隐去某特定不良结局，或使其发生的概率增加的因子，包括个人行为等等。 危险因素的反面称为保护因素，两者统称决定因素。而这个概念在流行病学中逐渐淡化，将以前所谓的「危险因素」等概念统称为「暴露」。\n暴露是研究对象接触过某种待研究的物质或具有某种研究的特征或行为，是本研究需要探讨的因素。\n队列有固定队列（fixed cohort）和动态队列（dynamic cohort）之分。\n动态队列由于开始时间的不统一和失访等退出队列的情况存在，所以存在不同样本对于队列的贡献无法统一衡量。 因此出现了一种新的衡量方法：「人年」，即用样本的数量乘以该样本进入队列的时间。\n队列研究的历史 从 Jone Snowl 的伦敦霍乱调查开始，奠定了队列研究的雏形。 最早的队列研究一词出现是美国流行病学家 1935 年提出，但是该概念与今天含义不同。 现代队列研究作为一种流行病学研究的方法，得到广泛的应用，相关论文数量变化十年翻一番，著名的研究包括：\ndoll 关于医生吸烟与肺癌的队列研究 美国佛明汉的心脏病研究 原子弹爆炸的幸存者研究 现在，「超大型队列」的概念突破了传统队列研究，摒弃了一因一果的概念。 世界最大的 50 到 55 万人超大型队列，总共有 3 个。\n特点 队列研究有以下几个特点：\n观察性研究 设立对照组 由因及果，时序合理 检验暴露于疾病的因果联系科学性强 用途 队列研究可以用来：\n检验病因假设：验证某种暴露因素对某种疾病发病率或死亡率的影响，也可以同时观察某种暴露因素对人群健康的系统影响； 描述疾病的自然史：疾病的自然发展过程，包括疾病的起病（病理发生期）、潜伏期（隐伏期）、临床前期、临床期到结局的全过程； 评价预防措施效果：验证某种行为或习惯变化产生的健康影响，如戒烟或使用大量蔬菜对肠癌的影响； 药物的上市后监测：对于通过临床三期的临床试验的治疗药物，上市后，监测人群用药的安全性、有效性。 类型 队列研究根据研究方法可以分为：前瞻性队列研究、历史性队列研究、双向性队列研究。 历史性队列回顾性地收集已有的历史资料，而前瞻性队列从此时开始收集资料，双向性则是两者的结合，即向后收集历史资料，也向前收集未知资料。\n前瞻性队列研究 研究对象的分组是根据目前的暴露情况，研究的结局需要随访观察一段时间才能得到。这是队列言情剧的基本形式。 该类的最大优点是可以获取相对真实可靠的资料，但是如果观察大量人群花费太大。如果疾病的潜伏期很长，则需要观察的时间很长。\n前瞻性研究需要研究的检验假设明确，疾病的率至少不低于 5%，暴露因素明确，且能有效测量；结局明确，可以可靠方式确定；有足够的研究人群； 队列能够有效随访；有足够的物质、人力和资金的保证。\n历史性队列研究 研究工作时现在开始的，研究对象的分组是过去某个时间，研究的结局在研究开始时已经发生，暴露到结局的方向是前瞻性的。\n历史性队列研究的特点包括节省时间、人力和物力，出结果快，因而适宜于长诱导期和长潜伏期的疾病； 常用于具有特殊暴露的职业人群的研究；研究常常缺乏影响暴露与疾病关系的混杂因素的资料，以至影响暴露组与未暴露组的可比性。\n历史性队列研究的条件除了要具备前瞻性队列条件外，还应具备足够数量的、完整可靠的、在过去时段的研究对象的暴露和结局的历史记录。\n双向性队列研究 ambispective cohort study 在历史性队列研究之后，继续进行前瞻性队列研究叫做双向性队列研究（ambispective cohort study）。 这种研究具有上述两种研究的优点，在一定程度上弥补了它们的不足。\n双向性研究需要基本具备历史性队列研究条件，如果暴露到现在的观察时间不足，且有条件继续进行观察，可以采用双向性队列。\n队列研究的实施 确定设计方法的原则 根据上述各种研究类型的条件选择合适的研究类型，设计研究方法。\n确定暴露因素 暴露因素通常是在描述性研究或病例对照研究的基础上确定的。尽量对暴露因素进行定量处理，除了暴露剂量水平外， 还应考虑暴露的时间长度，以及暴露是否连续。除了要确定主要的暴露因素外，也应同时手机其它次要的暴露因素资料及一般特征资料。\n对暴露的测量可以是定性或定量测量，具体测量方法一般包括访谈、实验室检查和查阅记录。随着科技的进步，暴露水平的测量已经从 宏观群体水平发展到个体水平，分子水平。\n确定结局 结局不仅限于发病，还有死亡和各种化验指标，如血清抗体的滴度，血脂、血糖等等。结局事件指研究对象个体而言，与观察期的终点不是一个概念。 判断结局的标准应当尽量采用国际或国内统一标准。\n确定研究现场与研究人群 尽量选择领导重视、群众支持，有足够符合条件的研究对象，医疗条件好，交通便利，发病率较高，有代表性的研究现场。 研究人群从目标人群中抽出的具有代表性的人，未患所研究疾病的人，并分为暴露人群和非暴露人群。\n暴露人群主要类型有：特殊暴露人群、一般人群、有组织的人群团体和志愿者。 其中特殊暴露人群指对某因素有高的暴露水平的人群。如果暴露因素与疾病有关，则高度暴露的人群中疾病的发病率或死亡率有可能高于其他人群。 某些职业中存在特殊暴露因素，可以作为特殊暴露人群。选择特殊暴露人群做队列研究时，常使用历史性队列研究。\n对照人群为排除未暴露于研究的因素外，其他各种因素或人群特征尽可能与暴露人群相同。 对照人群类型包括：内对照、外对照、总人口对照和多重对照。\n内对照是在同一研究人群中，采用没有暴露或暴露水平最低的人员作为对照。选择内对照，对照人群与暴露人群的可比性好。但研究环境或职业暴露时难以实施。 外对照是职业人群或特殊暴露人群常需要在该人群之外特设对照组，可以避免“污染”，但缺点是可比性受到影响，且工作量加大。\n一般人群对照，即不设立特殊对照，暴露人群发病率与一般人群进行比较。这样可以节省大量经费和时间，但资料不够全面，且可比性较差。\n多种对照就是对上述对照方法的综合，进行多重比较。\n确定样本大小 队列研究的样本量大小主要取决于 4 个参数：\n一般人群中研究疾病的发病率 暴露人群的发病率：与一般人群发病率差距越大，所需观察人数越少 显著性水平 把握度 样本大小可以通过以下公式计算：\n$$ N = \\frac { \\left( Z _ { a } \\times \\sqrt { 2 \\overline { P } ( 1 - \\overline { P } ) } + Z _ { \\beta } \\times \\sqrt { P _ { 1 } \\left( 1 - P _ { 1 } \\right) + P _ { 0 } \\left( 1 - P _ { 0 } \\right) } \\right) ^ { 2 } } { \\left( P _ { 1 } - P _ { 0 } \\right) ^ { 2 } } $$\n其中：\nP 代表两个发病率的平均值 P_1 代表暴露组预期发病率 P_0 代表对照组预期发病率 资料的收集 需要收集暴露、结局、可能产生混杂的因素等种类的资料。前期对队列进行基线调查的资料，以及之后随访观察的资料收集。 收集资料按如下四格表进行整理：\n病例 非病例 合计 暴露组 a b a+b = n1 非暴露组 c d c+d = n0 合计 a+c=m1 b+d=m0 a+b+c+d=t 暴露组发病率为 a/n1，非暴露组发病率为 c/n0。\n资料的整理和分析 率的计算 累积发病率（cumulative incidence, CI）：当观察人口比较稳定时，不论观察时间长短， 以开始观察时的人口数为分母，整个观察期内发病人数为分子，得到累积发病率。\n$$ 累积发病率 = \\frac{观察期间发病人数}{观察队列人数} $$\n发病密度（incidence density）适用于观察时间长，人口不稳定，存在失访的情况。计算方法为：\n$$ ID = \\frac{观察期内发病人数}{观察人时} $$\n其中观察人时即观察人数乘观察时间，最常用的时间单位是年，即以人年为单位计算发病率或死亡率。\n标化死亡比（standardized mortality ratio, SMR）适用于结局时间的发病率低的情况，计算方法是：\n$$ SMR = \\frac{研究人群观察发病（死亡）数}{标准人口预期发病（死亡）数} $$\nSMR 代表被研究人群发生某病的危险性是标准人群的多少倍。\n暴露于疾病关联的指标 病例 非病例 合计 暴露组 a b a+b = n1 非暴露组 c d c+d = n0 合计 a+c=m1 b+d=m0 a+b+c+d=t 再次借用上面提到的四格表形式。 队列研究的效应估计包括相对危险度、归因危险度、归因危险度百分比、人群归因危险度、人群归因危险度百分比等。\n相对危险度 relative risk, RR 相对危险度反映暴露与发病或死亡关联强度的指标，也叫危险比（risk ratio, RR），其本质是率比，为暴露组的率与非暴露组的率之比。\n$$ RR = \\frac{I_e}{I_0} = \\frac{a/n_1}{c/n_0} $$\nRR 值所代表的关联强度如下表：\nRR 关联强度 0.9~1.0 或 1.0~1.1 无 0.7~0.8 或 1.2~1.4 弱 0.4~0.6 或 1.5~2.9 中 0.1~0.3 或 3.0~9.9 强 ＜0.1 或 10~ 很强 归因危险度 attributable risk, AR 又叫特异危险度，本质为差率，即暴露组的率与非暴露组的率之差，说明由于暴露增加或减少的率的大小。\n$$ AR = I_e - I_0 = \\frac{a}{n_1} - \\frac{c}{n_0} $$\nRR 说明的是暴露对于个体增加发生危险的倍数，而 AR 是对人群来说，暴露增加的超额风险的比例。\n归因危险度百分比 AR% $$ AR% = \\frac{I_e - I_0}{I_e} \\times 100% $$\n归因危险度百分比代表暴露人群中的发病或死亡归因于暴露的部分占全部发病或死亡的百分比。\n人群归因危险度 population attributable risk, PAR 计算方法为总人群率减去非暴露组率：\n$$ PAR = I_t - I_0 $$\n人群归因危险度代表暴露人群与一般人群比较，所增加的疾病发生率的大小。 PAR 值越大，暴露因素消除后减少的疾病数量越多。\n队列研究相关偏倚 选择偏倚 selection bias 由于选择的研究对象有人不能参加，可能都为志愿者，早期病人早研究开始时未能发现等等原因，都可能会造成选择偏倚。 在队列研究的过程中，不可避免会出现失访，因此而造成的偏倚叫做失访偏倚。失访所产生的偏倚大小主要取决于失访率的大小和失访者的特征。\n选择偏倚可以通过以下措施控制：严格按照规定的标准选择研究对象；查明愿意加入和不愿意加入研究人员差异；尽可能提高研究对象的依从性； 对于失访可能的影响应当做进一步估计，从各种途径了解失访者的最后结局，与被随访到的人群的阶矩进行比较，以推测失访的影响。\n信息偏倚 information bias 主要为错分偏倚，包括暴露错分和疾病错分以及暴露于疾病的联合错分。错分主要原因可能是检验技术出现问题，诊断标准定义不明确或掌握不当等。 若这种错分偏倚以同样的程度发生于观察的各组，则结果可能不会对各组之间的相对关系产生太大影响，但会低估相对危险度，这种情况叫做非特异性错分。\n控制信息偏倚，可以通过提高设计水平和调查质量，做好质量控制；明确各项标准，严格按照规定执行，定期抽取一定比例的样本复查。\n混杂偏倚 confounding bias 混杂、混杂因素和混杂偏倚的概念和控制方法都与病例对照研究相似。\n优缺点 队列研究的优点包括，可以直接获得暴露组与非暴露组的发病率或死亡率。 且由于原因发生在前，结局发生在后，检验病因假说的能力比较强。这样就有助于了解疾病的自然史，并可以获得暴露于疾病结局的关系。 队列研究样本量大，结果稳定，收集的资料完整可靠，不存在回忆偏倚。\n缺点则是不适于发病率很低的疾病的病因研究，由于长期的研究与随访，因为死亡、退出、搬迁等造成的失访难以避免。 且研究耗费时间、人力和花费。随着时间推移，未知的变量引入人群可能导致结局收到影响。\n作业 找一个队列研究的文献，对它的暴露和结局，从暴露和结局测量有效性的角度进行评述。\n","id":61,"section":"posts","summary":"本文是流行病学学习笔记的第一篇，上课时间为 2019 年 2 月 25 日。 基本原理 队列（cohort）在流行病学中指有共同经历或有共同状态的一群人。 队列研究（","tags":["流行病学","笔记"],"title":"流行病学学习笔记（二）：队列研究（cohort study）","uri":"https://www.xzywisdili.com/2019/02/2019-02-28-cohortstudy/","year":"2019"},{"content":"越来越多的速成教程出现在我们的视野中。高中时期，贴吧就有不少关于“如何在一个月内高考数学提高XX分”这样的内容。 而之后，像这样的内容越来越多，无论是程序相关，考研，摄影，视频后期，音乐制作等等领域，都出现了诸如此类的速成教程。 似乎跟着这些内容学完，就能在短时间内摇身一变，成功升级为这一领域的大神。\n我以前也经常被这样的标题所迷惑，也在网盘或收藏夹存储了大量的这类教程。最近我开始反思这一类内容的真正用处。 这类内容一般有两类。第一类是真的可以在很短时间内速成学会，比如 Markdown 的语法，使用软件剪辑视频的操作， Office 软件的一些基础用法等等。但是这种技能不可能成为一个人的核心技能，你可以学会，别人也可以在短时间内学会。 如果只衡量一项技能的时间成本，这类技能等于是零门槛的。这不代表它们是没有价值的，但我们不应该把大量时间和精力花在搜寻和学习它们上，也不能因为自己会这类技能就沾沾自喜，自我满足。\n另一类则是以这样的标题作为吸睛的鸡血，引诱读者而打开链接，而实际上仍需要大量的时间精进。比如我最近学习的 Python Web 编程内容，24 节课，每节课 2 小时，按 2 倍速播放需要 1 小时，再加上课前预习内容和课后作业，每节课也得至少 1 个半小时，这样总共算下来需要 60 个小时，就算每周能抽出 6 个小时时间学习，也需要 3 个月时间才能学完（我不相信有人能一天 10 小时高强度学习）。 这样也才是刚刚入门，还需要大量时间去练习，才能够算得上掌握了一项新的技能。别的领域我想也是如此，摄影、绘画大神才出彩前付出大量的时间，考研成功的人也一定是在所有参与者中相对努力的那批人。这才是一个人的核心竞争力，也是不可替代的。\n经过在医院和疾控的实习，我也明白去分类工作业务内容，识别哪些工作业务是真正需要去学习的。像贴化验单，做心电图，按照模板补充资料的内容，简单讲一遍很多人都能上手。会做这些工作什么也说明不了。我们需要放下浮躁的心，抛弃那些短时间就能速成大神的错误思想，真正思考如何去培养自己的核心技能。\n","id":62,"section":"posts","summary":"越来越多的速成教程出现在我们的视野中。高中时期，贴吧就有不少关于“如何在一个月内高考数学提高XX分”这样的内容。 而之后，像这样的内容越来越多","tags":["随想","学习"],"title":"欲速则不达","uri":"https://www.xzywisdili.com/2019/02/2019-02-23-speededucation/","year":"2019"},{"content":" 本文是流行病学学习笔记的第一篇，上课时间为 2019 年 2 月 20 日。\n概述 概念 描述性研究是利用常规检测记录或通过专门调查获得数据资料，按不同地区、不同时间及不同人群特征分组， 描述人群中疾病或健康状态分布状况或暴露因素的分布情况，在此基础上进行比较分析，获得疾病三间分布特征， 进而提出病因假设和线索。\n种类 描述性研究主要有以下 7 种类型：\n现况研究 病例报告 病例系列分析 个案研究 历史（常规）资料分析 随访研究 生态学研究 现况研究和生态学研究会在之后重点介绍。\n病例报告（case report）是针对临床实践中某种函件疾病的单个病例或少数病例，重点关注个案特征，探究疾病背后产生的原因， 无需描述食物的集中趋势或离散程度，属于定性研究的范畴。\n病例系列分析（case series analsis）是对一组病例相同疾病的临床资料进行整理、统计、分析、总结并得出结论， 一般分析临床表现特征，评价预防、治疗措施的效果，为进一步研究提供线索，并能显示某些疾病的自然进程的规律性。\n个案研究（case study）运用流行病学的原理和方法，到疾病现场对新发病例的接触史、家属及周围人群的发病或健康状况 以及与发病有关的环境因素进行调查，以达到查明所研究病例的发病原因和条件，防止再发生类似疾病，控制疫情扩散及消灭疫源地的目的。个案研究的对象常为传染病病人。\n历史资料分析通过回顾性调查，提取和利用历史既有资料，研究疾病的三间分布、疾病危险因素和评价疾病防治措施效果， 包括相关机构的日常工作记录、登记、各类日常报告、统计表格、疾病记录档案等等。\n随访研究可分为纵向研究（longitudinal study）和前瞻性研究（prospective study）。 通过定期随访，观察疾病、健康状况或某卫生事件在一个固定人群中随着时间推移的动态变化情况。 随访研究常用于疾病自然史的研究，为该疾病的病因研究提供线索，提出或验证某些病因学假设。\n特点 描述性研究的主要特点是，以观察为主要研究手段，不对研究向采取任何干预措施，仅通过观察、收集和分析相关数据， 分析和总结研究对象或事件的特点。 暴露因素不是随机分配，不设立对照组，暴露和结局的时序也无法确定，对病因推断存在一定的局限。\n用途 描述性研究描述疾病或某种健康状况的分布及发生发展的规律， 包括疾病危险因素的发现、高危人群的确定、疾病患者的早发现、早诊断和早治疗、人群疾病防治策略与措施的提出、卫生政策和医疗卫生计划的制定。\n另外描述性研究可以通过比较三间分布的差异，获得病因线索，提出病因假设。\n现况研究 概述 概念 现况研究是对一个特定时点或时期内特定范围的人群中，对某种疾病或健康状况以及相关因素进行调查， 提供建立病因假设的依据。 调查包括描述疾病或健康状况以及相关因素在调查人群中的分布， 或按照不同因素的暴露特征或疾病状态进行比较分析。\n现况研究根据观察时间为横断面研究（cross-sectional study），根据分析指标为患病率研究（prevalence study）。\n研究目的与应用范围 掌握目标群体中疾病或健康状况的分布（患病率） 提供疾病病因研究的线索 确定高危人群 评价疾病监测、预防接种等防治措施的效果 研究特点与研究类型 现况研究的研究特点：\n开始时不设立对照组 研究的特定时间或时期 在确定因果联系时受到限制 不能确定时间顺序 研究对象更多的是存活期长的病人 对不会发生改变的暴露因素，可以提示因果联系 定期重复进行可以获得发病率资料 现况研究有两种类型：普查（census）和抽样调查（sampling survey）。\n普查（census）面向特定时点或时期、特定范围内的全部人群， 优点包括不存在抽样误差，可以同时调查多种疾病或健康状况，能发现目标人群中的全部病例； 缺点是不适用于患病率低的疾病，工作量大，存在漏查，调查工作人员水平参差不齐，耗时耗力。\n抽样调查（sampling survey）是通过随机抽样的方法，对特定时点、特定范围内人群的一个代表性样本进行调查，通过对样本中的研究对象的调查研究，来推论其所在总体情况。 抽样调查省时省力，工作比较细致；但缺点包括设计、实施和资料分析比普查复杂，资料的重复或遗漏不易发现， 不适用于变异过大的研究对象或因素，不适用于需要普查普治的疾病，不适用于患病率太低的疾病。\n研究设计与实施 研究设计与实施分为以下步骤:\n明确调查目的与类型 确定研究对象 确定样本量和抽样方法 资料的收集、整理和分析 获得结果与结论（三间分布） 确定研究对象 以研究目的为依据 明确规定调查对象的人群分布特征、地域范围及时间点 考虑可行性问题 确定样本量和抽样方法 计算样本量（1）：计数资料\n$$ n = \\frac{t_\\alpha^2 * p * q}{d^2} $$\n各个参数含义：\nn：样本量 p：预期现患率 q：1-p d：容许误差 t_\\alpha：显著性检验统计量 计算样本量（2）：计量资料\n$$ n = \\frac{t^2_\\alpha S^2}{d^2} $$\n各个参数含义：\nn：样本量 s：总体标准差的估计值 d：容许误差 t_\\alpha：显著性检验统计量 抽样方法：\n非随机抽样 随机抽样 单纯随机抽样 系统抽样 分层抽样 整群抽样 多阶段抽样 不同抽样方法的误差大小排序：整群 ≥ 单纯随机 ≥ 系统 ≥ 分层抽样\n质量控制 质量控制方法无非以下几种： 严格遵照抽样方法的要求，确保随机化原则 提高研究对象的依从性，正确选择测量工具和检测方法。 对调查员进行培训，统一标准。做好资料复查、复核工作。选择正确的统计分析方法。\n优缺点 优点：\n来自于人群，有较强的推广意义 同期对照组，结果具有可比性 一次调查可同时观察多种因素 缺点：\n难以确定先因后果的时序关系 不能获得发病率资料 误判潜伏期和临床前期的调查对象，低估研究群体的患病水平 实例 生态学研究 概述 在群体水平研究某种暴露因素与疾病之间的关系，以群体为观察和分析单位， 描述不同人群某因素的暴露状况与疾病的频率，分析该暴露因素与疾病之间的关系。\n研究指标有三类：\n群体本身的属性 (aggregate measures) 环境相关的变量 (environmental measures) 全局属性 (global measures) 用途：\n提供病因线索，产生病因假设 慢性病病因研究、环境因素 评估人群干预措施的效果 估计监测疾病的趋势 类型 生态比较研究 (ecological comparison study) 这种类型较为常用。主要可以分为两种：\n「结局 → 病因」：观察不同人群或地区中某种疾病的分布，根据疾病分布的差异，提出病因假设； 「病因 → 结局」：比较不同暴露水平的人群中疾病的发病率和死亡率，为病因探索提供线索。 举例：PM2.5 和糖尿病的关系\n研究结果发现，糖尿病患病率随大气中 PM2.5 的浓度升高而增加，PM2.5 每升高 10μg/m^3，患病率升高1%； 即使是在EPA安全范围内，这种两小关系依然存在。\n生态趋势研究 (ecological trend study) 生态趋势研究连续观察人群中某因素平均暴露水平的改变与某种疾病的发病率、死亡率变化的关系，了解其变动趋势。 通过比较暴露识别卡变化前后疾病频率的变化情况，来判断某因素与某疾病的联系。\n与生态比较研究的区别是，生态趋势研究注重时间维度上的区别。\n生态趋势研究有几种分析角度：\n个体层面（individual-level） 群体（completely ecologic analysis） 部分（partially ecologic analysis） 优缺点 生态学研究具有以下优点：\n省时省力 对病因未明的疾病提供病因线索 个体暴露剂量无法测量 当研究的暴露因素在一个人群中变异范围很小时，宜采用多个人群比较的生态学研究 适合于对人群干预措施的评价 在疾病监测中，可以估计某种疾病发展的趋势 生态学研究的局限性：\n生态学谬误（ecological fallacy）：由于生态学研究以各个不同情况的个体「集合」而成的群体为观察和分析的单位，以及存在的混杂因素等原因而造成研究结果与真实情况不符 混杂因素难以控制 变量多重共线性问题 难以确定因果关系 ","id":63,"section":"posts","summary":"本文是流行病学学习笔记的第一篇，上课时间为 2019 年 2 月 20 日。 概述 概念 描述性研究是利用常规检测记录或通过专门调查获得数据资料，按不同地区、不同时间","tags":["流行病学","笔记"],"title":"流行病学学习笔记（一）：描述性研究（descriptive study）","uri":"https://www.xzywisdili.com/2019/02/2019-02-21-descriptive-study/","year":"2019"},{"content":"本科生活即将结束了。最后的一周日子里，主要在复习职业病学和精神病学，准备考试，一边又不想落下 SAS 的学习，当然还有其他杂七杂八的任务。在学习的时候，自然会想到一个问题就是：我们怎么知道自己学会了？\n之前看到的比较多的一种说法是「讲出来」，当然也不一定是讲，总之就是学完一章，合上书本，能说或写出这一章的内容、脉络和重点，就代表掌握了。即如果你能够教别人，那么自己也就会了。所谓知乎等平台上常见的「输出」也是此理。\n但是如果自己一个人学习的话，确实难以通过「教」的方式来证明自己学会了没有。最常用的替代方式就是做题了。我学习一个新东西，还是喜欢有习题的书本或资料，能够让我进行一定程度的练习。比如我最近在使用的 SAS 学习资料 《Learning SAS by Example: A Programmer\u0026rsquo;s Guide, Second Edition》，每一章背后有 10 余道习题可供解答。我已经学习到 21 章，并且把我的所有解答过程放在 github 上面了。在复习考试的时候，也是喜欢去寻找和整理题库，比如 精神病学题库 和 职业病学题库 。每次刷完题目都有一种比较踏实的学习感。\n当然，今天在 Telegram 的 Monologueuse: 独言者独言之地 频道看到这样一段话： 「kentzhu： 某个字盯久了就不认识的原因是什么？是因为大脑的罢工。眼睛不断地获取同一信息传递给大脑，导致大脑疲惫而进入低运行状态，直至不再反馈信息。这种情况在心理学上称之为“语义饱和”，类似的还有在臭气冲天的厕所中待久了便觉得没有异味。同时也证明了作业罚抄、题海战术是相对无效的学习方式。 」\n这让我陷入了思考：刷题带来的满足感会不会是虚假的学习感受？\n","id":64,"section":"posts","summary":"\u003cp\u003e本科生活即将结束了。最后的一周日子里，主要在复习职业病学和精神病学，准备考试，一边又不想落下 SAS 的学习，当然还有其他杂七杂八的任务。在学习的时候，自然会想到一个问题就是：我们怎么知道自己学会了？\u003c/p\u003e","tags":["随想","SAS"],"title":"我们怎么知道自己学会了？","uri":"https://www.xzywisdili.com/2018/12/2018-12-8-learnthoughts/","year":"2018"},{"content":"强烈推荐纪录片《独行》。\n1 《独行》是关于中国独立游戏的一部纪录片。这会令人想到另一部纪录片——国外的《独立游戏大电影》。国外的《独立游戏大电影》主要讲述的是国外的三位开发者如何通过游戏来与玩家产生一种「联结」，核心在游戏上。而《独行》则更多聚焦于中国的开发者，他们是怎么样的人，在社会中的生活状态如何。一个明显的区别是，前者你可以牢记三部游戏的名字，而后者你只能知道他们在开发怎么样的游戏，却说不上游戏名字是什么。\n2 不过这并不妨碍《独行》也是一部非常优秀的纪录片。独立游戏本身不是一个赚钱的行业，却有不少人投身于此。这样一群特殊的人，鲜少出现在大家的视野里，我们或许会产生「逃避现实」「nerd」这样一些误解。《独行》通过介绍 5 个独立游戏开发团队，极其克制地讲述他们的日常工作和生活，让我们了解这样一群人，了解这个行业。\n3 在 5 组开发团队中，由于《蜡烛人》这款游戏的大火，来自清华的高鸣也许是最成功的。他眼见当年的同学或科研，或教学，或发财，自己在镜头面前豁达地讲：如果人生的最好状态是幸福而又快乐地度过，那我做游戏是一样的，甚至还更直接一些。\n4 不过最让我印象深刻和敬佩的是陈静。像他这样通晓程序，又对美术有极致追求，执行力又强的大才，一定可以选择这样一条路：进入腾讯网易这样的大公司，早就能在36岁这样的年纪实现财务自由。但可能也因为他那种难以抑制的创造欲，让待在大公司成为一件不太可能的事。不知道他看到朋友那份 3 天做好的略显粗糙的「进击的巨人」同人游戏爆火的时候，是怎样一种心情。他说他想在有生之年完成一款有《塞尔达传说》那样品质的游戏的时候，让我几度动容，甚至泪下。 5 我想不少人都还记得几个细节：穆飞桌上的《新税法下企业纳税筹划》，李远扬和发行商、投资人之间的接洽，在联系到近期国家对于游戏审批的强硬态度。独立游戏本身的魅力是引力，这样的现实社会环境是斥力。他们就在这二力的拉扯之间，努力朝着自己梦想的方向前进，真的非常令人敬佩。\n6 《独行》也好，《独立游戏大电影》也好，都一次次向我们证明了游戏绝对是第九艺术的存在。他们，也是实打实的艺术家。\n《独行》现在可以在爱奇艺看到，不过需要会员。\n","id":65,"section":"posts","summary":"\u003cp\u003e强烈推荐纪录片《独行》。\u003c/p\u003e","tags":["独立游戏","影评"],"title":"2018年，中国独立游戏，和他们","uri":"https://www.xzywisdili.com/2018/09/2018-9-3-indiagames/","year":"2018"},{"content":"现在的大学生不像十几年前，必定是人手一台笔记本电脑（有的人甚至有两台）。只要在自习室稍微留神观察一下 专注学习的同学们，就会发现几乎 100% 的同学的电脑上停留的软件不是微软家的 powerpoint，就是 PDF 阅读器。\n这其实不是个多么令人震惊的结论。现在老师讲课都用 ppt，学生下下来复习，或者把要看的论文存在电脑里用 PDF 看， 实在是太稀松平常的事情了。但我细细想过之后，却觉得奇怪而又费解。\npowerpoint 是否是制作幻灯片的不二选择？ 这个问题肯定有人笑着回答，当然是了：Prezi 制作幻灯片的拉伸旋转机制只能让观看者头晕目眩，而 keynote 软件 又是 mac OSX 系统的专属。在这个 windows 系统还是霸主的时代，大家当然更乐意看到 .pptx 格式的文件。\n毫无疑问，powerpoint 软件提供了大而全的无所不包的功能，花样百出的图片和文本效果，自由自在地拖动编辑模式， 再加上看起来酷炫的动画，一切看起来那么美好。\n但这是我们真正需要的吗？\n纵观老师们上课使用的 ppt，无非用到的就是文本框，文字的加粗和标记，列表层级结构，添加图片和表格这几个内容。 学生们的 Presentation 也是同理。而缺乏设计训练的老师同学们往往在 powerpoint 里放纵自己的随心所欲，让 幻灯片的可读性奇差无比。使用模板带来的问题，就是难以找到一个 100% 和自己内容匹配的模板，而且又会事倍功半。\n当你电脑中安装着这样一个臃肿的大家伙，只常用它提供功能的 1%，而很多按钮可能一辈子都不会碰到的时候，你为什么 还不丢掉它呢？\n另一个流派就是使用 LaTeX，打开 LaTeX 开源小屋搜索 beamer，你会得到：\n这些是非常赏心悦目的幻灯片文件，但关于使用 LaTeX 的痛苦感受我放在下一节里讲。\n所以我觉得目前对于幻灯片的最好解决方案，就是 Markdown 制作转成 HTML 格式的幻灯片。\nMarkdown 最早由 Aaron Swartz 这个天才发明出来（可以看看互联网之子就知道这个人有多伟大），直到现在被无数人广泛使用， 就是因为这个标记语言有着无数优点：\n足够简单，任何人（包括没学过编程的人）可以在 5 分钟之内上手 提供一般人写文档需要的绝大多数功能 不必为版式困扰，专注于内容 配合合适的 css，输出相当漂亮的文档 而谁能想到，是 Rmarkdown 让我们完成了从 Markdown 到幻灯片的完美转型：\n可以看到，就是用左边的简单的 Markdown 语法，再加上每两页之间使用 --- 进行分割， 就能完成像右边这样简洁大方的幻灯片。\n而国外也已经有不少开源软件尝试实现了使用 Markdown 完成幻灯片，比如 Marp， 使用它尝试复刻重制了一下统计课老师的 ppt，加上代码，数学符号和表格也完全 hold 得住：：\n也许会觉得这样的格式有点单调，没关系，给你提供了几种绝对不乱来的主题：\n这样的幻灯片传给任何人都没有问题，因为 HTML 格式的幻灯片只需要用浏览器打开就好了，任何人的电脑上都有 浏览器！\n","id":66,"section":"posts","summary":"\u003cp\u003e现在的大学生不像十几年前，必定是人手一台笔记本电脑（有的人甚至有两台）。只要在自习室稍微留神观察一下\n专注学习的同学们，就会发现几乎 100% 的同学的电脑上停留的软件不是微软家的 powerpoint，就是 PDF 阅读器。\u003c/p\u003e","tags":["Markdown","想法"],"title":"简化我们的工具","uri":"https://www.xzywisdili.com/2018/06/2018-6-4-simplifytools/","year":"2018"},{"content":" 前言 这几天读到一个观点，论文的发表在很久之前抛弃了写在纸上的模式，现在也到了抛弃 PDF 或 Word 格式的时候了。其实一直以来，论文核心呈现的，我们所关注的都是它的内容所在。既然这样，我们为什么不能把论文呈现为 HTML 格式呢？\n首先，现在的论文可能有一些交互式的展示或者动画展示，这些都是传统 PDF 格式文件所不能展示的。其次，互联网时代提倡信息共享和联通，HTML 放在网站上可以在浏览器里轻松点击不被获取。\n当我读到这些内容的时候，一拍大腿，觉得很有道理。所以这次我在博客上完成了我的统计学作业。当然上交的时候还是得灰溜溜的写到我的本子上，但这样至少表达了我的态度和想法。\nProblem #1 绘制下面这种检验方法的 ROC 曲线：\n对于这种检验方法来说，如果取 cut-off 值为 2 时，我们认为 Surgery Result 值为 3，4，5 时检验结果为 positive(+)，取值为 1，2 时检验结果为 negative(-)。\nProblem #2 对这种肝脏疾病检验方法的统计数据如下：\n计算出修正后的灵敏度和特异度：\n$$ sensitivity = \\frac{\\frac{231}{231+32} \\times n_1}{\\frac{231}{231+32} \\times n_1 + \\frac{27}{27 + 54} \\times n_2} = 0.84 $$\n$$ specificity = \\frac{\\frac{54}{27+54} \\times n_2}{\\frac{54}{27+54} \\times n_2 + \\frac{32}{32 + 231} \\times n_1} = 0.74 $$\n如果去掉所有没有经过金标准检测的病例，得到的灵敏度和特异度：\n$$ sensitivity = \\frac{231}{231 + 27} = 0.90 $$\n$$ specificity = \\frac{54}{32 + 54} = 0.63 $$\n可以看出不经过修正会出现明显的偏差。\n","id":67,"section":"posts","summary":"\u003c!-- raw HTML omitted --\u003e\n\u003ch2 id=\"前言\"\u003e前言\u003c/h2\u003e\n\u003cp\u003e这几天读到一个观点，论文的发表在很久之前抛弃了写在纸上的模式，现在也到了抛弃 PDF 或 Word 格式的时候了。其实一直以来，论文核心呈现的，我们所关注的都是它的内容所在。既然这样，我们为什么不能把论文呈现为 HTML 格式呢？\u003c/p\u003e","tags":["统计学","作图"],"title":"卫生统计学第九周作业","uri":"https://www.xzywisdili.com/2018/04/2018-04-25-statistichomework/","year":"2018"},{"content":"既上次写过一篇关于 Rmarkdown 的安利 之后，我在实际使用过程中遇到了一些难题，就是关于中文的问题。如果只使用英文写作的话，完全没有问题；但是一旦使用中文，各级标题，加粗强调和引用都会变得非常奇怪。这也使我暂时搁置下了 Rmarkdown 不用。\n但我多虑了，谢老师早已经提供了近乎完美的解决方案。\n前提准备 安装 Rstudio：地址 安装 Rmarkdown 包：install.packages(\u0026quot;rmarkdown\u0026quot;) 安装 Pandoc：地址 安装 CTeX：在 mac 下推荐 MacTeX 或 MiKTeX 具体细节按下不表。\n安装模板包 install.packages(\u0026quot;rticles\u0026quot;) 这个包中提供了 CTeX 相关模板，可以帮我们解决中文问题。在新建 Rmarkdown 中选择 「From Template」，再在其中选择「CTeX Documents」即可。\n这时候默认是一篇谢老师写好的文章，我们只需要在他的基础上修改就好了，点击 knit 按钮，或者使用快捷键 ctrl + shift + k 看一看生成的 PDF 文档：\n干净，整洁，看起来就像 LaTeX 写出来的！再附加 R 语言代码的强力加持，完美！\n所以我今天也迫不及待使用 Rmarkdown 完成了统计学作业：\n嘿嘿，虽然才学到方差分析，但我还是很骄傲，很膨胀地贴出来了。\n其余资源 关于 Rmarkdown 中的 chunk option Rmarkdown 语法和常用 chunk option 速查记录 使用 Rmarkdown 制作幻灯片的相关内容 使用谢老师完成的写轮眼包制作幻灯片 人们一直希望能够简便地完成一件事，并把它做得漂亮，这也是为什么 Markdown 如此火爆的原因。因为它够简单，够美。我觉得 R 语言也是一种简单而美的编程语言。把 R 和 Markdown 结合在一起，同时能够制作出使用 LaTeX 才能排版出来的文档，实在是令人难掩激动，再写一篇博客狂吹一波。\n","id":68,"section":"posts","summary":"\u003cp\u003e既上次写过一篇关于 \u003ca href=\"http://www.xzywisdili.com/post/2017/12/23/%E8%BD%BB%E6%9D%BE%E6%84%89%E5%BF%AB%E5%9C%B0%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8-rmarkdown/\"\u003eRmarkdown 的安利\u003c/a\u003e 之后，我在实际使用过程中遇到了一些难题，就是关于中文的问题。如果只使用英文写作的话，完全没有问题；但是一旦使用中文，各级标题，加粗强调和引用都会变得非常奇怪。这也使我暂时搁置下了 Rmarkdown 不用。\u003c/p\u003e","tags":["R 语言","Markdown"],"title":"再吹一波 Rmarkdown","uri":"https://www.xzywisdili.com/2018/04/2018-04-11-rmarkdown02/","year":"2018"},{"content":"这篇《R 语言基础入门》是我在别人的博客上看到的，完成时间不会晚于 2011 年。觉得总结得相当不错，于是搬运到我的博客上。需要提到，有些链接可能已经失效。\n引言 什么是 R 语言 R 语言是一个开源的数据分析环境，起初是由数位统计学家建立起来，以更好的进行统计计算和绘图，这篇 wiki 中包含了一些基本情况的介绍。由于 R 可以通过安装扩展包（Packages）而得到增强，所以其功能已经远远不限于统计分析，如果感兴趣的话可以到官方网站了解关于其功能的更多信息。\n至于 R 语言名称的由来则是根据两位主要作者的首字母 (Robert Gentleman and Ross Ihaka)，但过于简短的关键词也造成在搜索引擎中很不容易找到相关的资料。不过这个专门的搜索网站可以帮到你。\n为什么要学习 R 语言 R是免费开源软件： 现在很多学术期刊都对分析软件有版权要求，而免费的分析工具可以使你在这方面不会有什么担心。另一方面，如果学术界出现一种新的数据分析方法，那么要过很长一段时间才会出现在商业软件中。但开源软件的好处就在于，很快就会有人将这种方法编写成扩展包，或者你自己就可以做这件工作。 命令行工作方式： 许多人喜欢类似 SPSS 菜单式的操作，这对于初学者来说很方便入门，但对于数据分析来说，命令行操作会更加的灵活，更容易进行编程和自动化处理。而且命令行操作会更容易耍酷，不是嘛，一般人看到你在狂敲一推代码后得到一个分析结果，对你投来的目光是会不一样的。 小巧而精悍： R 语言的安装包更小，大约不到 40M，相比其它几个大家伙它算是非常小巧精悍了。目前 R 语言非常受到专业人士欢迎，根据对数据挖掘大赛胜出者的调查可以发现，他们用的工具基本上都是 R 语言。此外，从最近几次 R 语言大会上可以了解到，咨询业、金融业、医药业都在大量的使用 R 语言，包括 google / facebook 的大公司都在用它。因此，学习 R 语言对你的职业发展一定是有帮助的。 R 语言的下载和 GUI 界面 R 语言安装包可以在官方网站下载，windows 版可直接点击这个连接 在 ubuntu 下面安装 R 则更容易，在终端里头运行如下命令即可 sudo apt-get update sudo apt-get install r-base\n此外，学习 R 语言时强烈推荐安装 Rstudio 做为 R 的图形界面，关于 Rstudio 之前的博文有过简单介绍，点这里可能转到它的官方网站。\nR 语言的学习方法 学习 R 并不是一件非常轻松的事情，初学者需要记住的就是：\n亲手键入代码并理解其意义 在笔记里记下一些重点或心得（个人推荐 Evernote） 坚持练习，对手边的数据进行应用分析 理解背景知识，细节很重要。 哪里可以得到参考资料 官方网站 http://cran.csdb.cn/index.html （官方文献集中地） 统计之都论坛 人大经济论坛－R 子论坛 （免费资料也不少） http://library.nu/ 这是网上电子书最多的地方，其中有一个 R 语言专门书柜（也就是一个 shelves） 关于 R 语言的教材小结 笔者在 verycd 上发的一个书单 一个国外著名的 R 语言群博 http://www.r-bloggers.com/ 展示 R 语言的各类绘图 http://addictedtor.free.fr/graphiques/ 本人博客里也有一些关于 R 语言的资料：xccds1977.blogspot.com (需 FQ) 如果有一些简单的入门问题，也可以在推特上 follow me twitter: @xccds 本系列博文的目的 本系列入门的目的是为初学者提供最简洁清晰的资料，以迅速入门。所针对的读者人群是那些正在大学里学习初级统计学的同学。本系列计划包括内容有：基本命令，数据操作；描述统计和绘图；重要的 R 语言函数计算；统计推断和估计；非参数统计方法；方差分析；线性回归和一般线性模型。\n数据导入和描述统计 数据导入 对初学者来讲，面对一片空白的命令行窗口，第一道真正的难关也许就是数据的导入。数据导入有很多途径，例如从网页抓取、公共数据源获得、文本文件导入。为了快速入门，建议初学者采取 R 语言协同 Excel 电子表格的方法。也就是先用较为熟悉的 Excel 读取和整理你要处理的数据，然后 「粘贴」 到 R 中。\n例如我们先从这个地址下载 iris.csv 演示数据，在 Excel 中打开，框选所有的样本然后 「复制」。在 R 语言中输入如下命令：\ndata \u0026lt;- read.table('clipboard',T) Data frame 操作 在数据导入 R 语言后，会以数据框 (data frame) 的形式储存。data frame 是一种 R 的数据格式，可以将它想象成类似统计表格，每一行都代表一个样本点，而每一列则代表了样本的不同属性或特征。初学者需要掌握的基本操作方法就是 data frame 的编辑、抽取和运算。\n尽管建议初学者在 Excel 中就把数据处理好，但有时候还是需要在 R 中对数据进行编辑，下面的命令可以让你有机会修改数据并存入到新的变量 newdata 中：\nnewdata \u0026lt;- edit(data) 另一种情况就是我们可能只关注数据的一部分，例如从原数据中抽取第 20 到 30 号样本的 Sepal.Width 变量数据，因为 Sepal.Width 变量是第 2 个变量，所以此时键入下面的命令即可：\nnewdata \u0026lt;- data[20:30, 2] 如果需要抽取所有数据的 Sepal.Width 变量，那么下面两个命令是等价的：\nnewdata \u0026lt;- data[, 2] newdata \u0026lt;- data$Sepal.Width 第三种情况是需要对数据进行一些运算，例如需要将所有样本的 Sepal.Width 变量都放大 10 倍，我们先将原数据进行一个复制，再用 $ 符号来提取运算对象即可：\nnewdata \u0026lt;- data newdata$Sepal.Width \u0026lt;- newdata$Sepal.Width * 10 描述统计 描述统计是一种从大量数据中压缩提取信息的工具，最常用的就是 summary 命令，运行 summary(data) 得到结果如下：对于数值变量计算了五个分位点和均值，对于分类变量则计算了频数。\n也可以单独计算 Sepal.Width 变量的平均值和标准差：\nmean(data$Sepal.Width) sd(data$Sepal.Width) 计算分类数据 Species 变量的频数表和条形图：\nmean(data$Species) barplot(table(data$Species)) 对于一元数值数据，绘制直方图和箱线图观察其分布是常用的方法：\nhist(data$Sepal.Width) boxplot(data$Sepal.Width) 对于二元数值数据，则可以通过散点图来观察规律\nplot(data$Sepal.Width, Sepal.Length) 常用统计函数运算 在 R 语言中经常会用到函数，例如上节中讲到的求样本统计量就需要均值函数 (mean) 和标准差函数 (sd)。对于二元数值数据还用到协方差 (cov)，对于二元分类数据则可以用交叉联列表函数 (table)。下文讲述在初级统计学中最常用到的三类函数。\n数据汇总函数 data \u0026lt;- iris[, c(4, 5)] 下一步我们想计算不同种类花瓣的平均宽度，可以使用 tapply 函数，在计算前先用 attach 命令将 data 这个数据框解包以方便直接操作其变量，而不需再用 $ 符号。\nattach(data) tapply(X = Petal.Width, Index = Species, FUN = mean) 结果如下：\nsetosa versicolor virginica 0.246 1.326 2.026 和 tapply 类似的还有 sapply 函数，在进一步讲解前初学者还需搞清楚两种数据表现方式，即 stack（堆叠数据）和 unstack（非堆叠数据），上面的 data 就是一个堆叠数据，每一行表示一个样本。而非堆叠数据可以根据 unstack 函数转换而来\ndata.unstack \u0026lt;- unstack(data) head(data.unstack) 你应该明白这二者之间的区别了，如果要对非堆叠数据计算不同种类花瓣的平均宽度，可以利用如下函数。\nsapply(data.unstack, FUN = mean) 结果是一样的，也就是说 tapply 对应于 stack 数据，而 sapply 对应于 unstack 数据。\n概率计算函数 如果给定一种概率分布，通常会有四类计算问题：\n计算其概率密度 density （d） 计算其概率分布 probability（p） 计算其百分位数 quantile （q） 随机数模拟 random （r） 记住上面四类计算对应的英文首字母。\n举例来讲，我们求标准正态分布曲线下小于 1 的面积 p(x\u0026lt;1)，正态分布是 norm，而分布函数是 p，那么使用 pnorm(1) 就得出了结果 0.84；若计算扔 10 次硬币实验中有 3 次正面向上的概率，类似的 dbinom(x=3,size=10,prob=0.5) 得出 0.11。\n抽样函数 我们想从 1 到 10 中随机抽取 5 个数字，那么这样来做：首先产生一个序列，然后用 sample 函数进行无放回抽取。\nx \u0026lt;- 1:10 sample(x, size=5) 有放回抽取则是\nsample(x, size=5, replace=T) sample 函数在建模中经常用来对样本数据进行随机的划分，一部分作为训练数据，另一部分作为检验数据。\n常用的统计推断 通常一个研究项目能够获得的数据是有限的，以有限的样本特征来推断总体特征就称为统计推断。推断又可细分为区间估计和假设检验，二者虽有区别，但却是一枚硬币的两面，之间有着紧密的关联。\n对总体均值进行区间估计 假设我们从总体中抽得一个样本，希望根据样本均值判断总体均值的置信区间，如下例所示：\nx \u0026lt;- rnorm(50, mean=10, sd=5) mean(x)-qt(0.975, 49) * sd(x) / sqrt(50) mean(x)+qt(0.975, 49) * sd(x) / sqrt(50) 也可以直接利用 R 语言内置函数 t.test\nt.test(x, conf.level=0.95) 从如下结果可得 95% 置信区间为 (9.56，12.36)。\nOne Sample t-test data: x t = 15.7301, df = 49, p-value \u0026lt; 2.2e-16 alternative hypothesis: true mean is not equal to 0 95 percent confidence interval: 9.563346 12.364729 sample estimates: mean of x 10.96404 对总体均值进行假设检验 还是以上面的 X 数据作为对象，来检验总体均值是否为 10\nt.test(x, mu=10, alternative='two.sided') t 检验是极为常用的检验方法，除了单样本推断之外，t.test 命令还可以实现两样本推断和配对样本推断。如果要对总体比率或总体方差进行推断，可以使用 prop.test 和 var.test。\n正态分布检验 t 检验的前提条件是总体服从正态分布，因此我们有必要先检验正态性。而且在评价回归模型时，对残差也需要检验正态性。检验正态性的函数是 shapiro.test：\nshapiro.test(x) 结果如下：\nShapiro-Wilk normality test data: x W = 0.9863, p-value = 0.8265 该检验的原假设是服从正态分布，由 P 值为 0.82 可判断不能拒绝总体服从正态的假设。\n非参数检验 如果总体不服从正态分布，那么 t 检验就不再适用，此时我们可以利用非参数方法推断中位数。wilcoxon.test 函数可实现符号秩检验。\nwilcox.test(x, conf.int=T) wilcox.test(x, mu=1) 独立性检验（联列表检验） 卡方分布有一个重要应用就是根据样本数据来检验两个分类变量的独立性，我们以 CO2 数据为例来说明 chisq.test 函数的使用，help(CO2) 可以了解更多信息。\ndata(CO2) chisq.test(table(CO2$Type, CO2$Treatment)) 结果显示 P 值为 0.82，因此可以认为两因子之间独立。在样本较小的情况下，还可以使用 fisher 精确检验，对应的函数是 fisher.test。\n简单线性回归 线性回归可能是数据分析中最为常用的工具了，如果你认为手上的数据存在着线性定量关系，不妨先画个散点图观察一下，然后用线性回归加以分析。下面简单介绍一下如何在 R 中进行线性回归。\n回归建模 我们利用 R 语言中内置的 trees 数据，其中包含了 Volume（体积）、Girth（树围）、Height（树高）这三个变量，我们希望以体积为因变量，树围为自变量进行线性回归。\nplot(Volume~Girth, data=trees, pch=16, col='red') model \u0026lt;- lm(Volume ~ Girth, data=trees) abline(model, lty=2) summary(model) 首先绘制了两变量的散点图，然后用 lm 函数建立线性回归模型，并将回归直线加在原图上，最后用 summary 将模型结果进行了展示，从变量 P 值和 F 统计量可得回归模型是显著的。但截距项不应该为负数，所以也可以用下面方法将截距强制为 0。\nmodel2 \u0026lt;- lm(Volume ~ Girth-1, data=trees) 模型诊断 在模型建立后会利用各种方式来检验模型的正确性，对残差进行分析是常见的方法，下面我们来生成四种用于模型诊断的图形。\npar(mfrow=c(2,2)) plot(model) par(mfrow=c(1,1)) 这里左上图是残差对拟合值作图，整体呈现出一种先下降后下升的模式，显示残差中可能还存在未提炼出来的影响因素。右上图残差 QQ 图，用以观察残差是否符合正态分布。左下图是标准化残差对拟合值，用于判断模型残差是否等方差。右下图是标准化残差对杠杆值，虚线表示的 cooks 距离等高线。我们发现 31 号样本有较大的影响。\n变量变换 因为 31 号样本有着高影响力，为了降低其影响，一种方法就是将变量进行开方变换来改善回归结果，从残差标准误到残差图，各项观察都说明变换是有效的。\nplot(sqrt(Volume) ~ Girth, data=trees, pch=16, col='red') model2 \u0026lt;- lm(sqrt(Volume) ~ Girth, data=trees) abline(model2, lty=2) summary(model2) 模型预测 下面根据上述模型计算预测值以及置信区间，predict 函数可以获得模型的预测值，加入参数可以得到预测区间：\nplot(sqrt(Volume) ~ Girth, data=trees, pch=16, col='red') model2 \u0026lt;- lm(sqrt(Volume) ~ Girth, data=trees) data.pre \u0026lt;- data.frame(predict(model2, interval='prediction')) lines(data.pre$lwr ~ trees$Girth, col='blue', lty=2) lines(data.pre$upr ~ trees$Girth, col='blue', lty=2) 我们还可以将树围和树高都加入到模型中去，进行多元回归。如果要考虑的变量很多，可以用 step 函数进行变量筛选，它是以 AIC 作为评价指标来判断一个变量是否应该加入模型，建议使用这种自动判断函数时要谨慎。对于嵌套模型，还可以使用 anova 建立方差分析表来比较模型。对于变量变换的形式，则可以使用 MASS 扩展包中的 boxcox 函数来进行 COX 变换。\nLogistic 回归 让我们用 logistic 回归来结束本系列的内容吧，本文用例来自于 John Maindonald 所著的《Data Analysis and Graphics Using R》一书，其中所用的数据集是 anesthetic，数据集来自于一组医学数据，其中变量 conc 表示麻醉剂的用量，move 则表示手术病人是否有所移动，而我们用 nomove 做为因变量，因为研究的重点在于 conc 的增加是否会使 nomove 的概率增加。\n首先载入数据集并读取部分文件，为了观察两个变量之间关系，我们可以利 cdplot 函数来绘制条件密度图。\nlibrary(DAAG) head(anesthetic) cdplot(factor(nomove) ~ conc, data=anesthetic, main='条件密度图', ylab='病人移动', xlab='麻醉剂量') 从图中可见，随着麻醉剂量加大，手术病人倾向于静止。下面利用 logistic 回归进行建模，得到 intercept 和 conc 的系数为 -6.47 和 5.57，由此可见麻醉剂量超过 1.16(6.47/5.57) 时，病人静止概率超过 50%。\nanes1 \u0026lt;- glm(nomove~conc, family=binomial(link='logit'), data=anesthetic) summary(anes1) 上面的方法是使用原始的 0-1 数据进行建模, 即每一行数据均表示一个个体，另一种是使用汇总数据进行建模，先将原始数据按下面步骤进行汇总：\nanestot \u0026lt;- aggregate(anesthetic[,c('move', 'nomove')],by = list(conc=anesthetic$conc),FUN = sum) anestot$conc \u0026lt;- as.numeric(as.character(anestot$conc)) anestot$total \u0026lt;- apply(anestot[,c('move', 'nomove')],1,sum) anestot$prop \u0026lt;- anestot$nomove / anestot$total 得到汇总数据 anestot 如下所示：\nconc move nomove total prop 1 0.8 6 1 7 0.1428571 2 1.0 4 1 5 0.2000000 3 1.2 2 4 6 0.6666667 4 1.4 2 4 6 0.6666667 5 1.6 0 4 4 1.0000000 6 2.5 0 2 2 1.0000000 对于汇总数据，有两种方法可以得到同样的结果，一种是将两种结果的向量合并做为因变量，如 anes2 模型。另一种是将比率做为因变量，总量做为权重进行建模，如 anes3 模型。这两种建模结果是一样的。\nanes2 \u0026lt;- glm(cbind(nomove,move)~conc,family=binomial(link='logit'),data=anestot) anes3 \u0026lt;- glm(prop~conc,family=binomial(link='logit'),weights=total,data=anestot) 根据 logistic 模型，我们可以使用 predict 函数来预测结果，下面根据上述模型来绘图：\nx \u0026lt;- seq(from=0,to=3,length.out=30) y \u0026lt;- predict(anes1,data.frame(conc=x),type='response') plot(prop~conc,pch=16,col='red',data=anestot,xlim=c(0.5,3),main='Logistic 回归曲线图',ylab='病人静止概率',xlab='麻醉剂量') lines(y~x,lty=2,col='blue') ","id":69,"section":"posts","summary":"\u003cp\u003e这篇《R 语言基础入门》是我在别人的博客上看到的，完成时间不会晚于 2011 年。觉得总结得相当不错，于是搬运到我的博客上。需要提到，有些链接可能已经失效。\u003c/p\u003e","tags":["R 语言"],"title":"R 语言基础入门","uri":"https://www.xzywisdili.com/2018/04/2018-04-02-rlearningnote/","year":"2018"},{"content":"主要内容：\n三种 subset 操作 六类 subset 不同对象的的重要区别 众所周知，R 语言里面有几种不同的 subset 方法：[]，[[]]，$，但是有时候会产生混淆。所以这篇笔记主要分析理清 R 语言里面的 subset 操作。\n来让我们用 [] 来 subset！ 对 atomic vector 进行 subset 操作时，通常使用 []，总共有 6 种形式：\n正数：简单，就是返回这个位置上的数（注意 R 语言从 1 开始计数）\n负数：忽略这个位置上的数\n逻辑值：只选择那些值为 TRUE 的数，所以基于此可以在 [] 中放入某些逻辑判断条件\nx[c(TRUE, FALSE, TRUE)] x[x \u0026gt; 3] 什么也没有：返回原始的 vector\n0：返回一个长度为 0 的 vector\n字符：如果 vector 设置好了 name，就可以使用字符型进行 subset\ny \u0026lt;- setNames(x, letters[1:4]) y[c(\u0026quot;d\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;a\u0026quot;)] 对于 list 来说，subset 可以有两种：使用 [] 往往会返回一个 list，使用 [[]] 和 $ 则会得到 list 的组成部分。\n对于 data frame 来说，它兼具两种数据结构的特点：使用一个 vector 对其 subset，它就可以当做 list；使用两个 vector 对其 subset，它就可以当做 matrix。\ndf \u0026lt;- data.frame( x = 1:3, y = 3:1, z = letters[1:3] ) df[1:2] # x y # 1 1 3 # 2 2 2 # 3 3 1 df[1:2, ] # x y z # 1 1 3 a # 2 2 2 b 还有两种操作：[[]] 和 $ 前面提到 [[]] 这种方式对于 list 来说，会从里面取出它的元素。这里可以引申出简化操作和保留操作的概念。\n顾名思义，保留操作会保证输出和输入的数据结构类型是一样的，而简化操作会对输入进行不同程度的简化：\nAtomic vector：移除变量名字 List：返回的是 list 里的元素，而不是 list Factor： 舍弃没有出现的 level Matrix：如果某个维度长度是 1，舍弃这个维度 Data frame：如果输出一个单变量，把它变成一个 vector Simplifying Preserving Vector x[[1]] x[1] List x[[1]] x[1] Factor x[1:4, drop=T] x[1:4] Array x[1, ] x[, 1] x[1, , drop=F] x[, 1, drop=F] Data frame x[, 1] x[[1]] x[, 1, drop=F] x[1] $ 在某种方面上和 [[]] 一样，它经常被用来从一个 data frame 种提取变量，如 mtcars$cyl 等。\n$ 和 [[]] 有一些区别：\n不能这样使用 $（但 [[]] 可以）：\nvar \u0026lt;- cyl mtcars$var $ 可以进行名字检索：\nmod \u0026lt;- lm(mpg ~ wt, data = mtcars) mod$df.r # 返回 mod$df.residual 应用 对字符缩写的转换（本质上是对分类变量设置了标签）：\nx \u0026lt;- c(\u0026quot;m\u0026quot;, \u0026quot;f\u0026quot;, \u0026quot;m\u0026quot;, \u0026quot;m\u0026quot;, \u0026quot;f\u0026quot;, \u0026quot;u\u0026quot;) lookup \u0026lt;- c(m=\u0026quot;Male\u0026quot;, f=\u0026quot;Female\u0026quot;, u=\u0026quot;Unknown\u0026quot;) lookup[x] # m f m m f u # \u0026quot;Male\u0026quot; \u0026quot;Female\u0026quot; \u0026quot;Male\u0026quot; \u0026quot;Male\u0026quot; \u0026quot;Female\u0026quot; \u0026quot;Unknown\u0026quot; info 是关于成绩的相关信息，我们可以使用 integer subset 进行信息的匹配和合并：\ngrades \u0026lt;- c(1, 2, 3, 3, 1) info = data.frame( grade = 1:3, desc = c(\u0026quot;Excellent\u0026quot;, \u0026quot;Good\u0026quot;, \u0026quot;Poor\u0026quot;), fail = c(F, F, T) ) // 使用 match id \u0026lt;- match(grades, info$grade) info[id, ] // 使用 rownames rownames(info) \u0026lt;- info$grade info[grades, ] Data frame 对行进行随机抽样：\ndf \u0026lt;- data.frame( x = rep(1:3, each=2), y = 6:1, z = letters[1:6] ) df[sample(nrow(df)), ] df[sample(nrow(df), 8, rep = T), ] n 列表示了每个观测重复几次，这里相当于把频数表转换为原始数据：\ndf \u0026lt;- data.frame(x = c(2, 4, 1), y = c(9, 11, 6), n = c(3, 5, 1)) df[rep(1:nrow(df), df$n), ] 使用布尔值的 subset 可以让我们在 data frame 中进行条件筛选：\n注意和 if 中使用的 \u0026amp;\u0026amp; 和 || 不同，这里使用的是 \u0026amp; 和 | 如果有很多条件，不想重复写 data frame 的名字，可以使用 subset mtcars[mtcars$gear == 5 \u0026amp; mtcars$cyl == 4, ] subset(mtcars, gear == 5 \u0026amp; cyl == 4) ","id":70,"section":"posts","summary":"\u003cp\u003e主要内容：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e三种 subset 操作\u003c/li\u003e\n\u003cli\u003e六类 subset\u003c/li\u003e\n\u003cli\u003e不同对象的的重要区别\u003c/li\u003e\n\u003c/ul\u003e","tags":["R 语言"],"title":"学习 R 语言里的 subset","uri":"https://www.xzywisdili.com/2018/04/2018-04-02-rsubset/","year":"2018"},{"content":"介绍了 R 语言中的内置基础数据结构。\nR 的基础数据结构可以概括为：\n相同元素 不同元素 一维（1d） Atomic vector List 二维（2d） Matrix Data frame 多维（nd） Array R 实际上是没有标量，或者 0 维数据结构的。那些所谓的标量其实是长度为 1 的向量。\nR 的一维数据结构 R 语言中的一维数据结构向量（vector）有两种类型：atomic vector 和 list，它们有三个属性：\ntypeof() length() attributes() 这两者的区别是，atomic vector 所有元素相同种类，list 中的元素可以不同种类。\n注意：is.vector(x) 并不能检查 x 是不是 vector，最好使用 is.atomic(x) || is.list(x)。\nAtomic vectors 四种常见类型：logical, integer, double 和 character 两种少见类型：complex 和 raw atomic vectors 使用 c() 来创建（c 是 combine 的简写）。\nint_vec \u0026lt;- c(1L, 2L, 5L) dbl_vec \u0026lt;- c(1.2, 2, 3.4) log_vec \u0026lt;- c(TRUE, FALSE) cha_vec \u0026lt;- c('Li', 'Wang', 'Wu') 使用 is.integer(), is.double(), is.logical(), is.character() 来判断各自的类型，想判断是否为 atomic vector，使用 is.atomic()。对于 integer 和 double 类型的，is.numeric() 均会返回 TRUE。\n由于 atomic vectors 要保证元素类型一样，所以会自动进行类型转换。如数值型和字符型在一起会转换为字符型，TRUE 和 FALSE 会自动转换为 1 和 0。\nLists list 数据结构中可以保存不同类型的数据。使用 is.list() 来检查是否为 list，使用 as.list() 把变量转换为 list，使用 unlist() 把 list 转换为 atomic vector。\n疑问：为什么可以使用 unlist() 而不用 as.vector() 来将列表转换为向量？查阅网上的资料，为什么大家喜欢写 as.vector(unlist(list_a)) ？\n特性（attribute） 所有对象都可以有任意特性，用来保存一些关于对象的额外信息。\n使用 attr() 获得单独的一个特性，或者使用 attributes() 来获得所有特性，后者获得的是一个列表。\ny \u0026lt;- 1:10 attr(y, \u0026quot;my_attribute\u0026quot;) \u0026lt;- \u0026quot;This is a vector\u0026quot; attr(y, \u0026quot;my_attribute\u0026quot;) str(attributes(y)) 使用 structure() 可以直接生成一个含有特性的对象：\nstructure(1:10, my_attribute=\u0026quot;this is a vector\u0026quot;) 当更改 vector 时，会有三个很重要的特性不会丢失：\nname，使用 name(x) 获取 dimensions，使用 dim(x) 获取 class，使用 class(x) 获取 Names 可以通过三种方式命名一个 atomic vector 变量：\n创建时命名：x \u0026lt;- c(a=1, b=2, c=3) 设置 names 属性：x2 \u0026lt;- 1:3; names(x2) \u0026lt;- c('a', 'b', 'c') 修改另一个 vector 得到：x3 \u0026lt;- setNames(1:3, c('a', 'b', 'c')) 不需要所有值都有名字，如果不对其进行命名，names(x) 会得到 NULL。想要移除 vector 的名字，使用 unname(x) 或 names(x) \u0026lt;- NULL。\nFactors factor 是一种特殊的 vector，里面的元素只能是预先定义好的值，专门用来描述分类变量。它基于 vector，但额外有两个属性：class 和 levels，使用 table 可以快速查看各组的频数：\nx \u0026lt;- factor(c('M', 'F', 'F', 'M')) class(x) # \u0026quot;factor\u0026quot; levels(x) # \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; table(x) # x # F M # 2 2 很多加载数据的函数会把字符型的变量自动转化为 factor，设置参数 stringsAsFactors = FALSE 来制止转化行为。很多函数会把 factor 当做是 integer 类型的，会带来很多错误。如果想用 factor 原本的元素，先用 as.character() 把它转化为 character 型。\nmatrix 和 array array 可以存放多维数据，matrix 只能是二维，是一种特殊的 array。\n创建 array 和 matrix：\nm \u0026lt;- matrix(1:6, ncol = 3, nrow = 2) a \u0026lt;- array(1:12, dim=c(2, 3, 2)) c \u0026lt;- 1:6 dim(c) \u0026lt;- c(2, 3) 有两类属性：\n长度和维度：length()，dim()，nrow()，ncol() 命名：rownames()，colnames()，dimnames() 还可以使用 rbind() 和 cbinid() 来生成矩阵。转置 matrix 使用 t()，转置 array 使用 aperm()。\nData frame Data frame 本质上是一个由 vector 构成的 list 的二维数据结构，所以它兼具 list 和 matrix 的一些属性，如 names(), colnames(), rownames(), length(),\n使用 data.frame() 创建一个 Data frame：\ndf \u0026lt;- data.frame( x = 1:5, y = c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;d\u0026quot;, \u0026quot;e\u0026quot;), stringsAsFactors = FALSE ) 默认会把 character 型的 atomic vector 转换为 factor，需要参数 stringsAsFactors=FALSE 来禁止这一行为。\n检查和转换也很简单，和之前介绍的完全配套：is.data.frame()，as.data.frame()。\n可以使用 rbind() 和 cbind() 来对 data frame 进行合并，但要注意的是合并时对应的行和列必须配套。\n如果你把 list 或者 matrix 传入 data.frame() 中，想构建一列是 list 或者 matrix，它会报错。这时可以采用两种方法：\n使用 cbind() 将已有的 data frame 与之合并 还有就是使用 I()：data.frame(x=1:3, y=I(list(1:2, \u0026quot;a\u0026quot;, FALSE)) 另外还有个很有趣的一点是：使用 df[FALSE, ] 和 df[, FALSE] 可以分别得到 0 行的和 0 列的 data frame。\n","id":71,"section":"posts","summary":"\u003cp\u003e介绍了 R 语言中的内置基础数据结构。\u003c/p\u003e","tags":["R 语言"],"title":"R 语言基础数据结构","uri":"https://www.xzywisdili.com/2018/03/2018-03-28-rstructure/","year":"2018"},{"content":"自从 2016 年起，我逼自己养成一个习惯，每年准备一个记录阅读情况的 Excel 表格。一开始的时候，满怀热情，又是搞了图表，又是自学了 VBA 编程，然后模仿 Github 代码记录写了一个玩意儿——365 天对应 365 个小方格，每天读的页数越多，这天就越绿。当时熬夜完成的时候还特兴奋，但后来证明，没多大蛋用。当然读书记录这方法成效斐然，我在 2016 年读完了整整 40 本书！（要知道我以前一年可能翻不完一本书）\n2017 年当然继续坚持我的读书记录，当然没有整那么多幺蛾子，只有一个阅读进度条。一年匆匆而过，在 kindle 的加持下，我读了 73 本书，简单算算比 16 年增长了 82.5%。——嗯，抛开不谈这些无聊枯燥的数字，我也不想一本一本把书单列出来。这次打算通过几个关键词来大概讲一讲今年的阅读之旅。\n逆天改命 什么是「逆天改命」？上天明明告诉你命数已尽，而偏偏有人把你从死神手中抢回来。而关于「逆天改命」的作品，自然就是关于医疗，或医学这个话题下的作品了。\n关于医学的作品，常常涉及生死的话题。而生死，是最具有阅读冲击力，也是最容易上升到哲学高度的话题了。所以，在我意料之中的，今年读过的不少医学主题的作品都无比优秀。《神经外科的黑色幽默》从一个神经外科医生的角度，讲述了在他职业生涯中经历过的大大小小的具有启发的病例，《病毒来袭》则把目光聚焦在非洲，那个许多病毒，包括埃博拉已经在肆虐的那片土地，《上帝的手术刀》则更加偏向实验室，以深入浅出的方式解释了基因疗法的原理和进展。\n我自己作为一名医学生，读到这方面作品的时候，心中无比向往，但又觉得重任在肩。通过读书可以提前接触面对生死的处境，也更能保持一种平和又不冷漠的心态。至于知识和方法，还是留到课堂上系统学习吧。\n伊坂幸太郎 能从茫茫书海和伊坂幸太郎相遇，也是一种幸运。我认为伊坂幸太郎最大的特点是他的作品充满了新奇古怪有想象力的点子，从《金色梦乡》的大型逃亡烟火秀，到《奥杜邦的祈祷》的会预言会说话的稻草人，再到《魔王》里会控制别人说话的超能力。再配上总有股悬疑味道的情节，至少你会被迷着一直读下去。\n当然，他还不止这些。伊坂幸太郎的作品绝不会仅流于表面，而是探讨一些关于社会，人物情感，艺术，甚至是政治的议题——很少有小说作家会围绕政治议题展开一本小说。虽然我不是完全认同伊坂幸太郎在书中流露出的一些观点，有些作品也在豆瓣没有打 5 星，但我还是对他的其他作品抱以无比期待。\n尤·奈斯博 尤·奈斯博是另一位我在 17 年无比崇拜的小说作家，他的作品风格被冠以「硬汉推理小说」。这个说法我举双手双脚赞成。四部小说《雪人》《猎豹》《幽灵》《警察》读下来，一部借着一部把主角哈利警探虐的死去活来。就说第四部《警察》里面，凶手是一名警察杀手，死几个警察不够，哈利的老搭档被残忍碎尸，小女孩烧焦，署长的打手被打成猪头三满脸绷带，署长一只眼睛挖掉，还有两人钻冰箱躲避密室定时炸弹的情节……\n当然，说到这里你可能以为这是一系列小说颇有些「重口味」的感觉，甚至觉得无比喜欢奈斯波的我血液中也有点变态的要素。其实真不是这样的。作为一个喜爱推理小说的读者，我只是被北欧一种粗犷，奔放，浓烈，极富有张力的情节和叙事所折服了。也很难在其他书本里找到像哈利这样的角色了（电影里面还是有的，比如《飓风营救》里面的爸爸）。\n人生 有这么一种说法，在将死之时，人的一生就好像放电影一样从眼前经过。今年读过的约翰·威廉斯的《斯通纳》和毛姆的《人性的枷锁》都如同放电影一般完整展示了人的一生。\n我们可以看到，无论是《斯通纳》里的斯通纳，还是《人性的枷锁》里的菲利普，都历经各种各样生活的磨难，事业的重重阻碍，也在爱情关系和家庭关系中饱受折磨。作者把人生描述成一杯喝不完的苦咖啡，并想告诉你人生是无奈的，是无意义的，是注定会耗光你的激情，让你败下阵来的。\n说实话，刚读完的时候，真的一度有被这种情绪感染到。我们不可能永远面对人生都是一种积极的态度，更何况「人生」和「积极」都是两个大词。我想，对于我自己，在以后每个时间点上做出最适合自己的选择，这就是我面对人生的态度。\n畅销 关于畅销书我一直想说几句。各大书商，什么亚马逊京东都喜欢在年底出一个年度书籍盘点的东西。当然，依据当然很大一部分程度上当然是在平台上的销量了。从排行榜上可以看到我们广大的人民群众喜欢看什么样的书。\n最火的当然是东野圭吾了，居然在前十里独占三席，《解忧杂货铺》《白夜行》《嫌疑人 X 的献身》都读过了。刘慈欣的《三体》，伍绮诗的《无声告白》（虽然不是今年畅销），马伯庸《长安十二时辰》等等。我发现畅销书的共通特点就是好读，你手里捧着可以一直一直读下去，中间不用休息，更不会困得合眼睡着。但我猜想大部分人读过就像读了一个故事，至于书籍深处的主题，不愿多去思考和讨论。我也是这大部分人中的一员，本来要读畅销书就是图个乐。但我觉得 18 年我还是离畅销书稍微远一点。\n一篇文章 这类书籍就更有意思了。「一篇文章」的意思是，本来用一篇文章就能说清的，硬是给抻成一本书，就好像拉面一样。《断舍离》兜售了一个「断舍离」的观点，《聪明人用方格笔记本》推销了「方格笔记本」的观点，《为什么精英都是清单控》全书也就是对「清单」的大力推荐。\n读这种书时，你总以为也许翻到下一页，也许会有新观点，新方法论，看惯了玫瑰，给我来一枝梅花也好。但直到翻到最后一页，才发现就这么些内容翻来覆去地讲，怒得把书一摔，愤而上豆瓣打个低分，却还觉得不解气。本来就是一篇博文的量，就按照一篇博文来写。我一定给你点赞转发，干嘛非要写本书呢？\n结语 离 2018 年也就一两天了，我也提前早早做好了 18 年的阅读记录表格。表格和去年没太大变化，但我的阅读理念变了，我不想再去追求量，而想真的沉下来读几本好书。少读几本小说，多看看社科方面的书籍。刚才打开知乎年度总结看了一眼，我居然一年在知乎上看了快 2 千万字。惋惜不已，总料想，这 2 千万转换成书，那得看多少本啊。但是这种转换意义怕也是不大，毕竟你要让我戒掉知乎恐怕也是一时半会儿戒不掉的。\n决定好了，第一本就从《娱乐至死》读起。\n","id":72,"section":"posts","summary":"\u003cp\u003e自从 2016 年起，我逼自己养成一个习惯，每年准备一个记录阅读情况的 Excel 表格。一开始的时候，满怀热情，又是搞了图表，又是自学了 VBA 编程，然后模仿 Github 代码记录写了一个玩意儿——365 天对应 365 个小方格，每天读的页数越多，这天就越绿。当时熬夜完成的时候还特兴奋，但后来证明，没多大蛋用。当然读书记录这方法成效斐然，我在 2016 年读完了整整 40 本书！（要知道我以前一年可能翻不完一本书）\u003c/p\u003e","tags":["阅读","总结"],"title":"2017 读书印象","uri":"https://www.xzywisdili.com/2017/12/2017-12-30-reading/","year":"2017"},{"content":"R 语言世界里的一个大杀器。\n我想，在所有接触和学习 R 语言的初学者（当然包括我）眼里，有两个东西是相当酷的。一个是 shiny，可以轻松做出交互式的图表；而另一个就是 Rmarkdown 了。\n现如今我想 markdown 是何物已经不用过多介绍了，它语法简洁，门槛很低，已经到了一种是个人就会的地步。那 Rmarkdown 到底是什么呢？\nRmarkdown 的官网上有这样几段介绍：\nTurn your analyses into high quality documents, reports, presentations and dashboards. Use a productive notebook interface to weave together narrative text and code to produce elegantly formatted output. Use multiple languages including R, Python, and SQL.\n简单来说，就是 Rmarkdown 能够使用 markdown 的语法，支持多种语言代码的运行和输出（R, Python 和 SQL），输出高质量的文档。\n这个所谓的「高质量」的文档到底是怎么样的呢？可以这么说，有不少的人都认为 Rmarkdown 可以成为科技写作的主流：\n为什么 Markdown + R 有叫大概率成为科技写作主流\n数据文档的革命\n既然说了这么多，我们就来简单看一看 Rmarkdown 的真正表现。我会将 markdown 里使用的大多数常用语法在 Rmarkdown 里展示出来。\n新建一个 Rmarkdown 文件 我使用的是 Rstudio，可以在 File \u0026gt;\u0026gt; New File \u0026gt;\u0026gt; R Markdown... 里新建一个 Rmarkdown 文档。\n填入文档标题和作者名即可，下面的可以选择默认选项（在后来可以轻松改变）。\n如何使 Rmarkdown 完美支持中文 我的环境是 Mac OSX 系统，安装 MacTex 在 RStudio 的 Preferences 中的 「Sweave」 里把 「Weave Rnw files using」改为 「knitr」，「Type LaTeX into PDF using」 改为 「XeLaTeX」 在 Rmarkdown 文档的同路径下创建一个 header.tex，内容是\\usepackage{ctex}，然后把 Rmarkdown 文档 yaml 栏内的内容改为如下： title: \u0026quot;我的第一个 Rmarkdown 文档\u0026quot; author: \u0026quot;anthor\u0026quot; output: pdf_document: includes: in_header: header.tex keep_tex: yes latex_engine: xelatex html_document: default 测试一下：\n中文显示 测试中文的显示效果。 这里先输入一段文字，我们这段文字的目的是测试在 Rmarkdown 中中文的显示效果： * 列表项目 1 * 列表项目 2 * 列表项目 3 输出的 pdf 显示为： 完美！\n加粗，链接和行内代码块 一段常见的 Markdown 语法：\n## Start with a cool section You can use traditional **Markdown** syntax, such as [links](http://yihui.name/knitr) and `code`. Here is a quote: \u0026gt; A girl phoned me the other day and said \u0026quot;Come on over, there's nobody home.\u0026quot; I went over. Nobody was home. -- Rodney Dangerfield 显示为： 运行 R 代码 写入一段 R 语言代码，会在编译时自动运行： 在 pdf 中显示为： 还可以使用 R 语言强大的画图功能： 在 pdf 中显示为： 数学公式 Rmarkdown 中同样完美支持数学公式的书写：\n## A little bit math Our regression equation is $Y=`r b[1]` + `r b[2]`x$, and the model is: $$ Y = \\beta_0 + \\beta_1 x + \\epsilon$$ 显示为： 表格 使用 Markdown 提供的表格语法：\nTable: Demonstration of simple table syntax. Right Left Center Default ----- ---- ------ ------- 12 12 12 12 123 123 123 123 1 1 1 1 我们得到的是我们希望的三线图格式： 其他的比如脚注，有序列表，无序列表，定义等等不再做过多展示，上面提到的内容已经足以证明 Rmarkdown 的强大。而这些东西的学习成本几乎为零，这太令人不可思议了。\n","id":73,"section":"posts","summary":"R 语言世界里的一个大杀器。 我想，在所有接触和学习 R 语言的初学者（当然包括我）眼里，有两个东西是相当酷的。一个是 shiny，可以轻松做出交互式","tags":["R 语言","Markdown"],"title":"轻松愉快地开始使用 Rmarkdown","uri":"https://www.xzywisdili.com/2017/12/2017-12-23-rmarkdown/","year":"2017"},{"content":"最近沉迷于手游，导致博客久久没有更新。其实博客就是个自己阶段学习的总结，把自己学会的东西写成博客，算是自己复习了一遍，将来忘了的时候也有的看。最近学习的很简单，就是 dplyr 包里面最基础的 5 种数据处理方法。\n所用的数据集 这次我们所有数据处理的用法范例都是建立在 flights 数据集上的，先来看看这个数据集：\nlibrary(nycflights13) library(tidyverse) head(flights) 可以看到，每一条数据代表的是一次航班飞行记录的相关信息：\n# A tibble: 6 x 19 year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; 1 2013 1 1 517 515 2 830 819 2 2013 1 1 533 529 4 850 830 3 2013 1 1 542 540 2 923 850 4 2013 1 1 544 545 -1 1004 1022 5 2013 1 1 554 600 -6 812 837 6 2013 1 1 554 558 -4 740 728 # ... with 11 more variables: arr_delay \u0026lt;dbl\u0026gt;, carrier \u0026lt;chr\u0026gt;, flight \u0026lt;int\u0026gt;, # tailnum \u0026lt;chr\u0026gt;, origin \u0026lt;chr\u0026gt;, dest \u0026lt;chr\u0026gt;, air_time \u0026lt;dbl\u0026gt;, distance \u0026lt;dbl\u0026gt;, # hour \u0026lt;dbl\u0026gt;, minute \u0026lt;dbl\u0026gt;, time_hour \u0026lt;dttm\u0026gt; 看看这个数据集中都包含了哪些项目（其中一些项目的含义可以通过名字猜出一二）：\n\u0026gt; names(flights) [1] \u0026quot;year\u0026quot; \u0026quot;month\u0026quot; \u0026quot;day\u0026quot; \u0026quot;dep_time\u0026quot; [5] \u0026quot;sched_dep_time\u0026quot; \u0026quot;dep_delay\u0026quot; \u0026quot;arr_time\u0026quot; \u0026quot;sched_arr_time\u0026quot; [9] \u0026quot;arr_delay\u0026quot; \u0026quot;carrier\u0026quot; \u0026quot;flight\u0026quot; \u0026quot;tailnum\u0026quot; [13] \u0026quot;origin\u0026quot; \u0026quot;dest\u0026quot; \u0026quot;air_time\u0026quot; \u0026quot;distance\u0026quot; [17] \u0026quot;hour\u0026quot; \u0026quot;minute\u0026quot; \u0026quot;time_hour\u0026quot; 当然在 Rstudio 中，可以更加方便地通过 View(flights) 来查看数据集。\n接下来就来说一说 dplyr 最基础的 5 个处理数据的方法：\n使用 filter() 来过滤筛选\n使用 arrange() 来重新排列\n使用 select() 来选择数据集里面的变量\n使用 mutate()，基于已有的变量，创建需要的新变量\n使用 summarize() 打碎重组多个变量，来完成一些概括\n每个函数我都想拿例子来介绍。\n观测的「过滤器」：filter() filter() 筛选的目标是观测（也就是数据中横向的行）。\n如果想看看我生日那天的航班数据：\n\u0026gt; filter(flights, month == 7, day == 31) # A tibble: 1,001 x 19 year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; 1 2013 7 31 10 2359 11 344 340 2 2013 7 31 19 2359 20 355 344 3 2013 7 31 132 2359 93 510 350 4 2013 7 31 459 500 -1 633 640 5 2013 7 31 529 536 -7 755 806 6 2013 7 31 534 515 19 739 725 7 2013 7 31 540 540 0 829 840 8 2013 7 31 541 545 -4 912 921 9 2013 7 31 542 545 -3 803 813 10 2013 7 31 551 600 -9 640 700 # ... with 991 more rows, and 11 more variables: arr_delay \u0026lt;dbl\u0026gt;, carrier \u0026lt;chr\u0026gt;, # flight \u0026lt;int\u0026gt;, tailnum \u0026lt;chr\u0026gt;, origin \u0026lt;chr\u0026gt;, dest \u0026lt;chr\u0026gt;, air_time \u0026lt;dbl\u0026gt;, # distance \u0026lt;dbl\u0026gt;, hour \u0026lt;dbl\u0026gt;, minute \u0026lt;dbl\u0026gt;, time_hour \u0026lt;dttm\u0026gt; 看看整个夏天的航班数据（使用逻辑运算符）：\n\u0026gt; filter(flights, month == 6 | month == 7 | month == 8) # A tibble: 86,995 x 19 year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; 1 2013 6 1 2 2359 3 341 350 2 2013 6 1 451 500 -9 624 640 3 2013 6 1 506 515 -9 715 800 4 2013 6 1 534 545 -11 800 829 5 2013 6 1 538 545 -7 925 922 6 2013 6 1 539 540 -1 832 840 7 2013 6 1 546 600 -14 850 910 8 2013 6 1 551 600 -9 828 850 9 2013 6 1 552 600 -8 647 655 10 2013 6 1 553 600 -7 700 711 # ... with 86,985 more rows, and 11 more variables: arr_delay \u0026lt;dbl\u0026gt;, # carrier \u0026lt;chr\u0026gt;, flight \u0026lt;int\u0026gt;, tailnum \u0026lt;chr\u0026gt;, origin \u0026lt;chr\u0026gt;, dest \u0026lt;chr\u0026gt;, # air_time \u0026lt;dbl\u0026gt;, distance \u0026lt;dbl\u0026gt;, hour \u0026lt;dbl\u0026gt;, minute \u0026lt;dbl\u0026gt;, time_hour \u0026lt;dttm\u0026gt; 当然更好的方法可能是：\nfilter(flights, month %in% c(6, 7, 8)) 对观测进行重排：arrange() arrange()的目标也是观测（横向的行），只不过是对观测进行重排。比如我想要以起飞时的延迟时间 dep_delay 的倒序排列：\n\u0026gt; arrange(flights, desc(dep_delay)) # A tibble: 336,776 x 19 year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; 1 2013 1 9 641 900 1301 1242 1530 2 2013 6 15 1432 1935 1137 1607 2120 3 2013 1 10 1121 1635 1126 1239 1810 4 2013 9 20 1139 1845 1014 1457 2210 5 2013 7 22 845 1600 1005 1044 1815 6 2013 4 10 1100 1900 960 1342 2211 7 2013 3 17 2321 810 911 135 1020 8 2013 6 27 959 1900 899 1236 2226 9 2013 7 22 2257 759 898 121 1026 10 2013 12 5 756 1700 896 1058 2020 # ... with 336,766 more rows, and 11 more variables: arr_delay \u0026lt;dbl\u0026gt;, # carrier \u0026lt;chr\u0026gt;, flight \u0026lt;int\u0026gt;, tailnum \u0026lt;chr\u0026gt;, origin \u0026lt;chr\u0026gt;, dest \u0026lt;chr\u0026gt;, # air_time \u0026lt;dbl\u0026gt;, distance \u0026lt;dbl\u0026gt;, hour \u0026lt;dbl\u0026gt;, minute \u0026lt;dbl\u0026gt;, time_hour \u0026lt;dttm\u0026gt; 变量的选择器：select() select() 可以灵活选择数据集中的变量，或者叫特征（纵向的列）。 比如我们只想看日期和起飞时间这几个变量，可以用 year: dep_time 来进行选择：\n\u0026gt; select(flights, year:dep_time) # A tibble: 336,776 x 4 year month day dep_time \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; 1 2013 1 1 517 2 2013 1 1 533 3 2013 1 1 542 4 2013 1 1 544 5 2013 1 1 554 6 2013 1 1 554 7 2013 1 1 555 8 2013 1 1 557 9 2013 1 1 557 10 2013 1 1 558 # ... with 336,766 more rows 可以在参数中使用 everything() 来代表所有变量。\n如何添加新变量：mutate() 如果基于原有的数据集，想要添加新的变量（纵向的列），mutate() 正是你所需要的的。 比如我们已知航班的飞行时间和飞行距离，就可以计算出航班的平均速度，并添加这个变量：\n\u0026gt; mutate(select(flights, year: day, distance, air_time), + speed = distance / air_time * 60) # A tibble: 336,776 x 6 year month day distance air_time speed \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; 1 2013 1 1 1400 227 370.0441 2 2013 1 1 1416 227 374.2731 3 2013 1 1 1089 160 408.3750 4 2013 1 1 1576 183 516.7213 5 2013 1 1 762 116 394.1379 6 2013 1 1 719 150 287.6000 7 2013 1 1 1065 158 404.4304 8 2013 1 1 229 53 259.2453 9 2013 1 1 944 140 404.5714 10 2013 1 1 733 138 318.6957 # ... with 336,766 more rows 使用 transmute() 可以只保留新变量。\n概括一个数据集：summarize() 说「概括数据集」可能不太合适，summarize() 可以求得某个变量的平均值，总和等统计量。 比如我们求起飞延迟时间的平均值：\n\u0026gt; summarize(flights, delay = mean(dep_delay, na.rm = TRUE)) # A tibble: 1 x 1 delay \u0026lt;dbl\u0026gt; 1 12.63907 summarize() 经常和 group_by() 一起用，这样能对原数据集分组并求出想要的统计量：\n\u0026gt; flights %\u0026gt;% + group_by(year, month, day) %\u0026gt;% + summarize(delay = mean(dep_delay, na.rm = TRUE)) # A tibble: 365 x 4 # Groups: year, month [?] year month day delay \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; 1 2013 1 1 11.548926 2 2013 1 2 13.858824 3 2013 1 3 10.987832 4 2013 1 4 8.951595 5 2013 1 5 5.732218 6 2013 1 6 7.148014 7 2013 1 7 5.417204 8 2013 1 8 2.553073 9 2013 1 9 2.276477 10 2013 1 10 2.844995 # ... with 355 more rows 可以看到，所有观测都被按日期分组了，而 delay 表示的是每组的平均值。这里 na.rm 意思是 remove na，即移除缺失值。 %\u0026gt;% 代表的是一种操作流程（pipeline），这个符号之前的数据集作为符号之后函数的第一个参数。\n综合使用 直接拍上下面一段代码，这里用到了 filter()，summarize()，并使用 ggplot 画图。\n\u0026gt; library(ggplot) \u0026gt; flights %\u0026gt;% + filter(!is.na(dep_delay), !is.na(arr_delay)) %\u0026gt;% + group_by(tailnum) %\u0026gt;% + summarize( + delay = mean(arr_delay), + n = n() + ) %\u0026gt;% + filter(n \u0026gt; 25) %\u0026gt;% + ggplot(mapping = aes(x = n, y = delay)) + + geom_point(alpha = 1/10) 我们先把 dep_delay 和 arr_delay 这两列里面包含 na 值的观测移除掉，以航班尾号分组，查看每组的数量和到达延迟时间，并作图。\n这篇虽然只是相当基本地介绍了 5 种函数，但是博客也够长了，就先到这里打住吧。\n","id":74,"section":"posts","summary":"\u003cp\u003e最近沉迷于手游，导致博客久久没有更新。其实博客就是个自己阶段学习的总结，把自己学会的东西写成博客，算是自己复习了一遍，将来忘了的时候也有的看。最近学习的很简单，就是 \u003ccode\u003edplyr\u003c/code\u003e 包里面最基础的 5 种数据处理方法。\u003c/p\u003e","tags":["R 语言","数据处理"],"title":"dplyr 包里面必会的处理数据方法","uri":"https://www.xzywisdili.com/2017/12/2017-12-19-datatransform/","year":"2017"},{"content":"在 R 语言 的官方网址标题上写着「The R Project for Statistical Computing」，直接点明了 R 语言是一门主要用于统计计算的程序语言。如果你对统计感兴趣，那么就一定不能错过 R。本文只总结了 R 语言里面的那些最最最基础，想用好 R 必须要背过的内容。话不多说，赶紧上车。\n基本算数 直接进行算数运算：\n\u0026gt; 4 + 6 [1] 10 将值保存在对象中进行运算：\n\u0026gt; x \u0026lt;- 6 \u0026gt; y \u0026lt;- 4 \u0026gt; z \u0026lt;- x + y \u0026gt; z [1] 10 显示我们已经创建的对象：\n\u0026gt; ls() [1] \u0026quot;x\u0026quot; \u0026quot;y\u0026quot; \u0026quot;z\u0026quot; 清除一些对象：\n\u0026gt; rm(x, y) \u0026gt; ls() [1] \u0026quot;z\u0026quot; 创建向量（vector）：\n\u0026gt; z \u0026lt;- c(5, 9, 1, 0) 使用函数 c(x, y) 可以做到向量的连接：\n\u0026gt; x \u0026lt;- c(1, 2) \u0026gt; y \u0026lt;- c(3, 4) \u0026gt; z \u0026lt;- c(x, y) \u0026gt; z [1] 1 2 3 4 向量之间也可以完成运算，这些运算是按照元素之间发生（element-wise）的：\n\u0026gt; x + y [1] 4 6 \u0026gt; x * y [1] 3 8 [1] 3 8 \u0026gt; x ** 2 [1] 1 4 我们可以使用多种方式生成一个序列，额外参数 by 代表公差，length.out 参数表示生成序列的长度：\n\u0026gt; x \u0026lt;- 1:4 \u0026gt; x [1] 1 2 3 4 \u0026gt; seq(1, 9, by=2) [1] 1 3 5 7 9 \u0026gt; seq(8, 20, length=6) [1] 8.0 10.4 12.8 15.2 17.6 20.0 \u0026gt; seq(8, 20, length.out=6) [1] 8.0 10.4 12.8 15.2 17.6 20.0 另一个可以生成含有重复元素的向量的方法是rep()：\n\u0026gt; rep(0, 100) [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \u0026gt; rep(1:3, 6) [1] 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 \u0026gt; rep(1:3, c(4, 5, 6)) [1] 1 1 1 1 2 2 2 2 2 3 3 3 3 3 3 简单统计与下标 算出一个向量的均值，方差和概要：\n\u0026gt; y \u0026lt;- c(33, 44, 29, 16, 25, 45, 33, 19, 54, 22, 21, 59, 11, 24, 56) \u0026gt; mean(y) [1] 32.73333 \u0026gt; var(y) [1] 236.0667 \u0026gt; summary(y) Min. 1st Qu. Median Mean 3rd Qu. Max. 11.00 21.50 29.00 32.73 44.50 59.00 可以只计算向量中的一部分：\n\u0026gt; y[1:6] [1] 33 44 29 16 25 45 \u0026gt; summary(y[1:6]) Min. 1st Qu. Median Mean 3rd Qu. Max. 16.00 26.00 31.00 32.00 41.25 45.00 \u0026gt; mean(y[c(1, 4, 6, 9)]) [1] 37 有两个必须记住的函数length()和sum()：\n\u0026gt; length(y) [1] 15 \u0026gt; sum(y) [1] 491 矩阵 在 R 中可以使用 cbind() 和 rbind() 来组合向量创建矩阵，使用 dim() 可以查看矩阵的维度：\n\u0026gt; x \u0026lt;- c(5, 7, 9) \u0026gt; y \u0026lt;- c(6, 3, 4) \u0026gt; z \u0026lt;- cbind(x, y) \u0026gt; z x y [1,] 5 6 [2,] 7 3 [3,] 9 4 \u0026gt; dim(z) [1] 3 2 \u0026gt; rbind(z, z) x y [1,] 5 6 [2,] 7 3 [3,] 9 4 [4,] 5 6 [5,] 7 3 [6,] 9 4 还可以通过 matrix() 来创建矩阵，参数 nrow 表示想要矩阵有几行，byrow 如果为真，则表示按行填充：\n\u0026gt; z \u0026lt;- matrix(c(5, 7, 9, 6, 3, 4), nrow=3) \u0026gt; z [,1] [,2] [1,] 5 6 [2,] 7 3 [3,] 9 4 \u0026gt; z \u0026lt;- matrix(c(5, 7, 9, 6, 3, 4), nrow=3, byrow=T) \u0026gt; z [,1] [,2] [1,] 5 7 [2,] 9 6 [3,] 3 4 \u0026gt; z \u0026lt;- matrix(c(5, 7, 9, 6, 3, 4), nrow=3, byrow=T) 矩阵和矩阵之间的运算（也是 element-wise），矩阵乘法使用 %*%：\n\u0026gt; z [,1] [,2] [1,] 5 7 [2,] 9 6 [3,] 3 4 \u0026gt; y \u0026lt;- matrix(c(1, 3, 0, 9, 5, -1), nrow=3, byrow=T) \u0026gt; y [,1] [,2] [1,] 1 3 [2,] 0 9 [3,] 5 -1 \u0026gt; z + y [,1] [,2] [1,] 6 10 [2,] 9 15 [3,] 8 3 \u0026gt; z * y [,1] [,2] [1,] 5 21 [2,] 0 54 [3,] 15 -4 \u0026gt; x \u0026lt;- matrix(c(3, 4, -2, 6), nrow=2, byrow=T) \u0026gt; x [,1] [,2] [1,] 3 4 [2,] -2 6 \u0026gt; y%*%x [,1] [,2] [1,] -3 22 [2,] -18 54 [3,] 17 14 使用 t() 求矩阵的转置，使用 solve() 求矩阵的逆矩阵：\n\u0026gt; t(z) [,1] [,2] [,3] [1,] 5 9 3 [2,] 7 6 4 \u0026gt; solve(x) [,1] [,2] [1,] 0.23076923 -0.1538462 [2,] 0.07692308 0.1153846 使用合适的下标快速提取矩阵元素：\n\u0026gt; z [,1] [,2] [1,] 5 7 [2,] 9 6 [3,] 3 4 \u0026gt; z[1, 1] [1] 5 \u0026gt; z[c(2, 3), 2] [1] 6 4 \u0026gt; z[, 2] [1] 7 6 4 \u0026gt; z[1:2, ] [,1] [,2] [1,] 5 7 [2,] 9 6 apply 函数 对于 R 内置的一个数据集 trees，如果我们想求数据集中每一个变量的均值，可能首先想到一个一个去求，但如果有很多变量就行不通了；可能会想写一个循环，虽然在 R 语言里可以做到，但是并不推荐循环；我们可以使用 apply() 函数简单解决：\n\u0026gt; data(trees) \u0026gt; head(trees) Girth Height Volume 1 8.3 70 10.3 2 8.6 65 10.3 3 8.8 63 10.2 4 10.5 72 16.4 5 10.7 81 18.8 6 10.8 83 19.7 \u0026gt; apply(trees, 2, mean) Girth Height Volume 13.24839 76.00000 30.17097 统计计算和模拟 最开始我们说到 R 语言是用来做统计计算的，比如我们可以使用 dnorm()，pnorm() 和 qnorm()三板斧：\n\u0026gt; dnorm(0, 3, 2) [1] 0.0647588 \u0026gt; dnorm(seq(-10, 10), 0, 2) [1] 7.433598e-07 7.991871e-06 6.691511e-05 4.363413e-04 2.215924e-03 [6] 8.764150e-03 2.699548e-02 6.475880e-02 1.209854e-01 1.760327e-01 [11] 1.994711e-01 1.760327e-01 1.209854e-01 6.475880e-02 2.699548e-02 [16] 8.764150e-03 2.215924e-03 4.363413e-04 6.691511e-05 7.991871e-06 [21] 7.433598e-07 \u0026gt; plot(dnorm(seq(-10, 10), 0, 2)) 简单地画一个 dnorm() 函数生成的图，就可以看到它表示正态分布的密度函数： rnorm() 可以生成符合某个分布的随机数，pnorm() 计算出正态分布的累积概率函数：\n\u0026gt; rnorm(10, 3, 2) [1] 2.9455919 5.6487038 0.9341445 2.7948578 -1.5562341 1.6078429 [7] 0.4516596 3.3077327 5.6856195 3.8256133 \u0026gt; pnorm(10, 3, 2) [1] 0.9997674 其他分布也同理，只需要加上d，p，r即可。\n作图 使用 par() 完成 subplot：\n\u0026gt; par(mfrow=c(2,2)) \u0026gt; hist(trees$Height) \u0026gt; boxplot(trees$Height) \u0026gt; hist(trees$Volume) \u0026gt; boxplot(trees$Volume) 使用 plot() 飞速做出一个最简陋的散点图：\n\u0026gt; plot(trees$Height, trees$Volume) 使用 pairs() 飞速做出 trees 数据集中两两变量之间的关系：\n\u0026gt; pairs(trees) 编写函数 如下方法编写自己的函数：\n\u0026gt; sd \u0026lt;- function(x) sqrt(var(x)) \u0026gt; x \u0026lt;- c(9, 5, 2, 3, 7) \u0026gt; sd(x) [1] 2.863564 \u0026gt; several.plots \u0026lt;- function (x) { par(mfrow=c(3, 1)) hist(x[ ,1]) hist(x[, 2]) plot(x[, 1], x[, 2]) par(mfrow=c(1, 1)) apply(x, 2, summary) } \u0026gt; several.plots(faithful) eruptions waiting Min. 1.600000 43.00000 1st Qu. 2.162750 58.00000 Median 4.000000 76.00000 Mean 3.487783 70.89706 3rd Qu. 4.454250 82.00000 Max. 5.100000 96.00000 其他 如果想退出 R 语言，可以使用q()； 如果不明白某个函数的功能，使用help(func)即可； 使用 library(libraryname) 来载入别的包。 ","id":75,"section":"posts","summary":"在 R 语言 的官方网址标题上写着「The R Project for Statistical Computing」，直接点明了 R 语言是一门主要用于统计计算的程序语言。如果你对统计感兴趣，那么","tags":["R 语言","统计"],"title":"R 语言的那些最最最基础","uri":"https://www.xzywisdili.com/2017/11/2017-11-21-rbasic/","year":"2017"},{"content":"11 月 11 日这天注定对我具有了一定的意义。不是因为它是购物狂欢节或光棍节，而是因为在这一天，我第一次尝试使用 TensorFlow 搭建了一个简单的神经网络。我希望用几篇文章记录这个过程。\n最近在读 Fundamentals of Deep Learning 这本书。我选择它的原因是讲解得通俗易懂，又会直白地点出重点内容。然而当我读到第三章「Implementing Neural Networks in TensorFlow」时，整个人就好像懵了一样。对于一个从来没接触过 TensorFlow 的人来说，是难以通过看代码直接理解 Graph, Session 等等这些新概念的。于是联想到程序员的思维修炼里面提到的「SQ3R 阅读法」，赶紧先放下这本书，到网上找其他关于 TensorFlow 的资料，值得推荐的是：\nTensorFlow 官方文档中文版 TF Girls「TensorFlow Tutorial」修炼指南（这老师很幽默） youtube 地址 bilibili 地址 没想到我居然也能一天完成了一个基础的神经网络（虽然是从下午 1 点到晚上 2 点）。现在到了「SQ3R 阅读法」中的很重要的 Recite（复述）这步———把这个过程写成文章发布到博客里。\n所使用的数据集来自 The Street View House Numbers (SVHN) Dataset，这是一个关于识别街景照片中出现的数字的数据集。\n读取数据 首先下载 Format 2 格式的数据，即 .mat 格式的数据。我们先在 iPython 里面探索一下数据：\nIn[1]: from scipy.io import loadmat as load In[2]: train_data = load('data/train_32x32.mat') ...: test_data = load('data/test_32x32.mat') In[3]: train_data.keys() Out[3]: dict_keys(['__header__', '__version__', '__globals__', 'X', 'y']) In[4]: train_data['X'].shape Out[4]: (32, 32, 3, 73257) In[5]: train_data['y'].shape Out[5]: (73257, 1) 我们把训练集和测试集的样本和标签提取出来。我不想让训练数据集的标签是一个二维数组，所以简单调整一下：\nIn[6]: train_samples = train_data['X'] ...: train_labels = train_data['y'].reshape(train_data['y'].shape[0]) ...: test_samples = test_data['X'] ...: test_labels = test_data['y'].reshape(test_data['y'].shape[0]) In[7]: train_labels.shape Out[7]: (73257,) 转换数据维度 这时 train_samples 的维度是 (32, 32, 3, 73257)，即(图片高，图片宽，通道数，图片数)。很奇怪原始格式把图片数放在了第四个维度上。我们希望 train_samples 的维度是(图片数，图片高，图片宽，通道数)，即 (73257, 32, 32, 3) 的模式。而 train_labels 也需要一些变化。现在的 train_labels 中每个 label 都是图像上对应的数字，如 3，我们希望它变成 [0, 0, 0, 1, 0, 0, 0, 0, 0, 0] 的模式。其中麻烦一点的是 .mat 格式的数据中并没有 0，而是用 10 来表示 0，这需要我们做一点小小的调整：\nimport numpy as np def reformat(samples, labels): # 改变原始数据的形状 # (图片高，图片宽，通道数，图片数) -\u0026gt; (图片数，图片高，图片宽，通道数) # labels 转换为 one-hot encoding [3] -\u0026gt; [0, 0, 0, 1, 0, 0, 0, 0, 0, 0] samples = np.transpose(samples, (3, 0, 1, 2)) one_hot_labels = np.zeros((labels.shape[0], 10)) for i, label in enumerate(labels): index = label if label != 10 else 0 one_hot_labels[i, index] = 1.0 return samples, one_hot_labels 压缩数据通道数，可视化数据 然后再把图片的 RGB 三通道压缩成一通道的灰度模式，同时压缩映射到 -1.0~1.0 上：\ndef normalize(samples): \u0026quot;\u0026quot;\u0026quot; @ samples: numpy array \u0026quot;\u0026quot;\u0026quot; samples = np.add.reduce(samples, keepdims=True, axis=3) / 3.0 return samples / 128.0 - 1.0 改变成灰度图之后，我们现在想看看最初的图片和灰度图。参数 huidu 是用来表示想要展示的是否是灰度图。如果是，则调用关于灰度图的相关函数：\nimport matplotlib.pyplot as plt def inspect(datasets, labels, i, huidu=False): # 显示图片查看 print(labels[i]) if huidu: huidu_shape = (datasets.shape[1], datasets.shape[2]) plt.imshow(datasets[i].reshape(huidu_shape), cmap=\u0026quot;gray\u0026quot;) else: plt.imshow(datasets[i]) plt.show() 我们对训练集和测试集进行调整维度和压缩通道的预处理：\nIn[8]: re_train_samples, re_train_labels = reformat(train_samples, train_labels) ...: re_test_samples, re_test_labels = reformat(test_samples, test_labels) In[9]: final_train_samples = normalize(re_train_samples) ...: final_train_labels = re_train_labels ...: final_test_samples = normalize(re_test_samples) ...: final_test_labels = re_test_labels 现在我们随便选择训练集中的一个样本，看看它的原始图片和灰度图：\ninspect(re_train_samples, train_labels, 31960) inspect(final_train_samples, train_labels, 31960, huidu=True) 下面就是灰度图： 好了，一切完美运行！最后，我们想看看我们训练集和测试集的标签的分布情况，画出数字 0~9 的分布情况的直方图：\nfrom collections import Counter def distribution(labels, name): # 查看 labels 的分布，并画出统计图 count = Counter(labels) y_pos = np.arange(len(count)) y_count = [count[i] if i != 0 else count[10] for i in y_pos] plt.bar(y_pos, y_count, align='center', alpha=0.5) plt.xticks(y_pos, y_pos) plt.ylabel('Count') plt.title(name + ' Label Distribution') plt.show() In[10]: distribution(train_labels, 'Train') In[11]: distribution(test_labels, 'Train') 可以看到 train_labels 和 test_labels 具有相似的分布结构，说明训练集和测试集的划分还算合理，我们可以接下来继续用。\n至此，我们的预处理就结束了。我们把这里所有的代码整理为 load.py 以供接下来的神经网络使用：\nfrom scipy.io import loadmat as load import matplotlib.pyplot as plt import numpy as np from collections import Counter # 使用 tensorflow 实现图像识别 def load_data(): train_data = load('data/train_32x32.mat') test_data = load('data/test_32x32.mat') # extra_data = load('data/extra_32x32.mat') return train_data, test_data def reformat(samples, labels): # 改变原始数据的形状 # (图片高，图片宽，通道数，图片数) -\u0026gt; (图片数，图片高，图片宽，通道数) # labels 转换为 one-hot encoding [3] -\u0026gt; [0, 0, 0, 1, 0, 0, 0, 0, 0, 0] samples = np.transpose(samples, (3, 0, 1, 2)) one_hot_labels = np.zeros((labels.shape[0], 10)) for i, label in enumerate(labels): index = label if label != 10 else 0 one_hot_labels[i, index] = 1.0 return samples, one_hot_labels def normalize(samples): \u0026quot;\u0026quot;\u0026quot; 灰度化：(R + G + B) / 3（省内存，加快训练速度） 将图片从 0 ~ 255 映射到 -1.0 ~ 1.0 @ samples: numpy array \u0026quot;\u0026quot;\u0026quot; samples = np.add.reduce(samples, keepdims=True, axis=3) / 3.0 return samples / 128.0 - 1.0 def distribution(labels, name): # 查看 labels 的分布，并画出统计图 count = Counter(labels) y_pos = np.arange(len(count)) y_count = [count[i] if i != 0 else count[10] for i in y_pos] plt.bar(y_pos, y_count, align='center', alpha=0.5) plt.xticks(y_pos, y_pos) plt.ylabel('Count') plt.title(name + ' Label Distribution') plt.show() def inspect(datasets, labels, i, huidu=False): # 显示图片查看 print(labels[i]) if huidu: huidu_shape = (datasets.shape[1], datasets.shape[2]) plt.imshow(datasets[i].reshape(huidu_shape), cmap=\u0026quot;gray\u0026quot;) else: plt.imshow(datasets[i]) plt.show() train_data, test_data = load_data() train_samples = train_data['X'] train_labels = train_data['y'].reshape(train_data['y'].shape[0]) test_samples = test_data['X'] test_labels = test_data['y'].reshape(test_data['y'].shape[0]) # test_samples = extra_data['X'] # test_labels = extra_data['y'] print('Train Data Samples Shape: ', train_samples.shape) print('Train Data Labels Shape: ', train_labels.shape) print('Test Data Samples Shape: ', test_samples.shape) print('Test Data Labels Shape: ', test_labels.shape) # print('Extra Data Samples Shape: ', extra_data['X'].shape) # print('Extra Data Labels Shape: ', extra_data['y'].shape) re_train_samples, re_train_labels = reformat(train_samples, train_labels) re_test_samples, re_test_labels = reformat(test_samples, test_labels) final_train_samples = normalize(re_train_samples) final_train_labels = re_train_labels final_test_samples = normalize(re_test_samples) final_test_labels = re_test_labels num_labels = final_train_labels.shape[1] # 10 image_size = final_train_samples.shape[1] # 32 num_channel = final_train_samples.shape[3] # 1 if __name__ == '__main__': # See some pictures inspect(re_train_samples, train_labels, 1, huidu=False) # See some gray pictures inspect(final_train_samples, train_labels, 1, huidu=True) # See the distribution of the labels distribution(train_labels, 'Train') distribution(test_labels, 'Test') 结语 你可能心里说：「骗子，你根本没写 TensorFlow 的内容」。嘿嘿，先别打我，我们接下来会把神经网络的相关代码写在另一个程序 network.py 里面，在那里就需要 TensorFlow 的内容了。碍于篇幅，不好在一篇文章中写完。关于使用 TensorFlow 构建基础的神经网络，我会在下一篇文章中介绍。\n顺带一提，在写日志的时候，把自己写过的代码再过一遍，我觉得是一种很好的学习方法：）为早日精通 TensorFlow 奋斗！\n","id":76,"section":"posts","summary":"11 月 11 日这天注定对我具有了一定的意义。不是因为它是购物狂欢节或光棍节，而是因为在这一天，我第一次尝试使用 TensorFlow 搭建了一个简单的神经网络。我希望用","tags":["Python","机器学习","TensorFlow"],"title":"光棍节初探 TensorFlow（一）：数据集的预处理","uri":"https://www.xzywisdili.com/2017/11/2017-11-12-tensorflow01/","year":"2017"},{"content":"这个标题其实不是我对我自己的诘问。作为一个医学生，在习惯使用 LaTeX 完成日常的作业或是论文时，总会有几个同学凑上前来，好奇地问这是个什么样的写作工具。在我做一番简要的介绍后，他们或许会表露出感兴趣，或称赞的神情，然后问下一个重复了很多遍的问题：为什么要使用 LaTeX 呢？\n小众？看起来很高深？这些当然不是使用 LaTeX 的理由了。\n说起开始使用 LaTeX 的缘起，也有点人云亦云的意味。在刷知乎的时候看到了相关问题，了解这样一种排版工具是学术界的「标配」的时候，我想着那为什么不试试呢？\n我在面对一些新东西的时候，喜欢自己亲自上手试试。于是当即就在 Atom 里配置好了 Latex 的环境。从最早的数学建模竞赛的论文到这次的小作业，我总共使用 Latex 完成了 6 篇文章了，也想谈谈自己的体会：\n易于上手的难度\n在学习新东西的时候，总有人叫难。我觉得真的难的是对自然领域新规则的探索，比如物理定律，生物化学反应等。像 Latex，编程语言这些由人自己创造出来的工具，难度和上面提到差一个量级。在互联网大量入门教程，相关文章和工具包的储备下，我觉得只要半小时就可以完成自己的第一篇文档了。\n参考链接：\n自学 LaTeX 可以读什么书入门？ LaTeX 开源小屋 美观\nLaTeX 是诞生于 20 世纪 70 年代末到 80 年代初的一款计算机排版软件。计算机科学家高德纳教授在修订其巨著《计算机程序设计艺术》时，为了排版这本书产生的。他曾这样说：\n我不知道怎么办。我花了整整 15 年写这些书，可要是这么难看，我就再也不写了。我怎么能对这样的作品引以为豪呢？\n我一想象到他在说这段话的神情和样貌的时候就想笑，同时也能部分体会到他的感受。谁不希望写着自己名字的东西能够无比精致美观呢？\n也就是说，LaTeX 从诞生之初的定位就是排版。它就好像一个负责排版的管家，让你可以更加关注内容本身，其他东西统统不用你在意。LaTeX 有其内在的排版哲学，初次使用的时候会觉得编译出来的 pdf 是经过精心设计的艺术品。\n自动化工具\n还有一点让我心动的就是诸如自动化管理参考文献这样的工具。在使用 LaTeX 之前，无论是在 Word 里文献管理还是格式排版，但疲惫不堪。我们不应把大量时间花在排版和格式上，而是应该专注于内容。这也是我倍加推崇 Markdown 的原因。\n创作是一件快乐的事情，让创作出来的东西呈现出美观和整洁的一种状态也应该是简单而清晰的。使用 LaTeX 会给我一种省心的感觉。\n这篇博文并没有立体地，或是深度地讲解 LaTeX 的方方面面。我只是想象我的一些同学会是这篇文章的可能受众，谈谈我的一些浅显的想法而已。在使用过 LaTeX 完成文章后实在有一种无比舒畅的感觉，这值得使用一篇博文来分享。\n","id":77,"section":"posts","summary":"\u003cp\u003e这个标题其实不是我对我自己的诘问。作为一个医学生，在习惯使用 LaTeX 完成日常的作业或是论文时，总会有几个同学凑上前来，好奇地问这是个什么样的写作工具。在我做一番简要的介绍后，他们或许会表露出感兴趣，或称赞的神情，然后问下一个重复了很多遍的问题：为什么要使用 LaTeX 呢？\u003c/p\u003e","tags":["随想","LaTeX"],"title":"为什么要使用 LaTeX 呢？","uri":"https://www.xzywisdili.com/2017/10/2017-10-23-why-latex/","year":"2017"},{"content":"如果你经常读数据科学领域的文章的话，你可能会偶然发现 FiveThirtyEight 上的内容，然后被他们惊艳的图表迷住。于是你自己也想制作如此出色的可视化作品，于是去 Quora 和 Reddit 上问怎么做。你收到了几个回答，但是这些回答都很模糊。你还是不知道怎么搞定这样的图表。\n在这篇博文中，我会手把手地帮你。通过使用 Python 的 matplotlib 和 pandas 库，我们会发现复制出 FTE 可视化作品的核心部分是多么轻松写意。\n这是我们最初的图：\n在这篇文章的结束，我们会做到这样：\n为了跟上，你需要至少了解一些 Python 的基础知识。如果你知道方法和属性之间的区别，那我们就可以开始了。\n介绍数据集 我们将要处理的数据集展现的事从 1970 年到 2011 年在美国授予女性的学位比例。我们使用的数据集是数据科学家 Randal Olson 从国家教育统计中心采集的。\n如果你想通过自己写代码来学习，你可以从 Randal 的博客下载数据集。如果想节省时间的话，你可以跳过下载文件，直接把链接甩给 pandas 的 read_csv() 函数。在下面的代码中，我们做了：\n导入 pandas 模块 把数据集的链接通过字符串保存在变量 direct_link 中 通过 read_csv() 读取数据，并把内容保存在 women_majors 使用 info() 方法展示数据集的基本信息，了解行数和列数，同时找一找有没有缺失的值 使用 head() 方法显示出数据集的前 5 行可以帮助我们更好地理解数据集的结构 import pandas as pd direct_link = 'http://www.randalolson.com/wp-content/uploads/percent-bachelors-degrees-women-usa.csv' women_majors = pd.read_csv(direct_link) print(women_majors.info()) women_majors.head() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 42 entries, 0 to 41 Data columns (total 18 columns): Year 42 non-null int64 Agriculture 42 non-null float64 Architecture 42 non-null float64 Art and Performance 42 non-null float64 Biology 42 non-null float64 Business 42 non-null float64 Communications and Journalism 42 non-null float64 Computer Science 42 non-null float64 Education 42 non-null float64 Engineering 42 non-null float64 English 42 non-null float64 Foreign Languages 42 non-null float64 Health Professions 42 non-null float64 Math and Statistics 42 non-null float64 Physical Sciences 42 non-null float64 Psychology 42 non-null float64 Public Administration 42 non-null float64 Social Sciences and History 42 non-null float64 dtypes: float64(17), int64(1) memory usage: 6.0 KB None 除了 Year 这一列之外，其他每一列都指明了学士学位的具体科目。而在这些列之下的数据点代表了授予的学士学位中女性所占的百分比。当然每一行就表示的是在该行特指的年份之下，授予的学士学位中女性所占的百分比。\n正如之前讲过的，我们的数据是从 1970 年到 2011 年的。为了确认这个时间段无误，我们通过 tail() 方法来看看数据集的最后 5 行：\nwomen_majors.tail() FiveThirtyEight 插图的上下文 几乎每一个 FTE 图表都是一篇文章的一部分。这些图表通过展示一个小故事或者一个新观点来补足文字内容。我们在复制我们的图表的时候需要牢记这一点。\n为了避免这篇教程跑题，我们就假装我们已经写了一篇关于在美国教育中性别地位演变的文章的绝大部分了。我们现在需要制作一张图标来让读者对美国教育中的学士学位授予关于性别差异的演变有一个直观的感受，并且告诉读者在 1970 年，女性的地位确实很低。我们把最低阈值设定在 20%，现在我们想画出每一个在 1970 年毕业学士中女性比例低于 20% 的那些学科。\n让我们先找到那些学科。在下面的代码中，我们要：\n使用 .loc 一个基于标签的索引利器来： 选择第一行（就是 1970 年对应的那行） 在第一行之中找到值小于 20 的那些列；Year 的部分也应该检查，但是显然不用和 20 相比较 把选择出的部分保存在 under_20 under_20 = women_majors.loc[0, women_majors.loc[0] \u0026lt; 20] under_20 输出为：\nAgriculture 4.229798 Architecture 11.921005 Business 9.064439 Computer Science 13.600000 Engineering 0.800000 Physical Sciences 13.800000 Name: 0, dtype: float64 使用 matplotlib 的默认样式 让我们开始制作曲线图。我们先看一看我们使用默认样式做出来是什么样子，在下面的代码中，我们会：\n在 Jupyter 中用 %matplotlib 来让 Jupyter 和 matplotlib 能够正常工作，在后面加上 inline 能让画出的图直接在 Jupyter notebook 中显示。 使用 plot() 方法来画出 women_majors 的初始版本。我们将传入这些参数： x - 指明 women_majors 中用来做 x 轴的那一列； y - 指明 women_majors 中用来做 y 轴的那一列；我们将使用 under_20 下储存在 .index 中的下标索引； figsize - 设定图片的尺寸大小（设定格式是(width, height)，单位是英寸）。 把画出来的图形对象存储进 under_20_graph，然后打印出这个对象的类型，可以发现 pandas 实际上使用的是 matplotlib 的对象。 %matplotlib inline under_20_graph = women_majors.plot(x = 'Year', y = under_20.index, figsize = (12,8)) print('Type:', type(under_20_graph)) 使用 matplotlib 的 FTE 样式 上面的图具有一定的特征，比如宽度和线条的颜色，字体大小和 y 轴的标签，不显示网格等等。matplotlib 的默认样式包括了所有这些特征。\n稍稍插入一段话，在这篇博文中我们将会使用一点术语。如果你有任何疑惑，可以去下面的网站找到解答。\nSource: Matplotlib.org\n除了默认样式之外，matplotlib 也提供了几种我们可以直接来用的内建样式。让我们看看可用的样式列表：\n导入 matplotlib.style 模块 探索 matplotlib.style.available 的内容，里面包括了所有可用的内建样式 import matplotlib.style as style style.available ['seaborn-deep', 'seaborn-muted', 'bmh', 'seaborn-white', 'dark_background', 'seaborn-notebook', 'seaborn-darkgrid', 'grayscale', 'seaborn-paper', 'seaborn-talk', 'seaborn-bright', 'classic', 'seaborn-colorblind', 'seaborn-ticks', 'ggplot', 'seaborn', '_classic_test', 'fivethirtyeight', 'seaborn-dark-palette', 'seaborn-dark', 'seaborn-whitegrid', 'seaborn-pastel', 'seaborn-poster'] 你可能已经发现有一个样式名字叫「fivethirtyeight」。让我们使用它看看会得到什么效果。只需要从 matplotlib.style 模块下使用函数 use()，我们就可以生成我们想要的相同曲线图了。\nstyle.use('fivethirtyeight') women_majors.plot(x = 'Year', y = under_20.index, figsize = (12,8)) 确实改变了许多！对比我们的第一张图，我们可以发现这张曲线图的背景颜色不同，出现了网格线，没有明显的毛刺，字体大小和坐标轴刻度等等也有了很大的不同。\n你可以在 这里 读到 FTE 样式的更多介绍。里面会有使用这个样式时的具体代码讲解。在 这里 可以看到这个样式作者 Cameron David-Pilon 关于它的更多讨论。\nmatplotlib 的 FTE 样式的限制 总而言之，使用 matplotlib 提供的 FTE 样式让我们离目标近了一步。然而，还有许多事需要我们完成。让我们检查一下一个简单的 FTE 图表示例，看看我们还需要补充什么。\nSource: FiveThirtyEight\n通过比较上面的图示和我们所完成的那张图，我们还需要：\n增加一个标题和副标题 移除那个方块型的图例，而在相关的曲线旁边增加标签，并且这些标签旁边的网格线变成透明的 在图的底部增加一个签名，包括这张图的作者和数据源 其他一些小调整： 增加刻度标记的字体大小 在 y 轴的某个主要刻度标记上增加「%」标志 移除 x 轴的标签 加粗 y = 0 的水平线 在 y 轴的刻度标记旁边增加一条额外的网格线 增加图的侧边距 为了节省我们制图的时间，避免在开始的时候添加标题，副标题或者其他文字非常重要。在 matplotlib 中，一段文字是通过给定的 x 和 y 坐标放置的，我们会在之后的内容里看到。为了从细节上复制 FTE 的制图风格，我们需要把 y 轴刻度标记和标题与副标题竖直对齐。我们不希望看到当我们设定好对齐之后，却通过增加刻度标签字体大小的时候错乱了，到时候还得重新调整标题和副标题的位置。\n定制刻度标记 我们先从增加刻度标记的字体大小开始，在下面的代码中，我们会：\n使用之前的代码画图，把图像对象保存在 fte_graph 中。这样可以帮助我们探知它的一些属性，并且重复使用和修改这个对象。 使用 tick_params() 方法增加所有刻度标记的字体大小，各种参数如下： axis - 指明我们想要修改的坐标轴是哪条，在这里我们想要两条都改； which - 指明需要改哪里的刻度标记（主要的/次要的），如果不理解可以看上文给出的链接； labelsize - 设定刻度标记的字体大小 fte_graph = women_majors.plot(x = 'Year', y = under_20.index, figsize = (12,8)) fte_graph.tick_params(axis = 'both', which = 'major', labelsize = 18) 你可能已经发现我们这次没有使用 style.use('fivethirtyeight')。这是因为 matplotlib 样式的首选项已经在上文第一次提到 style 的时候全局更改为 fivethirtyeight 了。之后的所有图都会继承这个样式。如果你想要回到默认的状态，可以使用 style.use('default')。\n我们现在在原有的基础上继续调整 y 轴的刻度标记：\n我们对 y 轴可以看到的最高标记 50 增加一个「%」标记 我们还需要对其他的几个刻度标记增加一个空格字符，保证它们可以和「50%」完美对齐 为了改变 y 轴的刻度标记，我们使用了 set_yticklabels() 这个方法。从下面的代码中，你可以看到 label 这个参数接受一个多种数据类型混合的列表，而并不要求固定数量或形式的标记。\n# Customizing the tick labels of the y-axis fte_graph.set_yticklabels(labels = [-10, '0 ', '10 ', '20 ', '30 ', '40 ', '50%']) print('The tick labels of the y-axis:', fte_graph.get_yticks()) # -10 and 60 are not visible on the graph 输出结果：\nThe tick labels of the y-axis: [-10. 0. 10. 20. 30. 40. 50. 60.] 对 y = 0 处的水平线加粗 我们现在需要加粗 y 轴坐标为 0 时的水平线。使用 axhline() 方法可以增加一条新的水平网格线，覆盖住原有的水平轴。axhline() 的参数有：\ny - 指明水平线的 y 坐标； color - 指明线的颜色； linewidth - 设定线的宽度； alpha - 控制线的透明度，但我们在这里是用来控制黑色的强度；alpha 值的范围从 0 到 1（完全透明到完全不透明） # Generate a bolded horizontal line at y = 0 fte_graph.axhline(y = 0, color = 'black', linewidth = 1.3, alpha = .7) 增加一个额外的垂直线 如我们之前所讲，我们需要在紧靠 y 轴刻度标记旁边增加一条竖直的网格线。要加一条线的话，就需要稍微修改一下 x 轴的数据范围，把区间稍微往左边扩大一点，这样才有空间添加我们想要的新线。\n在下面，我们使用 set_xlim() 方法，两个参数 left 和 right 的意义不言自明。\n# Add an extra vertical line by tweaking the range of the x-axis fte_graph.set_xlim(left = 1969, right = 2011) 生成一条签名栏 FTE 图像示例的签名栏有着很明显的几个特征：\n位置位于图片底部 作者的姓名位于签名栏的左边 数据源位于签名栏的右边 文字的颜色是浅灰色（和图片背景的颜色一样），背景是深灰色 作者姓名和数据源之间的空间背景也是深灰色 增加这样一个签名栏好像很难，但我们稍微动动脑筋，就可以轻松地完成。\n我们增加一个单独的文字段，将其文字颜色和背景颜色分别设定为浅灰色和深灰色。我们可以把作者姓名和数据源放在一个文字段中，然后把它们间隔开，一个放在左边，一个放在右边。中间的空格是不会显示出来的，所以保持着背景颜色。\n我们还需要一些刻个来让作者姓名和数据源对齐，这些都会在下个代码块里面看到。\n也是时候移除 x 轴的刻度标记了！这样，我们可以看到签名栏在整体的曲线图中达到了什么样的视觉效果。在下一段代码中，我们要：\n对 fte_graph.xaxis.label 使用 set_visible() 方法移除 x 轴的刻度标记，把这个属性的值改成 False 通过我们上面讨论过的方法增加一个文字片段。我们所使用的 text() 方法有以下几个参数： x - 指明文字段的 x 坐标； y - 指明文字段的 y 坐标； s - 指明要添加的文字； fontsize - 设定文字的字体大小； color - 指明文字的颜色；下面我们使用这个值的格式是 16 进制的，这个颜色和整张图的背景颜色完全一致； backgroundcolor - 设定文字段的背景颜色 # Remove the label of the x-axis fte_graph.xaxis.label.set_visible(False) # The signature bar fte_graph.text(x = 1965.8, y = -7, s = ' ©DATAQUEST Source: National Center for Education Statistics ', fontsize = 14, color = '#f0f0f0', backgroundcolor = 'grey') 这块文字段的 x 和 y 坐标是通过了一段漫长的试验和错误之后才确定的。把坐标设定为浮点数能够让你更加精准地控制文字的位置。\n值得一提的是，我们通过调整签名栏的位置的同时，也在视觉上增加了侧边距。降低文字段的 x 坐标增加左边距，而在作者名字和数据源之间增加空格增加了右边距。\n另一种签名栏 还有另一种类型的签名栏：\nSource: FiveThirtyEight\n这种签名栏也可以简单地复制出来，我们只需要改变一些文字颜色和背景颜色就行了。\n我们通过增加一块包括许多下划线的「_」文字段来达到一条线的视觉效果。你可能想问为什么我们不使用 axhline() 来画一条我们想要的水平线。那是因为添加一条新的线会让整体的网格也向下扩张，这不是我们想要的效果。\n我们还可以试着加一个箭头，然后把指针去掉获得一条线，但是明显，「下划线」方案更加简单。\n在下一段代码中，我们将实现这些元素。这里用到的方法和参数，相信读者朋友们已经很熟悉了。\n# The other signature bar fte_graph.text(x = 1967.1, y = -6.5, s = '________________________________________________________________________________________________________________', color = 'grey', alpha = .7) fte_graph.text(x = 1966.1, y = -9, s = ' ©DATAQUEST Source: National Center for Education Statistics ', fontsize = 14, color = 'grey', alpha = .7) 增加一个标题和副标题 如果你检查许多 FTE 的图表，你会发现它们的标题和副标题遵循着这样的模式：\n标题文字几乎都需要一个副标题补充 一副特定图示的标题需要考虑到上下文的角度。它不会太复杂，太技术性，而是简单地表达出一个清晰的观点。标题也从来不会出现一个中立的情绪。就如同上文中「Fandango」的那幅图，我们可以看到一个简单的，饱含情绪的标题：「Fandango LOVES Movies」，而不是一个苍白的「电影评分类型的分布情况」这样的标题 副标题提供的事这幅图的技术性信息，它常常会使坐标轴的标签显得多余。我们在完成副标题时需要注意，因为我们已经把 x 轴的标签删掉了 从视觉上讲，标题和副标题有不一样的字重，而且是左对齐的（不像大多数标题是居中的），并且和 y 轴的主要刻度标记左对齐 让我们牢记以上几点，为这幅图增添一个标题和副标题。在下面的代码块中，我们将要：\n使用 text() 方法增添一个标题和副标题。如果你已经有使用 matplotlib 的经验的话，你可能好奇为什么我们不使用 title() 和 suptitle() 这两个方法。这是因为这两个方法在精确移动文字位置上显得无比笨拙。这次唯一的新参数是 weight，我们使用它来加粗标题。 # Adding a title and a subtitle fte_graph.text(x = 1966.65, y = 62.7, s = \u0026quot;The gender gap is transitory - even for extreme cases\u0026quot;, fontsize = 26, weight = 'bold', alpha = .75) fte_graph.text(x = 1966.65, y = 57, s = 'Percentage of Bachelors conferred to women from 1970 to 2011 in the US for\\nextreme cases where the percentage was less than 20% in 1970', fontsize = 19, alpha = .85) 如果你好奇的话，原本的 FTE 图示中使用的字体是一个付费字体「Decima Mono」。所以我们还是使用 matplotlib 的默认字体，看起来也不错。\n增加对色盲友好的颜色 现在，那块笨重的方形图例还在。我们需要彻底丢掉它，在每条曲线旁边增加相应的标签。每条曲线有一种特定的颜色，并且有一个对应于每条曲线的解释标签。\n首先，我们改变一下曲线的颜色，添加一些对色盲友好的颜色：\n我们所编写的面向色盲友好的颜色的 RGB 值出自上面这张图。作为边注，我们要避免使用黄色，因为这种颜色在灰色背景下的可读性非常差。\n完成之后，我们把 RGB 传入 plot() 方法中的颜色参数。注意 matplotlib 要求 RGB 参数在 0-1 的范围之间，所以我们把每个值除以 255（最大的 RGB 值）。\n# Colorblind-friendly colors colors = [[0,0,0], [230/255,159/255,0], [86/255,180/255,233/255], [0,158/255,115/255], [213/255,94/255,0], [0,114/255,178/255]] # The previous code we modify fte_graph = women_majors.plot(x = 'Year', y = under_20.index, figsize = (12,8), color = colors) # The previous code that remains the same fte_graph.tick_params(axis = 'both', which = 'major', labelsize = 18) fte_graph.set_yticklabels(labels = [-10, '0 ', '10 ', '20 ', '30 ', '40 ', '50%']) fte_graph.axhline(y = 0, color = 'black', linewidth = 1.3, alpha = .7) fte_graph.xaxis.label.set_visible(False) fte_graph.set_xlim(left = 1969, right = 2011) fte_graph.text(x = 1965.8, y = -7, s = ' ©DATAQUEST Source: National Center for Education Statistics ', fontsize = 14, color = '#f0f0f0', backgroundcolor = 'grey') fte_graph.text(x = 1966.65, y = 62.7, s = \u0026quot;The gender gap is transitory - even for extreme cases\u0026quot;, fontsize = 26, weight = 'bold', alpha = .75) fte_graph.text(x = 1966.65, y = 57, s = 'Percentage of Bachelors conferred to women from 1970 to 2011 in the US for\\nextreme cases where the percentage was less than 20% in 1970', fontsize = 19, alpha = .85) 把图例样式变成线条边的标签 最后，我们通过 text() 方法在每条曲线旁边增加一个颜色一致的标签。唯一的新参数是 rotation，我们用它旋转每个标签，这样看起来更加优雅。\n这里还需要一点小技巧，只需要给每个图例标签设定好背景颜色，就可以让它们旁边的网格线变得透明了。\n我们只需要把先前代码 plot() 方法中的 legend 参数改为 False 就可以让默认的图例消失不见了。而由于已经存储过了颜色列表，我们也不用再费心了。\n# The previous code we modify fte_graph = women_majors.plot(x = 'Year', y = under_20.index, figsize = (12,8), color = colors, legend = False) # The previous code that remains unchanged fte_graph.tick_params(axis = 'both', which = 'major', labelsize = 18) fte_graph.set_yticklabels(labels = [-10, '0 ', '10 ', '20 ', '30 ', '40 ', '50%']) fte_graph.axhline(y = 0, color = 'black', linewidth = 1.3, alpha = .7) fte_graph.xaxis.label.set_visible(False) fte_graph.set_xlim(left = 1969, right = 2011) fte_graph.text(x = 1965.8, y = -7, s = ' ©DATAQUEST Source: National Center for Education Statistics ', fontsize = 14, color = '#f0f0f0', backgroundcolor = 'grey') fte_graph.text(x = 1966.65, y = 62.7, s = \u0026quot;The gender gap is transitory - even for extreme cases\u0026quot;, fontsize = 26, weight = 'bold', alpha = .75) fte_graph.text(x = 1966.65, y = 57, s = 'Percentage of Bachelors conferred to women from 1970 to 2011 in the US for\\nextreme cases where the percentage was less than 20% in 1970', fontsize = 19, alpha = .85) # Add colored labels fte_graph.text(x = 1994, y = 44, s = 'Agriculture', color = colors[0], weight = 'bold', rotation = 33, backgroundcolor = '#f0f0f0') fte_graph.text(x = 1985, y = 42.2, s = 'Architecture', color = colors[1], weight = 'bold', rotation = 18, backgroundcolor = '#f0f0f0') fte_graph.text(x = 2004, y = 51, s = 'Business', color = colors[2], weight = 'bold', rotation = -5, backgroundcolor = '#f0f0f0') fte_graph.text(x = 2001, y = 30, s = 'Computer Science', color = colors[3], weight = 'bold', rotation = -42.5, backgroundcolor = '#f0f0f0') fte_graph.text(x = 1987, y = 11.5, s = 'Engineering', color = colors[4], weight = 'bold', backgroundcolor = '#f0f0f0') fte_graph.text(x = 1976, y = 25, s = 'Physical Sciences', color = colors[5], weight = 'bold', rotation = 27, backgroundcolor = '#f0f0f0') 下一步 这样，我们的图就可以发表了！ That’s it, our graph is now ready for publication!\n做一个小总结，我们开始的时候使用 matplotlib 的默认样式，然后我们一步步把这张图修改成了 FTE 的官方水平：\n使用了 matplotlib 内建的 fivethirtyeight 样式 增加了个性化的标题和副标题 增加了一个标题栏 去掉了默认的图例，增加了曲线旁边的图例 其他的一些小调整：定制了刻度标记，加粗了 y = 0 的水平线，在刻度标记旁边增加了竖直线，去除了 x 轴的标签，同时增加了 y 轴的边距。 你可以把下面这几件事作为学习之后的复习：\n使用其他科目学士数据生成一个相似的图表 生成一个其他类型的 FTE 图表：直方图，散点图等等 探索matplotlib gallery，找到一些新的元素来丰富你的 FTE 图表（比如插入图片，增加箭头等等）。插入图片能让你的图表到达一个新的层次： 原作者：Alexandru Olteanu 原文链接：How to Generate FiveThirtyEight Graphs in Python\n","id":78,"section":"posts","summary":"\u003cp\u003e如果你经常读数据科学领域的文章的话，你可能会偶然发现 \u003ca href=\"http://fivethirtyeight.com\"\u003eFiveThirtyEight\u003c/a\u003e 上的内容，然后被他们惊艳的图表迷住。于是你自己也想制作如此出色的可视化作品，于是去 Quora 和 Reddit 上问怎么做。你收到了几个回答，但是这些回答都很模糊。你还是不知道怎么搞定这样的图表。\u003c/p\u003e","tags":["Python","matplotlib","Python"],"title":"「翻译」如何用 Python 画出像 FiveThirtyEight 那么棒的图表","uri":"https://www.xzywisdili.com/2017/10/2017-10-15-trans01/","year":"2017"},{"content":"这是 Python 解决算法问题的第一期。我想以后每期写出三道题。本人是医学生，没有上过任何数据结构和算法课，对于解决算法题也纯属个人的课余兴趣爱好。还希望各位真正的大牛不吝赐教。我会继续坚持下去的。\n设计一个有 getMin 功能的栈 题目：实现一个特殊的栈，在实现栈的基本功能的基础上，再实现返回栈中最小元素的操作。 要求：\npop，push，get_min 操作的时间复杂度都是 O(1)。 设计的栈类型可以使用线程的栈结构。 首先先使用 Python 实现一个简单的栈：\nclass Stack: __slots__ = ('__items') def __init__(self): self.__items = [] def is_empty(self): return self.__items == [] def peek(self): return self.__items[-1] def size(self): return len(self.__items) def push(self, new_value): self.__items.append(new_value) def pop(self): return self.__items.pop() 这个问题很简单，解决方法就是使用两个栈，一个用来存放数据，一个用来记录最小值即可。 在 push 新数据的时候，需要和存放最小值的栈 stack_min 栈顶的数据相比较。如果新数据比它小或者等于它，那么也把新数据 push 进 stack_min 的栈顶就可以了。 在 pop 的时候，需要检查输出的数据是否也存在在 stack_min 的栈顶，如果也存在的话，说明这个数据是栈内的最小值，也需要把它从 stack_min 中 pop 出去。\nclass MyStack(object): def __init__(self): self.stack_data = Stack() self.stack_min = Stack() def push(self, new_value): self.stack_data.push(new_value) if self.stack_min.is_empty(): self.stack_min.push(new_value) elif new_value \u0026lt;= self.get_min(): self.stack_min.push(new_value) def pop(self): if self.stack_data.is_empty(): raise Exception(\u0026quot;Your stack is empty!\u0026quot;) value = self.stack_data.pop() if value == self.get_min(): self.stack_min.pop() return value def get_min(self): if self.stack_min.is_empty(): raise Exception(\u0026quot;Your stack is empty!\u0026quot;) return self.stack_min.peek() 以下是用来测试的代码：\nmy_test_stack1 = MyStack() my_test_stack1.push(3) assert my_test_stack1.get_min() == 3 my_test_stack1.push(4) assert my_test_stack1.get_min() == 3 my_test_stack1.push(5) assert my_test_stack1.get_min() == 3 my_test_stack1.push(1) assert my_test_stack1.get_min() == 1 my_test_stack1.push(2) assert my_test_stack1.get_min() == 1 print(my_test_stack1.pop()) # 输出：2 assert my_test_stack1.get_min() == 1 print(my_test_stack1.pop()) # 输出：1 assert my_test_stack1.get_min() == 3 my_test_stack2 = MyStack() my_test_stack2.pop() # 输出：Exception: Your stack is empty! 另一个稍稍不同的解决问题的方法是，当新 push 的数据比 stack_min 栈顶数据大时，再在 stack_min 的栈顶复制一个同样的栈顶值，这样就可以直接 pop 而不用检查这个值的大小了。这个方法会比上述方法在 push 时稍费空间，但在 pop 时稍省时间。但两种方法的空间和时间复杂度都是相同的。\n仅用递归函数和栈操作逆序一个栈 题目：一个栈依次压入1，2，3，4，5，那么从栈顶到栈底分别为5，4，3，2，1。将这个栈转置后，从栈顶到栈底为1，2，3，4，5，也就是实现栈中元素的逆序，但是只能用递归函数来实现，不能用其他数据结构。\n逆序一个栈的步骤显而易见是先依次取出位于栈底的数据，然后再按照相反的顺序把数据压入即可了。而递归的思想在于，我先取出栈底的元素，然后把剩下的栈逆序，然后再把刚取出来的元素压进栈顶。同样，这道题的解答中使用的栈的实现与上面一题相同。\ndef get_last_element(stack): result = stack.pop() if stack.is_empty(): return result else: last = get_last_element(stack) stack.push(result) return last def reverse(stack): if stack.is_empty(): return item = get_last_element(stack) reverse(stack) stack.push(item) 测试用的代码：\nmy_test_stack = Stack() my_test_stack.push(1) my_test_stack.push(2) my_test_stack.push(3) reverse(my_test_stack) assert my_test_stack.pop() == 1, \u0026quot;现在栈顶应该是最先 push 进去的 1\u0026quot; assert my_test_stack.pop() == 2 assert my_test_stack.pop() == 3 生成窗口最大值数据 题目：有一个整形数组 arr 和一个大小为 w 的窗口从数组的最左边滑到最右边，窗口每次向右边滑一个位置。比如数组为[4, 3, 5, 4, 3, 3, 6, 7]，就会有： [4 3 5] 4 3 3 6 7 4 [3 5 4] 3 3 6 7 4 3 [5 4 3] 3 6 7 4 3 5 [4 3 3] 6 7 4 3 5 4 [3 3 6] 7 4 3 5 4 3 [3 6 7] 最大值依次是[5, 5, 5, 4, 6, 7] 请实现一个函数，输入为数组 arr 和窗口大小 w，输出为各窗口的最大值数组。\n看到题目，我想都没想就写好了第一个函数：\ndef get_max_in_window_v1(arr, w): if arr == [] or w \u0026lt; 1 or len(arr) \u0026lt; w: raise Exception(\u0026quot;Wrong arr length or window size.\u0026quot;) max_result = [] for i in range(len(arr) - w + 1): max_result.append(max(arr[i:i+w])) return max_result 测试结果也正确，但是总觉得有些不好，仔细一看它的时间复杂度是 O(N * W) （因为循环里面的 max 函数的复杂度为 O(W) ）。应该有更快的方法。于是想可以在每读取一个数组的时候，更新窗口最大值。你想节省时间，就必须得付出空间。于是乎使用另一个数组来存储最大值的下标，不断更新。\ndef get_max_in_window_v2(arr, w): if arr == [] or w \u0026lt; 1 or len(arr) \u0026lt; w: raise Exception(\u0026quot;Wrong arr length or window size.\u0026quot;) qmax = [] max_result = [] for index, item in enumerate(arr): while len(qmax) \u0026gt; 0 and arr[qmax[-1]] \u0026lt;= item: qmax.pop() qmax.append(index) if qmax[0] == index - w: qmax.pop(0) if index \u0026gt;= w-1: max_result.append(arr[qmax[0]]) return max_result 主要就是在读取 arr 中的元素的时候，把最大值的下标存放在 qmax 的最左端。而小的依次往右排（你不能忽视那些相对小的数据，因为窗口在不停地向右移动。）这样，每读取一个元素，先更新 qmax，再把窗口最大值存放进 result，步骤非常清晰。在 Python 中 qmax.pop(0) 这条语句的时间复杂度是$O(W)$，是这个解法的一个缺憾。所以改天我想我需要写一个 Python 的链表实现，这样就比较好了。\n测试通过：\nprint(get_max_in_window_v2(arr, 3)) # 输出：[5, 5, 5, 4, 6, 7] 感谢阅读。\n","id":79,"section":"posts","summary":"\u003cp\u003e这是 Python 解决算法问题的第一期。我想以后每期写出三道题。本人是医学生，没有上过任何数据结构和算法课，对于解决算法题也纯属个人的课余兴趣爱好。还希望各位真正的大牛不吝赐教。我会继续坚持下去的。\u003c/p\u003e","tags":["算法","Python"],"title":"用 Python 解决数据结构与算法题 01","uri":"https://www.xzywisdili.com/2017/10/2017-10-05-algorithm01/","year":"2017"},{"content":"在国庆放假的期间，花了大概半个晚上的时间，使用 hexo 配置好了自己的个人博客。我觉得博客是一个很好的刺激自己不断学习和保持记录习惯的帮手。我也是在身边同学里第一个开通博客的。\n在这个过程中也产生了两点感想：\n科技不停进步，就计算机和互联网方面完成一件事是越来越简单了。以前如果想写网页，建网站，不可能不去学习 HTML 语言，CSS 以及 JavaScript 等等，而现在越来越多新工具的出现能让你在半个晚上就制作出美观的网页了。\n以前看到别人精美的博客的时候，总觉得是一件特别了不起的事情，同时也是一件很难办到的事情。但在基于很多前辈写博客的建议，自己也尝试去网上查找教程搭建，之后觉得这件事其实也没有那么难。很多看起来困难的事情，真正动手去做也许没有那么难。\n这个博客会不定期更新自己的学习笔记，成长过程和生活感悟等等。我更期待多年之后自己回看时候吃惊的那一刻。\n使用 Hexo 进行博客创作的流程 创建一个草稿 $ hexo new draft \u0026quot;new artical\u0026quot; 或者，在 _config.yml 的 Writing 下找到 default_layout，将 post 改为 draft，然后直接使用：\n$ hexo new \u0026quot;new artical\u0026quot; 这样 Hexo 就会自动在 source 文件夹下的 draft 中创建一个等待你编辑的 Markdown 文件了。我个人习惯先在草稿内完成创作，所以使用的是后者。如果想要在生成的页面上看到自己的草稿的话，可以键入命令：\n$ hexo server --draft 完成创作后发表 当在草稿中完成文章后，只需要一行命令：\n$ hexo publish \u0026quot;new artical\u0026quot; 这样 Hexo 会自动把 draft 文件夹里面的 Markdown 文件移动到 post 文件夹里面。随后使用那几个大家都知道的命令生成静态网页，更新到博客里面就好了。\n更多信息推荐访问 文档。\n","id":80,"section":"posts","summary":"\u003cp\u003e在国庆放假的期间，花了大概半个晚上的时间，使用 hexo 配置好了自己的个人博客。我觉得博客是一个很好的刺激自己不断学习和保持记录习惯的帮手。我也是在身边同学里第一个开通博客的。\u003c/p\u003e","tags":["Hexo","Blog"],"title":"Hello World","uri":"https://www.xzywisdili.com/2017/10/2017-10-04-helloworld/","year":"2017"}],"tags":[{"title":"Blog","uri":"https://www.xzywisdili.com/tags/blog/"},{"title":"CDISC","uri":"https://www.xzywisdili.com/tags/cdisc/"},{"title":"Excel","uri":"https://www.xzywisdili.com/tags/excel/"},{"title":"Hexo","uri":"https://www.xzywisdili.com/tags/hexo/"},{"title":"LaTeX","uri":"https://www.xzywisdili.com/tags/latex/"},{"title":"Markdown","uri":"https://www.xzywisdili.com/tags/markdown/"},{"title":"matplotlib","uri":"https://www.xzywisdili.com/tags/matplotlib/"},{"title":"Python","uri":"https://www.xzywisdili.com/tags/python/"},{"title":"R 语言","uri":"https://www.xzywisdili.com/tags/r-%E8%AF%AD%E8%A8%80/"},{"title":"R语言","uri":"https://www.xzywisdili.com/tags/r%E8%AF%AD%E8%A8%80/"},{"title":"SAS","uri":"https://www.xzywisdili.com/tags/sas/"},{"title":"TensorFlow","uri":"https://www.xzywisdili.com/tags/tensorflow/"},{"title":"临床数据分析","uri":"https://www.xzywisdili.com/tags/%E4%B8%B4%E5%BA%8A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"title":"人工智能","uri":"https://www.xzywisdili.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"title":"作图","uri":"https://www.xzywisdili.com/tags/%E4%BD%9C%E5%9B%BE/"},{"title":"健康科普","uri":"https://www.xzywisdili.com/tags/%E5%81%A5%E5%BA%B7%E7%A7%91%E6%99%AE/"},{"title":"分享","uri":"https://www.xzywisdili.com/tags/%E5%88%86%E4%BA%AB/"},{"title":"医学预测","uri":"https://www.xzywisdili.com/tags/%E5%8C%BB%E5%AD%A6%E9%A2%84%E6%B5%8B/"},{"title":"博客","uri":"https://www.xzywisdili.com/tags/%E5%8D%9A%E5%AE%A2/"},{"title":"可视化","uri":"https://www.xzywisdili.com/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"title":"周记","uri":"https://www.xzywisdili.com/tags/%E5%91%A8%E8%AE%B0/"},{"title":"城市","uri":"https://www.xzywisdili.com/tags/%E5%9F%8E%E5%B8%82/"},{"title":"学习","uri":"https://www.xzywisdili.com/tags/%E5%AD%A6%E4%B9%A0/"},{"title":"工作","uri":"https://www.xzywisdili.com/tags/%E5%B7%A5%E4%BD%9C/"},{"title":"影评","uri":"https://www.xzywisdili.com/tags/%E5%BD%B1%E8%AF%84/"},{"title":"总结","uri":"https://www.xzywisdili.com/tags/%E6%80%BB%E7%BB%93/"},{"title":"想法","uri":"https://www.xzywisdili.com/tags/%E6%83%B3%E6%B3%95/"},{"title":"推理","uri":"https://www.xzywisdili.com/tags/%E6%8E%A8%E7%90%86/"},{"title":"数据处理","uri":"https://www.xzywisdili.com/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"},{"title":"机器学习","uri":"https://www.xzywisdili.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"title":"流行病学","uri":"https://www.xzywisdili.com/tags/%E6%B5%81%E8%A1%8C%E7%97%85%E5%AD%A6/"},{"title":"爬虫","uri":"https://www.xzywisdili.com/tags/%E7%88%AC%E8%99%AB/"},{"title":"独立游戏","uri":"https://www.xzywisdili.com/tags/%E7%8B%AC%E7%AB%8B%E6%B8%B8%E6%88%8F/"},{"title":"电影","uri":"https://www.xzywisdili.com/tags/%E7%94%B5%E5%BD%B1/"},{"title":"笔记","uri":"https://www.xzywisdili.com/tags/%E7%AC%94%E8%AE%B0/"},{"title":"算法","uri":"https://www.xzywisdili.com/tags/%E7%AE%97%E6%B3%95/"},{"title":"统计","uri":"https://www.xzywisdili.com/tags/%E7%BB%9F%E8%AE%A1/"},{"title":"统计分析","uri":"https://www.xzywisdili.com/tags/%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/"},{"title":"统计图","uri":"https://www.xzywisdili.com/tags/%E7%BB%9F%E8%AE%A1%E5%9B%BE/"},{"title":"统计学","uri":"https://www.xzywisdili.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"},{"title":"编程","uri":"https://www.xzywisdili.com/tags/%E7%BC%96%E7%A8%8B/"},{"title":"美剧","uri":"https://www.xzywisdili.com/tags/%E7%BE%8E%E5%89%A7/"},{"title":"考试","uri":"https://www.xzywisdili.com/tags/%E8%80%83%E8%AF%95/"},{"title":"记录","uri":"https://www.xzywisdili.com/tags/%E8%AE%B0%E5%BD%95/"},{"title":"读书","uri":"https://www.xzywisdili.com/tags/%E8%AF%BB%E4%B9%A6/"},{"title":"读书笔记","uri":"https://www.xzywisdili.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"title":"追星","uri":"https://www.xzywisdili.com/tags/%E8%BF%BD%E6%98%9F/"},{"title":"阅读","uri":"https://www.xzywisdili.com/tags/%E9%98%85%E8%AF%BB/"},{"title":"随想","uri":"https://www.xzywisdili.com/tags/%E9%9A%8F%E6%83%B3/"}]}