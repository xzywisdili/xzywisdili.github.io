<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on XZY&#39;s BLOG</title>
    <link>https://www.xzywisdili.com/tags/python/</link>
    <description>Recent content in Python on XZY&#39;s BLOG</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 26 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://www.xzywisdili.com/tags/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>5 分钟简单生成指定形状的词云</title>
      <link>https://www.xzywisdili.com/post/2023-04-26-generatewordcloudin5minutes/</link>
      <pubDate>Wed, 26 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.xzywisdili.com/post/2023-04-26-generatewordcloudin5minutes/</guid>
      <description>群里有人问，想要做一张这样的指定形状的词云：
我试了一下，其实很简单，基本上 5 分钟就搞定。这里的简单思路就是：
使用 jieba 包对原始文本进行分词处理; 将分词之后的结果导入 wordcloud 包生成分词。 至于指定形状的需求，需要注意在 WordCloud() 中的 mask 参数导入指定的形状遮罩图片就好，具体的代码如下：
import jieba from wordcloud import WordCloud import numpy as np import PIL.Image as image # 对分析文本做分词处理 def cut(text): word_list = jieba.cut(text) result = &amp;#34; &amp;#34;.join(word_list) return result # 读取分析文本，进行分词 with open(&amp;#34;D:\\测试文章.txt&amp;#34;, &amp;#39;rb&amp;#39;) as fp: text = fp.read().decode(&amp;#39;utf-8&amp;#39;) words = cut(text) # 设置目标词云的遮罩 mask = np.array(image.open(&amp;#34;D:\\测试图片.png&amp;#34;)) # 进行词云分析并生成 test_cloud = WordCloud( mask = mask, background_color = &amp;#39;#FFFFFF&amp;#39;, font_path=&amp;#39;C:\\Windows\\Fonts\\方正聚珍新仿.</description>
    </item>
    
    <item>
      <title>动态页面的爬虫示例一则：抓取微博粉丝列表</title>
      <link>https://www.xzywisdili.com/post/2022-10-05-dynamichtmlspider/</link>
      <pubDate>Wed, 05 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.xzywisdili.com/post/2022-10-05-dynamichtmlspider/</guid>
      <description>在碰到一些网页，想通过爬虫抓取页面信息的时候，会发现网页采用了一些动态 HTML 的相关技术来展示信息。这样直接使用 requests 是无法直接获取想要的 HTML 元素内容的。比如我们查看微博网页端的粉丝列表：
微博网页端的粉丝列表，在向下刷的时候是会动态加载和更新的。这种情况下，我们想要的元素是通过 js 事件动态请求和返回的。那么，就需要我们分析页面请求，找到那个发送的请求和对应返回的数据。在 F12 的「网络」选项卡下面进行刷新，可以比较轻松地找到对应发送的请求和返回的数据（json 格式）：
可以看到，这个请求会返回 json 格式的数据。当用户下拉粉丝列表页面时，会触发一个 js 事件，项服务器发送这个请求获取数据，再通过一定的逻辑将这些 json 数据填充到 HTML 页面中。而我们的爬虫只要获取这些 json 数据，再进行整理就可以了。
这样一来就比较轻松了：（当然需要注意一下，发出的相关请求需要你保持登录的 Cookie，不然返回会报错）
import requests headers = { &amp;#39;cookie&amp;#39;: &amp;#39;Your Cookie&amp;#39;, &amp;#39;User-Agent&amp;#39;: &amp;#39;Your User Agent&amp;#39;, &amp;#39;referer&amp;#39;: &amp;#39;Your referer&amp;#39; } url = &amp;#34;https://weibo.com/ajax/friendships/friends?relate=fans&amp;amp;page=1&amp;amp;uid=3668829440&amp;amp;type=all&amp;amp;newFollowerCount=0&amp;#34; r = requests.get(url, headers=headers).json() for user in r[&amp;#39;users&amp;#39;]: print(&amp;#34;id: {} - name: {} = fans:{}&amp;#34;.format(user[&amp;#39;id&amp;#39;], user[&amp;#39;screen_name&amp;#39;], user[&amp;#39;followers_count&amp;#39;])) 当然，我们可以看到 url 里面的参数 page=1，那么我们可以修改这个参数就可以获得很多页面的粉丝列表信息了。再保存到准备好的数据库中：
import requests import pymysql # 连接 MySQL 数据库 conn = pymysql.</description>
    </item>
    
    <item>
      <title>光棍节初探 TensorFlow（一）：数据集的预处理</title>
      <link>https://www.xzywisdili.com/post/2017-11-12-tensorflow01/</link>
      <pubDate>Sun, 12 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.xzywisdili.com/post/2017-11-12-tensorflow01/</guid>
      <description>11 月 11 日这天注定对我具有了一定的意义。不是因为它是购物狂欢节或光棍节，而是因为在这一天，我第一次尝试使用 TensorFlow 搭建了一个简单的神经网络。我希望用几篇文章记录这个过程。
最近在读 Fundamentals of Deep Learning 这本书。我选择它的原因是讲解得通俗易懂，又会直白地点出重点内容。然而当我读到第三章「Implementing Neural Networks in TensorFlow」时，整个人就好像懵了一样。对于一个从来没接触过 TensorFlow 的人来说，是难以通过看代码直接理解 Graph, Session 等等这些新概念的。于是联想到程序员的思维修炼里面提到的「SQ3R 阅读法」，赶紧先放下这本书，到网上找其他关于 TensorFlow 的资料，值得推荐的是：
TensorFlow 官方文档中文版 TF Girls「TensorFlow Tutorial」修炼指南（这老师很幽默） youtube 地址 bilibili 地址 没想到我居然也能一天完成了一个基础的神经网络（虽然是从下午 1 点到晚上 2 点）。现在到了「SQ3R 阅读法」中的很重要的 Recite（复述）这步———把这个过程写成文章发布到博客里。
所使用的数据集来自 The Street View House Numbers (SVHN) Dataset，这是一个关于识别街景照片中出现的数字的数据集。
读取数据 首先下载 Format 2 格式的数据，即 .mat 格式的数据。我们先在 iPython 里面探索一下数据：
In[1]: from scipy.io import loadmat as load In[2]: train_data = load(&amp;#39;data/train_32x32.mat&amp;#39;) ...: test_data = load(&amp;#39;data/test_32x32.</description>
    </item>
    
    <item>
      <title>「翻译」如何用 Python 画出像 FiveThirtyEight 那么棒的图表</title>
      <link>https://www.xzywisdili.com/post/2017-10-15-trans01/</link>
      <pubDate>Sun, 15 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.xzywisdili.com/post/2017-10-15-trans01/</guid>
      <description>&lt;p&gt;如果你经常读数据科学领域的文章的话，你可能会偶然发现 &lt;a href=&#34;http://fivethirtyeight.com&#34;&gt;FiveThirtyEight&lt;/a&gt; 上的内容，然后被他们惊艳的图表迷住。于是你自己也想制作如此出色的可视化作品，于是去 Quora 和 Reddit 上问怎么做。你收到了几个回答，但是这些回答都很模糊。你还是不知道怎么搞定这样的图表。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>用 Python 解决数据结构与算法题 01</title>
      <link>https://www.xzywisdili.com/post/2017-10-05-algorithm01/</link>
      <pubDate>Thu, 05 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.xzywisdili.com/post/2017-10-05-algorithm01/</guid>
      <description>&lt;p&gt;这是 Python 解决算法问题的第一期。我想以后每期写出三道题。本人是医学生，没有上过任何数据结构和算法课，对于解决算法题也纯属个人的课余兴趣爱好。还希望各位真正的大牛不吝赐教。我会继续坚持下去的。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
