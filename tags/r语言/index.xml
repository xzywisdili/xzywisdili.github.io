<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R语言 on XZY&#39;s BLOG</title>
    <link>https://www.xzywisdili.com/tags/r%E8%AF%AD%E8%A8%80/</link>
    <description>Recent content in R语言 on XZY&#39;s BLOG</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sun, 23 Apr 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://www.xzywisdili.com/tags/r%E8%AF%AD%E8%A8%80/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>数据挖掘的奇效——爬到网站没有显示的数据</title>
      <link>https://www.xzywisdili.com/post/2023-04-23-userjsonfindextradata/</link>
      <pubDate>Sun, 23 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://www.xzywisdili.com/post/2023-04-23-userjsonfindextradata/</guid>
      <description>有没有想过，有些网站的页面上展示的数据只有一部分的时候，通过稍微一点挖掘就能发现额外的数据，收获更多信息。&#xA;这里就举一个最近发现的例子——云顶之弈公开赛“云巅赛道”的官方实时排名更新。 可以看到，这里只展示了云巅赛道前 60 名的用户昵称、段位、胜点、排位场数、登顶率和前四率。&#xA;但是其实，只要扒到网站请求的数据，兴许可以看到更多的数据。&#xA;发现网站请求返回的数据 这里其实没有想象中那么难。在「检查」中的「网络」选项卡查看网站的请求，一眼就能发现这个全是大写的请求返回的 json 数据。 稍微看一下，各类数据也都对的上，就是这个了：&#xA;这里可以复制请求链接，再每隔一段时间请求一次，查看实时更新变化，我这里就不做了，有兴趣的可以查看我的另一篇文章。我就直接将 json 数据保存到了本地。&#xA;使用 RJson 解析 json 格式数据 这里就使用一下 R 语言中的 RJson 包来解析一下 json 格式数据：&#xA;library(rjson) result &amp;lt;- fromJSON(file=&amp;#34;云巅.json&amp;#34;) result 可以看到，直接使用 fromJSON 读取出来的是 list 格式的数据，还是多次嵌套的 list：&#xA;第一层：每一个用户作为一层; 第二层：单个用户下的每一个变量数据作为第二层； 同时，可以看到，最关键的用户名 sName 变量用了 URL Code 编码，需要再使用 URLdecode 解一下。&#xA;这就需要先进行多层 list 数据的解包，加上稍微一点预处理了：&#xA;library(tidyverse) # 将 json 数据的每一层（每一个用户）转换成单行数据 row_results &amp;lt;- lapply(result[[2]], function(x) { as_tibble(x) }) # 对单行数据进行合并，得到最终数据 toc_rank_result &amp;lt;- do.call(&amp;#34;rbind&amp;#34;, row_results) # 使用 URLdecode 转换用户昵称 toc_rank_result &amp;lt;- toc_rank_result %&amp;gt;% mutate(nickname = URLdecode(sName)) %&amp;gt;% select(nickname, iTier, iRank, iPoints) # 查看转换后的数据 View(toc_rank_result) 大功告成！可以看到，相比于官网页面上的 60 条数据，这里分析的请求数据总共有 100 条，足足多了 40 条。 而且爬下来的数据想要做更多的查询和分析也更加方便了。 有时候，网页上没有显示的数据，可能还真需要扩展一下思路，看看能不能用其他方法得到。</description>
    </item>
    <item>
      <title>【学习笔记】实用统计学复习手册01——探索性数据分析</title>
      <link>https://www.xzywisdili.com/post/2023-03-23-dataanalysisnote01/</link>
      <pubDate>Thu, 23 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://www.xzywisdili.com/post/2023-03-23-dataanalysisnote01/</guid>
      <description>总结 介绍了数据的分类（数值型、分类型）等； 单变量分析：位置度量（均值、中位数）、变异性度量（方差、标准差、百分位数）等； 单变量的可视化：箱线图、频数表、直方图等； 多变量分析：探索相关性。 多变量的可视化：散点图、热力图、列联表、分类箱线图等； 探索性数据分析（Exploratory Data Analysis，EDA）是数据科学项目的第一步。&#xA;数据的分类和主要形式 首先，需要了解数据的分类。在目前世界，数据来源非常丰富，而大多数数据是非结构化的，比如图像、文本、用户交互。这些“信息”，或者说非结构化的数据必须先转化为结构化的数据，才能够用于下一步的分析。&#xA;结构化数据的分类：&#xA;数值型数据 离散型数据：只能取整数； 连续型数据：在一个区间可以取任意值 分类型数据 二元数据：两个值中取其一，0 或 1 多元数据 特殊形式：有序数据，按照分类进行排序 需要强调的是，数据的分类对于后续可视化方法的选择、统计模型的选择都非常重要。&#xA;接下来需要说明的是数据科学中最经典的引用结构——矩形数据。最常见的就是我们经典的电子表格。其中每一行代表一条观测记录，每一列代表一个特征（变量）。在不同统计编程语言中也有对应的处理方式：&#xA;R语言: data.table Python Pandas: DataFrame 其他的几种非矩形的数据形式包括：时间序列数据、空间数据和图形或网络数据。他们都有对应的处理方法。&#xA;如何描述连续性变量？ 位置度量 变量代表了测量数据或者计数数据。探索数据的一个基本步骤，就是了解每个变量（特征）的特点。不同种类的数据，我们对其的主要关注点也不尽相同。统计学中主要关注以下数据特征值：&#xA;均值：基本的位置度量，对极值敏感 加权均值：将每个值乘一个权重值然后除以总和 切尾均值：消除极值之后的均值，比均值更加稳健 中位数：更加稳健，不易受均值影响 加权中位数：将数据集排序之后进行加权，加权中位数就是可以使数据集上下两部分的权重总和相同的值 离群值：并不一定是无效或错误的数据，但往往是由于数据的错误所导致的。 统计学家替换用估计量（estimate）来表示从手头已有数据计算得到的值，来描述数据情况与真实状态之间的差异。数据科学家和商业分析师更倾向于把这些值称为度量（metric）。因为统计学的核心在于如何解释不确定度，而数据科学则更关注如何解决一个具体的商业或企业目标。&#xA;变异性度量 变异性（variability），也称为离差（dispersion），是另外一个描述数据的视角，表示数据是紧密聚集的还是发散的。在分析中主要会考虑：&#xA;测量数据的变异性； 识别各种变异性的来源； 如何降低变异性 统计学中有以下数据特征描述变异性：&#xA;偏差：观测值和估计值之间的直接差异 方差 标准差：方差的平方根 平均绝对偏差：对偏差值取绝对值然后求平均 中位数绝对偏差 极差：最大值和最小值之间的差值 顺序统计量：又称为秩 百分位数 四分位距（IQR）：75 百分位数和 25 百分位数之间的差值 方差和标准偏差是最广泛使用的变异统计量，且都对离群值敏感。更稳健的度量包括百分位数、四分位距和中位数绝对偏差。&#xA;sd (state$Population) IQR(state$Population) mad (state$Population) quantile(state$Murder.Rate, p=c(.05, .25, .5, .75, .95)) 使用图表描述数据分布 箱线图 boxplot(state$Population/1000000, ylab=&amp;#34;Population (millions)&amp;#34;) 箱线图的顶部和底部分别是 75 百分位数和 25 百分位数。水平线代表的是中位数，虚线称为须（whisker） ，从最大值一直延伸到最小值，体现了数据的极差。</description>
    </item>
    <item>
      <title>【学习笔记】保序回归——适用于单调递增数据的统计回归方法</title>
      <link>https://www.xzywisdili.com/post/2023-03-07-isotonicregression/</link>
      <pubDate>Tue, 07 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://www.xzywisdili.com/post/2023-03-07-isotonicregression/</guid>
      <description>什么是保序回归？ 保序回归（Isotonic Regression）是一种适用于单调函数非参数统计回归方法，即在一个序列中，Xn ≥ Xn-1。通过字面理解「Isotonic Regression」：&#xA;iso的意思就是「相等、相同」的意思； tonic 就是 tone 的意思，指的是「调」。 所以保序回归的核心还是在于单调递增的函数，当需要分析的数据资料符合单调递增的趋势可以用。 保序回归最常用的应用场景之一是探索药量和药效的关系，因为一般来说药物剂量越高，药效应该更强，因此通过保序回归的方式可以从药效和经济学的角度估计最合适的药量。&#xA;保序回归使用 weighted least-squares 来进行拟合： 怎么做保序回归？ 求解保序回归的一种最常用算法是 PAVA 算法（ Pool-Adjacent-Violators Algorithm，池相邻违规者算法）。PAVA 算法的直观形式只需要看下面这张图就行了：&#xA;这种算法是通过从左往右逐渐扫描数据序列，并且保证整个序列是单调递增的，以此来获得 Beta 值的结果。如果 Beta_i &amp;lt; Beta_i-1，那么就同时把这两个值替换为 (Beta_i + Beta_i-1) / 2。以此就能获得严格且平滑的保序回归。&#xA;通过 PAVA 算法，可以获得一个包括多个 Beta 参数组成的单调递增序列，用可视化的方法可以看到是由多条上升线和水平线组成的函数图：&#xA;如何使用 R 语言进行保序回归？ 在 R 中可以轻松进行保序回归，只需要使用 stats 包中的 isoreg 函数即可。下面的代码提供一个简单的示例，并将原始数据和拟合值（蓝点）绘制出来。注意，拟合后的蓝点是单调递增的。&#xA;# Generate Training Data set.seed(15) x &amp;lt;- sample(2 * 1:15) y &amp;lt;- 0.5 * x + rnorm(length(x)) # Isotonic Regression Model Fit reg.</description>
    </item>
    <item>
      <title>【医学预测01】Logistic 回归构建变量筛选：先单后多</title>
      <link>https://www.xzywisdili.com/post/2022-09-07-rlogistic01/</link>
      <pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://www.xzywisdili.com/post/2022-09-07-rlogistic01/</guid>
      <description>总结 介绍了 Logistic 回归模型构建过程中的一种变量筛选方式——先单后多； 配合对应的代码实例和结果解释，让方法更加清楚，同时便于日后复习查看。 在 Logistic 回归模型的构建过程中，面对的第一个问题是变量筛选，即筛选哪些作为预测因子进入到最后构建的模型中。其中最为基础的方法是 「先单后多」，顾名思义，即先进行单因素分析，再将单因素分析中具有统计学意义的变量再一起纳入多因素模型中。这样的方法最为简单和实用。&#xA;接下来会通过 R 语言代码的实例来演示整个过程。数据采用的是 [Framingham 十年冠心病风险数据](Framingham_CHD_preprocessed_data | Kaggle。&#xA;Step 1 数据的读取和预处理 第一步，当然是读取数据并且通过 names 、summary 和 str 分别查看数据集的变量名称、基本统计信息和变量的类型。尤其是后两者，能够提供变量的数据类型和统计信息，有助于发现异常的数据，在分析之前发现原因提前纠正。&#xA;# 导入数据，命名为 data library(readr) data &amp;lt;- read_csv(&amp;#34;framingham.csv&amp;#34;) # 查看变量名称 names(data) # 查看基本统计信息 summary(data) # 查看变量类型 str(data) 接下来对数据做基础的预处理。对于医学数据，需要注意的预处理主要是对于分类变量的转换：&#xA;将分类变量转换为 factor； 二分类变量处理或者不处理不影响结果，多分类变量则一定要进行处理，为了保险和便利起见，尽量都进行处理； 转换中，使用 labels 标清楚各个分类代表的含义，之后不容易发生混淆。 # 标明连续型变量和分类型变量 contin_vars &amp;lt;- c(&amp;#34;age&amp;#34;, &amp;#34;cigsPerDay&amp;#34;, &amp;#34;totChol&amp;#34;, &amp;#34;sysBP&amp;#34;, &amp;#34;diaBP&amp;#34;, &amp;#34;BMI&amp;#34;, &amp;#34;heartRate&amp;#34;, &amp;#34;glucose&amp;#34;) discre_vars &amp;lt;- c(&amp;#34;male&amp;#34;, &amp;#34;education&amp;#34;, &amp;#34;currentSmoker&amp;#34;, &amp;#34;BPMeds&amp;#34;, &amp;#34;prevalentStroke&amp;#34;, &amp;#34;prevalentHyp&amp;#34;, &amp;#34;diabetes&amp;#34;) # 处理分类变量 data$TenYearCHD &amp;lt;- factor(data$TenYearCHD, levels=c(0, 1), labels = c(&amp;#34;未来10年无冠心病风险&amp;#34;, &amp;#34;未来10年具有冠心病风险&amp;#34;)) data[discre_vars] &amp;lt;- lapply(data[discre_vars], factor) Step 2 单因素分析 一般而言，单因素可以采用的统计分析方法包括：t 检验、卡方检验和秩和检验。t 检验需要满足数据符合正态分布，如果不满足则需要考虑秩和检验这种非参数检验。另外卡方检验则一般面向分类的数据，而非定量数据，比如性别（1代表男，2代表女，数字无比较意义）。</description>
    </item>
  </channel>
</rss>
